"use strict";(self.webpackChunkacd=self.webpackChunkacd||[]).push([[1070],{1508:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>h,frontMatter:()=>o,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"task/sensor_data_processing/Semantic-Image-Segmentation","title":"Semantic Image Segmentation Applied on Camera Images","description":"ROS2","source":"@site/docs/task/02_sensor_data_processing/06_Semantic-Image-Segmentation.md","sourceDirName":"task/02_sensor_data_processing","slug":"/task/sensor_data_processing/Semantic-Image-Segmentation","permalink":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Semantic-Image-Segmentation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/task/02_sensor_data_processing/06_Semantic-Image-Segmentation.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{},"sidebar":"taskSidebar","previous":{"title":"Object Detection","permalink":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Object-Detection"},"next":{"title":"Perform Deep Learning Based Semantic Point Cloud Segmentation","permalink":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Semantic-Point-Cloud-Segmentation"}}');var i=s(4848),t=s(8453);const o={},r="Semantic Image Segmentation Applied on Camera Images",c={},l=[{value:"Start the Docker Environment",id:"start-the-docker-environment",level:2},{value:"Download and Inspect Bag file",id:"download-and-inspect-bag-file",level:2},{value:"ROS2&#39;s <code>sensor_msgs/msg/Image</code> Message",id:"ros2s-sensor_msgsmsgimage-message",level:2},{value:"ROS2&#39;s <code>sensor_msgs/msg/CameraInfo</code> Message",id:"ros2s-sensor_msgsmsgcamerainfo-message",level:2},{value:"Build and source the package",id:"build-and-source-the-package",level:2},{value:"Replay rosbag and run image segmentation",id:"replay-rosbag-and-run-image-segmentation",level:2},{value:"Review of file image_segmentation.py",id:"review-of-file-image_segmentationpy",level:2},{value:"Task 1: Implement conversion from segmentation map to RGB encoding",id:"task-1-implement-conversion-from-segmentation-map-to-rgb-encoding",level:2},{value:"Expected output",id:"expected-output",level:3},{value:"Wrap-up",id:"wrap-up",level:2},{value:"ROS1 Instructions",id:"ros1-instructions",level:2},{value:"Perform Deep Learning based semantic image segmentation applied on camera images",id:"perform-deep-learning-based-semantic-image-segmentation-applied-on-camera-images",level:4},{value:"Start the Docker Environment",id:"start-the-docker-environment-1",level:3},{value:"Download and Inspect Bag file",id:"download-and-inspect-bag-file-1",level:2},{value:"ROS&#39; <code>sensor_msgs/Image</code> Message",id:"ros-sensor_msgsimage-message",level:2},{value:"ROS&#39; <code>sensor_msgs/CameraInfo</code>",id:"ros-sensor_msgscamerainfo",level:2},{value:"Build and source the package",id:"build-and-source-the-package-1",level:2},{value:"Replay rosbag and run image segmentation",id:"replay-rosbag-and-run-image-segmentation-1",level:2},{value:"Review of file image_segmentation.py",id:"review-of-file-image_segmentationpy-1",level:2},{value:"Task 1: Implement conversion from segmentation map to RGB encoding",id:"task-1-implement-conversion-from-segmentation-map-to-rgb-encoding-1",level:2},{value:"Expected output",id:"expected-output-1",level:4},{value:"Wrap-up",id:"wrap-up-1",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"semantic-image-segmentation-applied-on-camera-images",children:"Semantic Image Segmentation Applied on Camera Images"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://img.shields.io/badge/ROS2-red",alt:"ROS2"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Image Segmentation",src:s(9167).A+"",width:"1534",height:"392"})}),"\n",(0,i.jsxs)(n.p,{children:["In this workshop, we will perform ",(0,i.jsx)(n.strong,{children:"semantic image segmentation"})," on raw camera data using a deep learning model. In particular, we will take a recording from our test vehicle which is equipped with a on board camera and we will apply the segmentation model on the sensor data stream."]}),"\n",(0,i.jsx)(n.p,{children:"The learning goals of this workshop are"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Inspect a rosbag which contains camera data"}),"\n",(0,i.jsx)(n.li,{children:"Learn about ROS2 standard camera and camera info message format"}),"\n",(0,i.jsx)(n.li,{children:"Learn about a simple Python inference node for semantic image segmentation"}),"\n",(0,i.jsx)(n.li,{children:"Learn to visualize the output of semantic image segmentation"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"start-the-docker-environment",children:"Start the Docker Environment"}),"\n",(0,i.jsxs)(n.p,{children:["Navigate to the local directory ",(0,i.jsx)(n.code,{children:"${REPOSITORY}/docker"})," and execute ",(0,i.jsx)(n.code,{children:"./ros2_run.sh"}),". This will start the Docker container, in which ROS and all required libraries are preinstalled. You can stop the container by pressing ",(0,i.jsx)("kbd",{children:"Ctrl"}),"+",(0,i.jsx)("kbd",{children:"C"})," in the terminal. If everything is setup correctly you will see the following:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Starting new container...\n================================================================================\n\n=== CONTAINER INFORMATION ======================================================\nArchitecture: x86_64\nUbuntu: 22.04.2 LTS (Jammy Jellyfish)\nPython: 3.10.6\nROS: humble\nCMake: 3.22.1\nCUDA: 12.1.105\ncuDNN: 8.9.2\nTensorRT: 8.6.1\nTensorFlow Python: 2.13.0\nTensorFlow C/C++: \nPyTorch Python: \nPyTorch C/C++: \nAvailable GPUs: 1\n  name               driver_version   utilization.gpu [%]   utilization.memory [%]   memory.used [MiB]   memory.total [MiB]\n  NVIDIA TITAN RTX   470.182.03       0 %                   2 %                      552 MiB             24217 MiB\n===============================================================================\n\nroot@******:/home/rosuser/ws/colcon_workspace# \n"})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"acdc"})," folder is mounted from your host into the container. Note that your current working directory inside the container is ",(0,i.jsx)(n.code,{children:"/home/rosuser/ws/colcon_workspace"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"download-and-inspect-bag-file",children:"Download and Inspect Bag file"}),"\n",(0,i.jsxs)(n.p,{children:["Download the rosbag ",(0,i.jsx)(n.code,{children:"left_camera_templergraben.db3"})," from ",(0,i.jsx)(n.a,{href:"https://rwth-aachen.sciebo.de/s/QxGbpvIQF4Mk8Uo",children:(0,i.jsx)(n.strong,{children:"here (1.2 GB)"})}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["Save this file to your local directory ",(0,i.jsx)(n.code,{children:"${REPOSITORY}/bag"}),". This directory will be mounted into the docker container to the path ",(0,i.jsx)(n.code,{children:"/home/rosuser/ws/bag"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["You can start the docker container now with ",(0,i.jsx)(n.code,{children:"./ros2_run.sh"})," (if you haven't already)."]}),"\n",(0,i.jsxs)(n.p,{children:["Inside the container, you can navigate to ",(0,i.jsx)(n.code,{children:"/home/rosuser/ws/bag"})," and execute ",(0,i.jsx)(n.code,{children:"ros2 bag info left_camera_templergraben.db3"})," to inspect the rosbag:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"~/bag$ ros2 bag info lef_camera_templegraben\nFiles:             left_camera_templergraben.db3\nBag size:          1.2 GiB\nStorage id:        sqlite3\nDuration:          17.999s\nStart:             Aug  9 2019 10:59:01.823 (1565341141.823)\nEnd:               Aug  9 2019 10:59:19.823 (1565341159.823)\nMessages:          2881\nTopic information: \n    Topic: /tf | Type: tf2_msgs/msg/TFMessage | Count: 1801 | Serialization Format: cdr\n    Topic: /sensors/camera/left/camera_info | \n    Type: sensor_msgs/msg/CameraInfo | Count: 540 | Serialization \n    Format: cdr\n    Topic: /sensors/camera/left/image_raw | \n    Type: sensor_msgs/msg/Image | Count: 540 |  Serialization \n    Format: cdr\n"})}),"\n",(0,i.jsxs)(n.p,{children:["You can see that the rosbag has a duration of 18 seconds and contains 540 image frames of type ",(0,i.jsx)(n.code,{children:"sensor_msgs/Image"})," and 540 corresponding ",(0,i.jsx)(n.code,{children:"sensor_msgs/CameraInfo"})," messages. We will use these camera images in this assignment in order to apply image segmentation."]}),"\n",(0,i.jsxs)(n.h2,{id:"ros2s-sensor_msgsmsgimage-message",children:["ROS2's ",(0,i.jsx)(n.code,{children:"sensor_msgs/msg/Image"})," Message"]}),"\n",(0,i.jsxs)(n.p,{children:["The message definition ",(0,i.jsx)(n.a,{href:"https://docs.ros2.org/latest/api/sensor_msgs/msg/Image.html",children:"sensor_msgs/msg/Image"})," is ROS2's standard image message format. It is used for all kind of camera image message types and can be used seamlessly with many different ROS2 visualization and image processing tools. Please read the documentation about the ",(0,i.jsx)(n.a,{href:"https://docs.ros2.org/latest/api/sensor_msgs/msg/Image.html",children:"detailed message format"})," and it's content.\nMessage"]}),"\n",(0,i.jsxs)(n.h2,{id:"ros2s-sensor_msgsmsgcamerainfo-message",children:["ROS2's ",(0,i.jsx)(n.code,{children:"sensor_msgs/msg/CameraInfo"})," Message"]}),"\n",(0,i.jsxs)(n.p,{children:["The message definition ",(0,i.jsx)(n.a,{href:"hhttps://docs.ros2.org/latest/api/sensor_msgs/msg/CameraInfo.html",children:"sensor_msgs/msg/CameraInfo"})," is ROS2's standard camera info message format. It is send together with ",(0,i.jsx)(n.code,{children:"sensor_msgs/msg/Image"})," to provide additional information about the current camera image such as ",(0,i.jsx)(n.strong,{children:"camera calibration parameters"}),". Feel free to read the documentation about the ",(0,i.jsx)(n.a,{href:"https://docs.ros2.org/latest/api/sensor_msgs/msg/CameraInfo.html",children:"detailed message format"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"build-and-source-the-package",children:"Build and source the package"}),"\n",(0,i.jsxs)(n.p,{children:["The code for the image segmentation inference node can be found in the directory ",(0,i.jsx)(n.code,{children:"colcon_workspace/src/section_2/image_segmentation_r2"}),". The structure of this ",(0,i.jsx)(n.strong,{children:"Python package"})," is illustrated in the following:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"image_segmentation_r2/\n\u251c\u2500\u2500 package.xml\n\u251c\u2500\u2500 setup.cfg\n\u2502   \u2514\u2500\u2500 params.yaml\n\u251c\u2500\u2500 image_segmentation_r2\n\u2502   \u251c\u2500\u2500 image_segmentation.py\n\u2502   \u251c\u2500\u2500 img_utils.py\n\u2502   \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 launch\n\u2502\xa0\xa0 \u2514\u2500\u2500 image_segmentation_r2.launch.py\n\u251c\u2500\u2500 models\n\u2502\xa0\xa0 \u251c\u2500\u2500 convert_cityscapes_to_ika_reduced.xml\n\u2502\xa0\xa0 \u251c\u2500\u2500 mobilenet_v3_large_968_608_os8.pb\n\u2502\xa0\xa0 \u2514\u2500\u2500 mobilenet_v3_small_968_608_os8.pb\n\u251c\u2500\u2500 resource\n\u2514\u2500\u2500 test\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The main source code is located in the directory ",(0,i.jsx)(n.code,{children:"image_segmentation_r2"}),", the pretrained segmentation models are located in ",(0,i.jsx)(n.code,{children:"models"})," and the launch file are located in directory ",(0,i.jsx)(n.code,{children:"launch"}),"and parameters are located in ",(0,i.jsx)(n.code,{children:"config"}),". Feel free to read all the code, parameters and launch files."]}),"\n",(0,i.jsx)(n.p,{children:"Note, that we provide here two image segmentation models for you. Both rely on the MobilnetV3 architecture:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"mobilenet_v3_large_968_608_os8.pb"}),": Larger model, slower inference, more RAM needed"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"mobilenet_v3_small_968_608_os8.pb"}),": Smaller model, faster inference, less RAM needed"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"These models are trained on a much larger dataset compared to the model you have trained in the exercise, but the overall training pipeline and additional augmentation methods applied during the training are almost identical."}),"\n",(0,i.jsxs)(n.p,{children:["You might change the model in the ",(0,i.jsx)(n.code,{children:"params.yaml"})," configuration file depending on the capabilities of your computer."]}),"\n",(0,i.jsxs)(n.p,{children:["Now, let's build the package with with ",(0,i.jsx)(n.code,{children:"colcon build"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"colcon build --packages-select image_segmentation_r2 --symlink-install\n"})}),"\n",(0,i.jsx)(n.p,{children:"and source the workspace"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"source install/setup.bash\n"})}),"\n",(0,i.jsx)(n.p,{children:"Perfect! Now you will be able to perform inference on camera images with this package. Let's go to the next section."}),"\n",(0,i.jsx)(n.h2,{id:"replay-rosbag-and-run-image-segmentation",children:"Replay rosbag and run image segmentation"}),"\n",(0,i.jsx)(n.p,{children:"We have already prepared a launch file for you to execute the image segmentation. Please read carefully through the following lines of code."}),"\n",(0,i.jsxs)(n.p,{children:["Contents of the file ",(0,i.jsx)(n.code,{children:"image_segmentation_r2.launch.py"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-py",children:"import os\n\nfrom ament_index_python.packages import get_package_share_directory\nfrom launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument\nfrom launch_ros.actions import Node\nfrom launch.actions import ExecuteProcess\n\ndef generate_launch_description():\n\n    # Get the package and params directory\n    image_segmentation_dir = get_package_share_directory('image_segmentation_r2')\n    config = os.path.join(image_segmentation_dir, \"config\",\"params.yaml\")\n\n    # Declare launch arguments\n    use_sim_time = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='true',\n        description='Use simulation clock time')\n\n    # ROSBAG PLAY node\n    rosbag_play_node = ExecuteProcess(\n        cmd=['ros2', 'bag', 'play', '--rate', '0.05', '-l',\n             '/home/rosuser/ws/bag/left_camera_templergraben',\n             '--topics', '/sensors/camera/left/image_raw',\n             '/sensors/camera/left/camera_info'],\n        output='screen'\n    )\n\n    # IMAGE_PROC node\n    image_proc_node = Node(\n        package='image_proc',\n        name='image_proc',\n        executable='image_proc',\n        namespace='sensors/camera/left',\n        output='screen',\n        remappings=[\n            ('image', 'image_raw'),\n        ],\n    )\n        # CAMERA SEGMENTATION NODE\n    camera_segmentation_node = Node(\n        package='image_segmentation_r2',\n        name='image_segmentation',\n        executable='image_segmentation',\n        output='screen',\n        parameters=[config],\n        remappings=[\n            ('image_color', 'sensors/camera/left/image_color')\n        ]\n    )\n\n        # NODES FOR VISUALIZATION\n    segmentation_viewer_node = Node(\n        package='image_view',\n        executable='image_view',\n        name='segmentatibashon_viewer',\n        remappings=[\n            ('image', 'image_segmented'),ros2 launch image_segmentation_py start_all.launch\n        ],\n    )\n\n    camera_node = Node(\n        package='image_view',\n        executable='image_view',\n        name='camera_viewer',\n        remappings=[\n            ('image', 'sensors/camera/left/image_color'),\n        ],\n        parameters=[\n                {'autosize': True},\n        ]\n    )\n\n    # Create the launch description and populate\n    ld = LaunchDescription()\n\n    # Add the actions to the launch description\n    ld.add_action(use_sim_time)\n    ld.add_action(rosbag_play_node)\n    ld.add_action(image_proc_node)\n    ld.add_action(camera_node)\n    ld.add_action(camera_segmentation_node)\n    ld.add_action(segmentation_viewer_node)\n\n    return ld\n"})}),"\n",(0,i.jsx)(n.p,{children:"Hence, we perform the following tasks:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Replay the rosbag"})," with a speed of 0.05. Note, that we set the speed to a very low value here, because your computer might be very slow. You can adapt this values if your computer is fast enough to compute the segmentation at a higher speed."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Apply ",(0,i.jsx)(n.code,{children:"image_proc"})]})," to the raw sensor data. The topic ",(0,i.jsx)(n.code,{children:"/sensors/camera/left/image_raw"})," was recorded in the raw data format. With ",(0,i.jsx)(n.code,{children:"image_proc"})," we convert it to a RGB encoding. Read more ",(0,i.jsx)(n.a,{href:"https://wiki.ros.org/image_proc",children:"here"})," about it."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Start the ",(0,i.jsx)(n.code,{children:"image_segmentation"})]})," node and feed it with the correct topic name and load the parameters that are necessary for the node."]}),"\n",(0,i.jsxs)(n.li,{children:["Start two ",(0,i.jsx)(n.strong,{children:"nodes for visualization"})]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Note: You can also use RVIZ to visualize the RGB camera image and the segmented camera image."}),"\n",(0,i.jsx)(n.p,{children:"We can now start the launch file with:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"ros2 launch image_segmentation_r2 image_segmentation_r2.launch.py \n"})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"image_view"})," node should show you directly the camera image as shown in this image:"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Image Segmentation",src:s(6366).A+"",width:"1612",height:"883"})}),"\n",(0,i.jsx)(n.p,{children:"However, the segmented image looks like this"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Image Segmentation",src:s(1802).A+"",width:"967",height:"640"})}),"\n",(0,i.jsx)(n.p,{children:"Something is wrong. We have apparently a bug in the code !!! Let's solve this problem."}),"\n",(0,i.jsx)(n.h2,{id:"review-of-file-image_segmentationpy",children:"Review of file image_segmentation.py"}),"\n",(0,i.jsxs)(n.p,{children:["Before we start with the task, let's try to understand what happens in the file ",(0,i.jsx)(n.a,{href:"https://github.com/ika-rwth-aachen/acdc/blob/main/colcon_workspace/src/section_2/image_segmentation_r2/image_segmentation_r2/image_segmentation.py",children:(0,i.jsx)(n.code,{children:"image_segmentation.py"})}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["The file ",(0,i.jsx)(n.code,{children:"image_segmentation.py"})," contains the inference node for the image segmentation task. The inference node is implemented as a Python class called ",(0,i.jsx)(n.code,{children:"ImageSegmentation"}),". The class has the following member functions. We will give here a short description of each class so you can understand what each class is doing."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"class ImageSegmentation"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["Class which implements image segmentation inference applied on images send as ROS2 MSG of type ",(0,i.jsx)(n.code,{children:"sensor_msgs/msg/Image"})]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"__init__(self)"})})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["Initializes the class by initializing it with a ROS Node. Calls\n",(0,i.jsx)(n.code,{children:"setup()"})," and ",(0,i.jsx)(n.code,{children:"load_parameters()"})," to load the model and necessary parameters."]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"load_parameters(self)"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Loads the ROS params and stores them into the current instance of ImageSegmentation"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"setup(self)"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["Loads the image segmentation model which are stored in the directory ",(0,i.jsx)(n.code,{children:"models"}),". Also creates a ",(0,i.jsx)(n.code,{children:"cv_bridge"})," which allows to convert ",(0,i.jsx)(n.code,{children:"sensor_msgs/msg/Image"})," into an image which can be processed by the neural network. Also registers subscribers and publishers."]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"predict(self, img_color_msg)"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["This is the so called callback function. This function is triggered, when the subscriber ",(0,i.jsx)(n.code,{children:"self.sub_image"})," receives a message on topic ",(0,i.jsx)(n.code,{children:'"/image_color"'}),". This function performs the actual inference of the neural network. It converts the ",(0,i.jsx)(n.code,{children:"sensor_msgs/msg/Image"})," message into a format that can be processed by the neural network, then performs the inference and then converts the output, a segmentation maps, into a RGB encoding which is then send as an image using publisher ",(0,i.jsx)(n.code,{children:"self.pub_seg"})]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"load_frozen_graph(path_to_frozen_graph)"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["Takes the path to one of the models stored in the directory ",(0,i.jsx)(n.code,{children:"models"})," and converts the frozen graph into an executable Python function. Uses helper function ",(0,i.jsx)(n.code,{children:"wrap_frozen_graph()"})]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"wrap_frozen_graph(graph_def, inputs, outputs, print_graph=False)"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Helper function that converts a frozen graph, which is a type how a neural network can be stored using Tensorflow, into an executable Python function"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"segmentation_map_to_rgb(segmentation_map)"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"A function which converts a segmentation map into RGB encoded image"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"parse_convert_xml(conversion_file_path)"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["Reads a xml file which is located in ",(0,i.jsx)(n.code,{children:"models"})," and which contains information which class ID is associated with which RGB value and vice versa. It processes the xml file in such a way that it can be used to convert a segmentation map in to an RGB encoded image."]})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"task-1-implement-conversion-from-segmentation-map-to-rgb-encoding",children:"Task 1: Implement conversion from segmentation map to RGB encoding"}),"\n",(0,i.jsxs)(n.p,{children:["Unfortunately, the function ",(0,i.jsx)(n.code,{children:"segmentation_map_to_rgb()"})," in the file ",(0,i.jsx)(n.a,{href:"https://github.com/ika-rwth-aachen/acdc/blob/main/catkin_workspace/src/workshops/section_2/image_segmentation_py/src/image_segmentation.py",children:(0,i.jsx)(n.code,{children:"image_segmentation.py"})})," wasn't implemented correctly. Open this file with your favorite code editor and let's have a look on this function."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def segmentation_map_to_rgb(self, segmentation_map):\n    """\n    Converts segmentation map to a RGB encoding according to self.color_palette\n    Eg. 0 (Class 0) -> Pixel value [128, 64, 128] which is on index 0 of self.color_palette\n        1 (Class 1) -> Pixel value [244, 35, 232] which is on index 1 of self.color_palette\n\n    self.color_palette has shape [256, 3]. Each index of the first dimension is associated\n    with an RGB value. The index corresponds to the class ID.\n\n    :param segmentation_map: ndarray numpy with shape (height, width)\n    :return: RGB encoding with shape (height, width, 3)\n    """\n    ### START CODE HERE ###\n    \n    # Replace the following command\n    rgb_encoding = np.random.randint(\n        low=0,\n        high=255,\n        size=[self.resize_height, self.resize_width, 3]\n    )\n\n    ### END CODE HERE ###\n    return rgb_encoding\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Instead of computing the RGB encoding from the segmentation map, the function generates only a random image. ",(0,i.jsx)(n.strong,{children:"Your task"})," is now to implement the function correctly.  Note that ",(0,i.jsx)(n.code,{children:"self.color_palette"})," is here not a function parameter but a class attribute of the class ",(0,i.jsx)(n.code,{children:"ImageSegmentation"}),"."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Hints"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"There are several approaches how to convert from segmentation map to the color encoding"}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Loop approach:"})," Iterate over all class IDs in segmentation map and retrieve the corresponding RGB value for each class and place this triplet (R,G,B) at the correct location of the returned RGB image."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Advanced vectorized approach:"})," Should be the faster and more efficient implementation. Avoid using a loop, but rather rely on numpy's vectorized indexing operations!"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"expected-output",children:"Expected output"}),"\n",(0,i.jsxs)(n.p,{children:["After fixing the function ",(0,i.jsx)(n.code,{children:"segmentation_map_to_rgb()"}),", you will see that the inference node now will publish correct RGB encoded segmentations of the camera image. You will obtain segmented images as shown here:"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Image Segmentation",src:s(7164).A+"",width:"973",height:"607"})}),"\n",(0,i.jsx)(n.h2,{id:"wrap-up",children:"Wrap-up"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"You learned about the ROS2 definitions for camera images and camera info messages"}),"\n",(0,i.jsx)(n.li,{children:"You learned about a simple Python ROS2 package for semantic image segmentation"}),"\n",(0,i.jsx)(n.li,{children:"You learned about encoding the segmentation map to the RGB encoding"}),"\n",(0,i.jsxs)(n.li,{children:["You learned about ",(0,i.jsx)(n.code,{children:"image_view"}),", a node for visualizing image data"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"ros1-instructions",children:"ROS1 Instructions"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{src:"https://img.shields.io/badge/ROS1-blue",alt:"ROS1"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"Image Segmentation",src:s(9167).A+"",width:"1534",height:"392"})}),"\n",(0,i.jsx)(n.h4,{id:"perform-deep-learning-based-semantic-image-segmentation-applied-on-camera-images",children:"Perform Deep Learning based semantic image segmentation applied on camera images"}),"\n",(0,i.jsxs)(n.p,{children:["In this workshop, we will perform ",(0,i.jsx)(n.strong,{children:"semantic image segmentation"})," on raw camera data using a deep learning model. In particular, we will take a recording from our test vehicle which is equipped with a on board camera and we will apply the segmentation model on the sensor data stream."]}),"\n",(0,i.jsx)(n.p,{children:"The learning goals of this workshop are"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Inspect a rosbag which contains camera data"}),"\n",(0,i.jsx)(n.li,{children:"Learn about ROS' standard camera and camera info message format"}),"\n",(0,i.jsx)(n.li,{children:"Learn about a simple Python inference node for semantic image segmentation"}),"\n",(0,i.jsx)(n.li,{children:"Learn to visualize the output of semantic image segmentation"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"start-the-docker-environment-1",children:"Start the Docker Environment"}),"\n",(0,i.jsxs)(n.p,{children:["Navigate to the local directory ",(0,i.jsx)(n.code,{children:"${REPOSITORY}/docker"})," and execute ",(0,i.jsx)(n.code,{children:"./ros1_run.sh"}),". This will start the Docker container, in which ROS and all required libraries are preinstalled. You can stop the container by pressing ",(0,i.jsx)("kbd",{children:"Ctrl"}),"+",(0,i.jsx)("kbd",{children:"C"})," in the terminal. If everything is setup correctly you will see the following:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Starting container ...\nStarting container in mode: gpu\nnon-network local connections being added to access control list\nContainer setup:\n- Ubuntu: 20.04.2 LTS (Focal Fossa) (user: rosuser, password: rosuser)\n- CUDA: Cuda compilation tools, release 11.2, V11.2.152\n- cuDNN: 8.1.0\n- TensorRT: 8.0.3\n- TensorFlow Python3: 2.6.0 (GPUs available: 1)\n- TensorFlow C/C++: 2.6\n- ROS: noetic\n- CMake: cmake version 3.12.3\n\nTemplate Commands:\n- Create new ROS package:            ros-add-package\n  - Add node to package:               ros-add-node\n  - Add nodelet to package:            ros-add-nodelet\n- Initialize ROS GitLab repository:  ros-init-repo\n\n\nThe container is running. Execute the run script again from another terminal to open a shell in the container or press `CTRL-C` to stop the container.\n"})}),"\n",(0,i.jsxs)(n.p,{children:["From another terminal, execute ",(0,i.jsx)(n.code,{children:"./ros1_run.sh"})," again to open a shell in the running container. You should see this:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"Attaching to running container ...\n===================================================================\n= ROS Docker Container                                            =\n===================================================================\n\nThis is the image.\nrosuser@******:~/ws/catkin_workspace$\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"acdc"})," folder is mounted from your host into the container. Note that your current working directory inside the container is ",(0,i.jsx)(n.code,{children:"/home/rosuser/ws/catkin_workspace"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"download-and-inspect-bag-file-1",children:"Download and Inspect Bag file"}),"\n",(0,i.jsxs)(n.p,{children:["Download the file ",(0,i.jsx)(n.code,{children:"left_camera_templergraben.bag"})," from ",(0,i.jsx)(n.a,{href:"https://rwth-aachen.sciebo.de/s/sbSBamXYCfQw9kM",children:(0,i.jsx)(n.strong,{children:"here (1.2 GB)"})}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["Save this file to your local directory ",(0,i.jsx)(n.code,{children:"${REPOSITORY}/bag"}),". This directory will be mounted into the docker container to the path ",(0,i.jsx)(n.code,{children:"/home/rosuser/ws/bag"}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["You can start the docker container now with ",(0,i.jsx)(n.code,{children:"./ros1_run.sh"})," (if you haven't already)."]}),"\n",(0,i.jsxs)(n.p,{children:["Inside the container, you can navigate to ",(0,i.jsx)(n.code,{children:"/home/rosuser/ws/bag"})," and execute ",(0,i.jsx)(n.code,{children:"rosbag info left_camera_templergraben.bag"})," to inspect the rosbag:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"rosuser@i2000033:~/ws/bag$ rosbag info left_camera_templergraben.bag \npath:        left_camera_templergraben.bag\nversion:     2.0\nduration:    18.0s\nstart:       Aug 09 2019 10:59:01.82 (1565341141.82)\nend:         Aug 09 2019 10:59:19.82 (1565341159.82)\nsize:        1.2 GB\nmessages:    2881\ncompression: none [541/541 chunks]\ntypes:       sensor_msgs/CameraInfo [c9a58c1b0b154e0e6da7578cb991d214]\n             sensor_msgs/Image      [060021388200f6f0f447d0fcd9c64743]\n             tf2_msgs/TFMessage     [94810edda583a504dfda3829e70d7eec]\ntopics:      /sensors/camera/left/camera_info    540 msgs    : sensor_msgs/CameraInfo\n             /sensors/camera/left/image_raw      540 msgs    : sensor_msgs/Image     \n             /tf                                1801 msgs    : tf2_msgs/TFMessage\n\n"})}),"\n",(0,i.jsxs)(n.p,{children:["You can see that the rosbag has a duration of 18 seconds and contains 540 image frames of type ",(0,i.jsx)(n.code,{children:"sensor_msgs/Image"})," and 540 corresponding ",(0,i.jsx)(n.code,{children:"sensor_msgs/CameraInfo"})," messages. We will use these camera images in this assignment in order to apply image segmentation."]}),"\n",(0,i.jsxs)(n.h2,{id:"ros-sensor_msgsimage-message",children:["ROS' ",(0,i.jsx)(n.code,{children:"sensor_msgs/Image"})," Message"]}),"\n",(0,i.jsxs)(n.p,{children:["The message definition ",(0,i.jsx)(n.a,{href:"https://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/Image.html",children:"sensor_msgs/Image"})," is ROS' standard image message format. It is used for all kind of camera image message types and can be used seamlessly with many different ROS visualization and image processing tools. Please read the documentation about the ",(0,i.jsx)(n.a,{href:"https://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/Image.html",children:"detailed message format"})," and it's content."]}),"\n",(0,i.jsxs)(n.h2,{id:"ros-sensor_msgscamerainfo",children:["ROS' ",(0,i.jsx)(n.code,{children:"sensor_msgs/CameraInfo"})]}),"\n",(0,i.jsxs)(n.p,{children:["The message definition ",(0,i.jsx)(n.a,{href:"https://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/CameraInfo.html",children:"sensor_msgs/CameraInfo"})," is ROS' standard camera info message format. It is send together with ",(0,i.jsx)(n.code,{children:"sensor_msgs/Image"})," to provide additional information about the current camera image such as ",(0,i.jsx)(n.strong,{children:"camera calibration parameters"}),". Feel free to read the documentation about the ",(0,i.jsx)(n.a,{href:"https://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/CameraInfo.html",children:"detailed message format"}),"."]}),"\n",(0,i.jsx)(n.h2,{id:"build-and-source-the-package-1",children:"Build and source the package"}),"\n",(0,i.jsxs)(n.p,{children:["The code for the image segmentation inference node can be found in the directory ",(0,i.jsx)(n.code,{children:"workshops/section_2/image_segmentation_py"}),". The structure of this ",(0,i.jsx)(n.strong,{children:"Python package"})," is illustrated in the following:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"image_segmentation_py/\n\u251c\u2500\u2500 package.xml\n\u251c\u2500\u2500 CMakeLists.txt\n\u251c\u2500\u2500 launch\n\u2502\xa0\xa0 \u251c\u2500\u2500 params.yaml\n\u2502\xa0\xa0 \u2514\u2500\u2500 start_all.launch\n\u251c\u2500\u2500 models\n\u2502\xa0\xa0 \u251c\u2500\u2500 convert_cityscapes_to_ika_reduced.xml\n\u2502\xa0\xa0 \u251c\u2500\u2500 mobilenet_v3_large_968_608_os8.pb\n\u2502\xa0\xa0 \u2514\u2500\u2500 mobilenet_v3_small_968_608_os8.pb\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 image_segmentation.py\n    \u251c\u2500\u2500 img_utils.py\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The main source code is located in the directory ",(0,i.jsx)(n.code,{children:"src"}),", the pretrained segmentation models are located in ",(0,i.jsx)(n.code,{children:"models"})," and the launch file and parameters are located in directory ",(0,i.jsx)(n.code,{children:"launch"}),". Feel free to read all the code, parameters and launch files."]}),"\n",(0,i.jsx)(n.p,{children:"Note, that we provide here two image segmentation models for you. Both rely on the MobilnetV3 architecture:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"mobilenet_v3_large_968_608_os8.pb"}),": Larger model, slower inference, more RAM needed"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"mobilenet_v3_small_968_608_os8.pb"}),": Smaller model, faster inference, less RAM needed"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"These models are trained on a much larger dataset compared to the model you have trained in the exercise, but the overall training pipeline and additional augmentation methods applied during the training are almost identical."}),"\n",(0,i.jsxs)(n.p,{children:["You might change the model in the ",(0,i.jsx)(n.code,{children:"params.yaml"})," configuration file depending on the capabilities of your computer."]}),"\n",(0,i.jsxs)(n.p,{children:["Now, let's build the package with with ",(0,i.jsx)(n.code,{children:"catkin build"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"catkin build image_segmentation_py\n"})}),"\n",(0,i.jsx)(n.p,{children:"and source the workspace"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"source devel/setup.bash\n"})}),"\n",(0,i.jsx)(n.p,{children:"Perfect! Now you will be able to perform inference on camera images with this package. Let's go to the next section."}),"\n",(0,i.jsx)(n.h2,{id:"replay-rosbag-and-run-image-segmentation-1",children:"Replay rosbag and run image segmentation"}),"\n",(0,i.jsx)(n.p,{children:"We have already prepared a launch file for you to execute the image segmentation. Please read carefully through the following lines of code."}),"\n",(0,i.jsxs)(n.p,{children:["Contents of the file ",(0,i.jsx)(n.code,{children:"start_all.launch"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<launch>\n    <param name ="/use_sim_time" value="true"/>\n    \n    \x3c!-- ROSBAG PLAY --\x3e\n    <node pkg="rosbag" \n          type="play"\n          name="player"\n          output="screen"\n          args="--rate 0.05 -s 0 --clock /home/rosuser/bag/left_camera_templergraben.bag\n                --topics /sensors/camera/left/image_raw\n                         /sensors/camera/left/camera_info">\n    </node>\n\n    \x3c!-- STEREO IMAGE PROC --\x3e\n    <node name="stereo_image_proc"\n          pkg="stereo_image_proc"\n          type="stereo_image_proc"\n          ns="sensors/camera/"\n          output="screen">\n    </node>\n\n\n    \x3c!--- CAMERA SEGMENTATION NODE --\x3e\n    <rosparam\n      command="load"\n      file="$(find image_segmentation_py)/launch/params.yaml"\n    />\n\n    <node\n        name="image_segmentation"\n        pkg="image_segmentation_py"\n        type="image_segmentation.py"\n        output="screen">\n        <remap from="/image_rect_color" to="/sensors/camera/left/image_rect_color"/>\n    </node>\n\n\n    \x3c!--- NODES FOR VISUALIZATION --\x3e\n    <node pkg="image_view"\n          type="image_view"\n          name="segmentation_viewer"\n          args="image:=/image_rect_segmented">\n    </node>\n\n    <node pkg="image_view"\n          type="image_view"\n          name="camera_left"\n          args="image:=/sensors/camera/left/image_rect_color">\n    </node>\n</launch>\n'})}),"\n",(0,i.jsx)(n.p,{children:"Hence, we perform the following tasks:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Replay the rosbag"})," with a speed of 0.05. Note, that we set the speed to a very low value here, because your computer might be very slow. You can adapt this values if your computer is fast enough to compute the segmentation at a higher speed."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Apply ",(0,i.jsx)(n.code,{children:"stereo_image_proc"})]})," to the raw sensor data. The topic ",(0,i.jsx)(n.code,{children:"/sensors/camera/left/image_raw"})," was recorded in the raw data format. With ",(0,i.jsx)(n.code,{children:"stereo_image_proc"})," we convert it to a RGB encoding. Read more ",(0,i.jsx)(n.a,{href:"https://wiki.ros.org/stereo_image_proc",children:"here"})," about it."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Load the parameters"})," that are necessary for the ",(0,i.jsx)(n.code,{children:"image_segmentation_py"})," node"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Start the ",(0,i.jsx)(n.code,{children:"image_segmentation_py"})]})," node and feed it with the correct topic name"]}),"\n",(0,i.jsxs)(n.li,{children:["Start two ",(0,i.jsx)(n.strong,{children:"nodes for visualization"})]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Note: You can also use RVIZ to visualize the RGB camera image and the segmented camera image."}),"\n",(0,i.jsx)(n.p,{children:"We can now start the launch file with:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"roslaunch image_segmentation_py start_all.launch\n"})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"image_view"})," node should show you directly the camera image as shown in this image:"]}),"\n",(0,i.jsx)("img",{src:"../images/96116c6f4b59104e706cb4b425d7afad/image.png",alt:"Description of image"}),"\n",(0,i.jsx)(n.p,{children:"However, the segmented image looks like this"}),"\n",(0,i.jsx)("img",{src:"../images/6a552394d42276e58d491197bad6547d/image.png",alt:"Description of image"}),"\n",(0,i.jsx)(n.p,{children:"Something is wrong. We have apparently a bug in the code !!! Let's solve this problem."}),"\n",(0,i.jsx)(n.h2,{id:"review-of-file-image_segmentationpy-1",children:"Review of file image_segmentation.py"}),"\n",(0,i.jsxs)(n.p,{children:["Before we start with the task, let's try to understand what happens in the file ",(0,i.jsx)(n.a,{href:"https://github.com/ika-rwth-aachen/acdc/blob/main/catkin_workspace/src/workshops/section_2/image_segmentation_py/src/image_segmentation.py",children:(0,i.jsx)(n.code,{children:"image_segmentation.py"})}),"."]}),"\n",(0,i.jsxs)(n.p,{children:["The file ",(0,i.jsx)(n.code,{children:"image_segmentation.py"})," contains the inference node for the image segmentation task. The inference node is implemented as a Python class called ",(0,i.jsx)(n.code,{children:"ImageSegmentation"}),". The class has the following member functions. We will give here a short description of each class so you can understand what each class is doing."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"class ImageSegmentation"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["Class which implements image segmentation inference applied on images send as ROS MSG of type ",(0,i.jsx)(n.code,{children:"sensor_msgs/Image"})]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"__init__(self)"})})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["Initializes the class by initializing it with a ROS Node. Calls ",(0,i.jsx)(n.code,{children:"setup()"})," and ",(0,i.jsx)(n.code,{children:"load_parameters()"})," to load the model and necessary parameters."]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"load_parameters(self)"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Loads the ROS params and stores them into the current instance of ImageSegmentation"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"setup(self)"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["Loads the image segmentation model which are stored in the directory ",(0,i.jsx)(n.code,{children:"models"}),". Also creates a ",(0,i.jsx)(n.code,{children:"cv_bridge"})," which allows to convert ",(0,i.jsx)(n.code,{children:"sensor_msgs/Image"})," into an image which can be processed by the neural network. Also registers subscribers and publishers."]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"predict(self, img_rect_color_msg)"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["This is the so called callback function. This function is triggered, when the subscriber ",(0,i.jsx)(n.code,{children:"self.sub_image"})," receives a message on topic ",(0,i.jsx)(n.code,{children:'"/image_rect_color"'}),". This function performs the actual inference of the neural network. It converts the ",(0,i.jsx)(n.code,{children:"sensor_msgs/Image"})," message into a format that can be processed by the neural network, then performs the inference and then converts the output, a segmentation maps, into a RGB encoding which is then send as an image using publisher ",(0,i.jsx)(n.code,{children:"self.pub_seg"})]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"load_frozen_graph(path_to_frozen_graph)"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["Takes the path to one of the models stored in the directory ",(0,i.jsx)(n.code,{children:"models"})," and converts the frozen graph into an executable Python function. Uses helper function ",(0,i.jsx)(n.code,{children:"wrap_frozen_graph()"})]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"wrap_frozen_graph(graph_def, inputs, outputs, print_graph=False)"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"Helper function that converts a frozen graph, which is a type how a neural network can be stored using Tensorflow, into an executable Python function"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"segmentation_map_to_rgb(segmentation_map)"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"A function which converts a segmentation map into RGB encoded image"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"parse_convert_xml(conversion_file_path)"})}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["Reads a xml file which is located in ",(0,i.jsx)(n.code,{children:"models"})," and which contains information which class ID is associated with which RGB value and vice versa. It processes the xml file in such a way that it can be used to convert a segmentation map in to an RGB encoded image."]})}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"task-1-implement-conversion-from-segmentation-map-to-rgb-encoding-1",children:"Task 1: Implement conversion from segmentation map to RGB encoding"}),"\n",(0,i.jsxs)(n.p,{children:["Unfortunately, the function ",(0,i.jsx)(n.code,{children:"segmentation_map_to_rgb()"})," in the file ",(0,i.jsx)(n.a,{href:"https://github.com/ika-rwth-aachen/acdc/blob/main/catkin_workspace/src/workshops/section_2/image_segmentation_py/src/image_segmentation.py",children:(0,i.jsx)(n.code,{children:"image_segmentation.py"})})," wasn't implemented correctly. Open this file with your favorite code editor and let's have a look on this function."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def segmentation_map_to_rgb(self, segmentation_map):\n    """\n    Converts segmentation map to a RGB encoding according to self.color_palette\n    Eg. 0 (Class 0) -> Pixel value [128, 64, 128] which is on index 0 of self.color_palette\n        1 (Class 1) -> Pixel value [244, 35, 232] which is on index 1 of self.color_palette\n\n    self.color_palette has shape [256, 3]. Each index of the first dimension is associated\n    with an RGB value. The index corresponds to the class ID.\n\n    :param segmentation_map: ndarray numpy with shape (height, width)\n    :return: RGB encoding with shape (height, width, 3)\n    """\n    ### START CODE HERE ###\n    \n    # Replace the following command\n    rgb_encoding = np.random.randint(\n        low=0,\n        high=255,\n        size=[self.resize_height, self.resize_width, 3]\n    )\n\n    ### END CODE HERE ###\n    return rgb_encoding\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Instead of computing the RGB encoding from the segmentation map, the function generates only a random image. ",(0,i.jsx)(n.strong,{children:"Your task"})," is now to implement the function correctly.  Note that ",(0,i.jsx)(n.code,{children:"self.color_palette"})," is here not a function parameter but a class attribute of the class ",(0,i.jsx)(n.code,{children:"ImageSegmentation"}),"."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Hints"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"There are several approaches how to convert from segmentation map to the color encoding"}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Loop approach:"})," Iterate over all class IDs in segmentation map and retrieve the corresponding RGB value for each class and place this triplet (R,G,B) at the correct location of the returned RGB image."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Advanced vectorized approach:"})," Should be the faster and more efficient implementation. Avoid using a loop, but rather rely on numpy's vectorized indexing operations!"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"expected-output-1",children:"Expected output"}),"\n",(0,i.jsxs)(n.p,{children:["After fixing the function ",(0,i.jsx)(n.code,{children:"segmentation_map_to_rgb()"}),", you will see that the inference node now will publish correct RGB encoded segmentations of the camera image. You will obtain segmented images as shown here:"]}),"\n",(0,i.jsx)("img",{src:"../images/uploads/f3f5b8942b8d2a32363ca9560bafdf14/image.png",alt:"Description of image"}),"\n",(0,i.jsx)(n.h2,{id:"wrap-up-1",children:"Wrap-up"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"You learned about the ROS definitions for camera images and camera info messages"}),"\n",(0,i.jsx)(n.li,{children:"You learned about a simple Python ROS package for semantic image segmentation"}),"\n",(0,i.jsx)(n.li,{children:"You learned about encoding the segmentation map to the RGB encoding"}),"\n",(0,i.jsxs)(n.li,{children:["You learned about ",(0,i.jsx)(n.code,{children:"image_view"}),", a node for visualizing image data"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},6366:(e,n,s)=>{s.d(n,{A:()=>a});const a=s.p+"assets/images/city-9e40a2d7d1cc1eb07396b4786905ae0b.png"},1802:(e,n,s)=>{s.d(n,{A:()=>a});const a=s.p+"assets/images/image_rect_segmented-c427d0ce5043e23c8aa84456efafd8b2.png"},9167:(e,n,s)=>{s.d(n,{A:()=>a});const a=s.p+"assets/images/image_segmentation-b35830485d68cc7ff142254e2516ebe7.png"},7164:(e,n,s)=>{s.d(n,{A:()=>a});const a=s.p+"assets/images/sem_seg-29a08be97f9d5afe2713b77125c33dee.png"},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>r});var a=s(6540);const i={},t=a.createContext(i);function o(e){const n=a.useContext(t);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);