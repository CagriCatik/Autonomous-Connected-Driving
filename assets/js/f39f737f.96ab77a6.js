"use strict";(self.webpackChunkacd=self.webpackChunkacd||[]).push([[4098],{546:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>t,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"task/sensor_data_processing/Camera-based-Semantic-Grid-Mapping","title":"Camera-based Semantic Grid Mapping","description":"ROS2","source":"@site/docs/task/02_sensor_data_processing/01_Camera-based-Semantic-Grid-Mapping.md","sourceDirName":"task/02_sensor_data_processing","slug":"/task/sensor_data_processing/Camera-based-Semantic-Grid-Mapping","permalink":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Camera-based-Semantic-Grid-Mapping","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/task/02_sensor_data_processing/01_Camera-based-Semantic-Grid-Mapping.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"taskSidebar","previous":{"title":"Sensor Data Processing","permalink":"/Autonomous-Connected-Driving/docs/category/sensor-data-processing"},"next":{"title":"Deep Learning-based Point Cloud Occupancy Grid Mapping","permalink":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Deep-OGM"}}');var r=s(4848),i=s(8453);const t={},o="Camera-based Semantic Grid Mapping",c={},d=[{value:"Perform Camera-based Semantic Grid Mapping using geometry-based inverse perspective mapping",id:"perform-camera-based-semantic-grid-mapping-using-geometry-based-inverse-perspective-mapping",level:2},{value:"Introduction to this workshop",id:"introduction-to-this-workshop",level:2},{value:"ROS2&#39;s <code>sensor_msgs/msg/Image</code> Message",id:"ros2s-sensor_msgsmsgimage-message",level:2},{value:"ROS2&#39;s <code>sensor_msgs/msg/CameraInfo</code> Message",id:"ros2s-sensor_msgsmsgcamerainfo-message",level:2},{value:"Task 1: Explore the semantic grid mapping package and build and source the package",id:"task-1-explore-the-semantic-grid-mapping-package-and-build-and-source-the-package",level:2},{value:"Task 2: Replay rosbag and run the camera-based grid mapping node",id:"task-2-replay-rosbag-and-run-the-camera-based-grid-mapping-node",level:2},{value:"Task 3: Synchronize the subscribers",id:"task-3-synchronize-the-subscribers",level:2},{value:"Task 4: Extract the camera intrinsic matrix from the CameraInfo message",id:"task-4-extract-the-camera-intrinsic-matrix-from-the-camerainfo-message",level:2},{value:"hints:",id:"hints",level:4},{value:"Task 5: Calculate the camera extrinsic matrix",id:"task-5-calculate-the-camera-extrinsic-matrix",level:2},{value:"Hints:",id:"hints-1",level:4},{value:"Result",id:"result",level:2},{value:"Wrap-up",id:"wrap-up",level:2},{value:"ROS1 Instructions",id:"ros1-instructions",level:2},{value:"Perform Camera-based Semantic Grid Mapping using geometry-based inverse perspective mapping",id:"perform-camera-based-semantic-grid-mapping-using-geometry-based-inverse-perspective-mapping-1",level:2},{value:"Introduction to this workshop",id:"introduction-to-this-workshop-1",level:2},{value:"ROS <code>sensor_msgs/Image</code> Message",id:"ros-sensor_msgsimage-message",level:3},{value:"ROS <code>sensor_msgs/CameraInfo</code>",id:"ros-sensor_msgscamerainfo",level:3},{value:"Task 1: Explore the semantic grid mapping package and build and source the package",id:"task-1-explore-the-semantic-grid-mapping-package-and-build-and-source-the-package-1",level:2},{value:"Task 2: Replay rosbag and run the camera-based grid mapping node",id:"task-2-replay-rosbag-and-run-the-camera-based-grid-mapping-node-1",level:2},{value:"Task 3: Synchronize the subscribers",id:"task-3-synchronize-the-subscribers-1",level:2},{value:"Task 4: Extract the camera intrinsic matrix from the CameraInfo message",id:"task-4-extract-the-camera-intrinsic-matrix-from-the-camerainfo-message-1",level:2},{value:"hints:",id:"hints-2",level:4},{value:"Task 5: Calculate the camera extrinsic matrix",id:"task-5-calculate-the-camera-extrinsic-matrix-1",level:2},{value:"Hints:",id:"hints-3",level:4},{value:"Result",id:"result-1",level:2},{value:"Wrap-up",id:"wrap-up-1",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"camera-based-semantic-grid-mapping",children:"Camera-based Semantic Grid Mapping"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://img.shields.io/badge/ROS2-red",alt:"ROS2"})}),"\n",(0,r.jsx)(n.h2,{id:"perform-camera-based-semantic-grid-mapping-using-geometry-based-inverse-perspective-mapping",children:"Perform Camera-based Semantic Grid Mapping using geometry-based inverse perspective mapping"}),"\n",(0,r.jsxs)(n.p,{children:["In this workshop, we will perform ",(0,r.jsx)(n.strong,{children:"semantic grid mapping"})," based on images taken by vehicle-mounted cameras.\nThe approach will make use of inverse perspective mapping (IPM)."]}),"\n",(0,r.jsx)(n.p,{children:"We will use a recording from a simulation containing images from eight cameras."}),"\n",(0,r.jsx)(n.p,{children:"The learning goals of this workshop are ..."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Inspecting a rosbag which contains camera data"}),"\n",(0,r.jsx)(n.li,{children:"Learn about the ROS2 message format for camera images and camera infos"}),"\n",(0,r.jsx)(n.li,{children:"Learn about synchronized subscribers"}),"\n",(0,r.jsx)(n.li,{children:"Learn how to use the tf2 library"}),"\n",(0,r.jsx)(n.li,{children:"Learn how to visualize the output of semantic grid mapping"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"introduction-to-this-workshop",children:"Introduction to this workshop"}),"\n",(0,r.jsx)(n.p,{children:"We prepared a rosbag with camera data for you to use."}),"\n",(0,r.jsxs)(n.p,{children:["Download the file ",(0,r.jsx)(n.code,{children:"semantic_8_cams.db3"})," from ",(0,r.jsx)(n.a,{href:"https://rwth-aachen.sciebo.de/s/Fzsu2uLpqApG1ru",children:(0,r.jsx)(n.strong,{children:"here (684.1 MB)"})}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["Save this file to your local directory ",(0,r.jsx)(n.code,{children:"${REPOSITORY}/bag"}),". This directory will be mounted into the docker container to the path ",(0,r.jsx)(n.code,{children:"/home/rosuser/ws/bag"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["After the download, navigate to the local directory ",(0,r.jsx)(n.code,{children:"${REPOSITORY}/docker"})," on your host and execute ",(0,r.jsx)(n.code,{children:"./ros2_run.sh"})," to start the ACDC docker container."]}),"\n",(0,r.jsxs)(n.p,{children:["Inside the container, you can navigate to ",(0,r.jsx)(n.code,{children:"/home/rosuser/ws/bag"})," and execute ",(0,r.jsx)(n.code,{children:"ros2 bag info semantic_8_cams.db3"})," to inspect the rosbag:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Files:             semantic_8_cams.db3\nBag size:          684.1 MiB\nStorage id:        sqlite3\nDuration:          7.450s\nStart:             Jan  1 1970 01:00:26.592 (26.592)\nEnd:               Jan  1 1970 01:00:34.42 (34.42)\nMessages:          4622\n\nTopic information: Topic: /carla/ego_vehicle/rgb_view/camera_info | Type: sensor_msgs/msg/CameraInfo | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/rgb_view/image | Type: sensor_msgs/msg/Image | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/semantic_segmentation_back_left_1/camera_info | Type: sensor_msgs/msg/CameraInfo | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/semantic_segmentation_back_left_1/image | Type: sensor_msgs/msg/Image | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/semantic_segmentation_back_left_2/camera_info | Type: sensor_msgs/msg/CameraInfo | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/semantic_segmentation_back_left_2/image | Type: sensor_msgs/msg/Image | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/semantic_segmentation_back_right_1/camera_info | Type: sensor_msgs/msg/CameraInfo | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/semantic_segmentation_back_right_1/image | Type: sensor_msgs/msg/Image | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/semantic_segmentation_back_right_2/camera_info | Type: sensor_msgs/msg/CameraInfo | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/semantic_segmentation_back_right_2/image | Type: sensor_msgs/msg/Image | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/semantic_segmentation_front_left_1/camera_info | Type: sensor_msgs/msg/CameraInfo | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/semantic_segmentation_front_left_1/image | Type: sensor_msgs/msg/Image | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/semantic_segmentation_front_left_2/camera_info | Type: sensor_msgs/msg/CameraInfo | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/semantic_segmentation_front_left_2/image | Type: sensor_msgs/msg/Image | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/semantic_segmentation_front_right_1/camera_info | Type: sensor_msgs/msg/CameraInfo | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/semantic_segmentation_front_right_1/image | Type: sensor_msgs/msg/Image | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/semantic_segmentation_front_right_2/camera_info | Type: sensor_msgs/msg/CameraInfo | Count: 149 | Serialization Format: cdr\n\nTopic: /carla/ego_vehicle/semantic_segmentation_front_right_2/image | Type: sensor_msgs/msg/Image | Count: 149 | Serialization Format: cdr\n\nTopic: /clock | Type: rosgraph_msgs/msg/Clock | Count: 150 | Serialization Format: cdr\n\nTopic: /rosout | Type: rosgraph_msgs/msg/Log | Count: 2 | Serialization Format: cdr\n\nTopic: /tf | Type: tf2_msgs/msg/TFMessage | Count: 1788 | Serialization Format: cdr\n"})}),"\n",(0,r.jsxs)(n.p,{children:["You can see that the rosbag has a duration of 7.5 seconds and contains segmented images of type ",(0,r.jsx)(n.code,{children:"sensor_msgs/msg/Image"})," and corresponding ",(0,r.jsx)(n.code,{children:"sensor_msgs/msg/CameraInfo"})," messages.\nWe will use these camera images in this assignment in order to apply semantic grid mapping."]}),"\n",(0,r.jsxs)(n.h2,{id:"ros2s-sensor_msgsmsgimage-message",children:["ROS2's ",(0,r.jsx)(n.code,{children:"sensor_msgs/msg/Image"})," Message"]}),"\n",(0,r.jsxs)(n.p,{children:["The message definition ",(0,r.jsx)(n.a,{href:"https://docs.ros2.org/latest/api/sensor_msgs/msg/Image.html",children:"sensor_msgs/msg/Image"})," is ROS2's standard image message format. It is used for all kind of camera image message types and can be used seamlessly with many different ROS2 visualization and image processing tools. Please read the documentation about the ",(0,r.jsx)(n.a,{href:"https://docs.ros2.org/latest/api/sensor_msgs/msg/Image.html",children:"detailed message format"})," and it's content.\nMessage"]}),"\n",(0,r.jsxs)(n.h2,{id:"ros2s-sensor_msgsmsgcamerainfo-message",children:["ROS2's ",(0,r.jsx)(n.code,{children:"sensor_msgs/msg/CameraInfo"})," Message"]}),"\n",(0,r.jsxs)(n.p,{children:["The message definition ",(0,r.jsx)(n.a,{href:"hhttps://docs.ros2.org/latest/api/sensor_msgs/msg/CameraInfo.html",children:"sensor_msgs/msg/CameraInfo"})," is ROS2's standard camera info message format. It is send together with ",(0,r.jsx)(n.code,{children:"sensor_msgs/msg/Image"})," to provide additional information about the current camera image such as ",(0,r.jsx)(n.strong,{children:"camera calibration parameters"}),". Feel free to read the documentation about the ",(0,r.jsx)(n.a,{href:"https://docs.ros2.org/latest/api/sensor_msgs/msg/CameraInfo.html",children:"detailed message format"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"task-1-explore-the-semantic-grid-mapping-package-and-build-and-source-the-package",children:"Task 1: Explore the semantic grid mapping package and build and source the package"}),"\n",(0,r.jsxs)(n.p,{children:["The code for the camera-based semantic grid mapping node can be found in the directory ",(0,r.jsx)(n.code,{children:"colcon_workspace_src/section_2/camera_based_semantic_grid_mapping_r2"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["The main source code is located in the directory ",(0,r.jsx)(n.code,{children:"camera_based_semantic_grid_mapping_r2"}),". The launch file are located in directory ",(0,r.jsx)(n.code,{children:"launch"}),"and parameters are located in ",(0,r.jsx)(n.code,{children:"config"}),". Feel free to read all the code, parameters and launch files."]}),"\n",(0,r.jsxs)(n.p,{children:["Now, in the container, navigate to ",(0,r.jsx)(n.code,{children:"colcon_workspace"})," and build the package with with ",(0,r.jsx)(n.code,{children:"colcon build"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"colcon build --packages-select camera_based_semantic_grid_mapping_r2 --symlink-install\n"})}),"\n",(0,r.jsx)(n.p,{children:"and source the workspace"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"source install/setup.bash\n"})}),"\n",(0,r.jsx)(n.p,{children:"Perfect! Now you will be able to perform semantic grid mapping using camera images with this package. Let's go to the next task."}),"\n",(0,r.jsx)(n.h2,{id:"task-2-replay-rosbag-and-run-the-camera-based-grid-mapping-node",children:"Task 2: Replay rosbag and run the camera-based grid mapping node"}),"\n",(0,r.jsx)(n.p,{children:"We have already prepared a launch file for you to execute the camera-based semantic grid mapping package. Please read through the following lines of code carefully."}),"\n",(0,r.jsxs)(n.p,{children:["Contents of the file ",(0,r.jsx)(n.code,{children:"semantic_grid_mapping.launch.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-py",children:"import os\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument, ExecuteProcess\nfrom ament_index_python.packages import get_package_share_directory\n\ndef generate_launch_description():\n\n    # Get the package and params directory\n    semantic_grid_mapping_dir = get_package_share_directory('camera_based_semantic_grid_mapping_r2')\n    config = os.path.join(semantic_grid_mapping_dir, 'config', 'params.yaml')\n\n    # Declare launch arguments\n    use_sim_time = DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='true',\n        description='Use simulation clock time')\n\n    # ROSBAG PLAY node\n    rosbag_play_node = ExecuteProcess(\n        cmd=['ros2', 'bag', 'play', '--rate', '0.1', '-l',\n             '/home/rosuser/bag/semantic_8_cams.db3'],\n        output='screen'\n    )\n\n    # SEMANTIC GRID MAPPING NODE\n    semantic_grid_mapping_node = Node(\n        package='camera_based_semantic_grid_mapping_r2',\n        name='camera_based_semantic_grid_mapping',\n        executable='semantic_grid_mapping',\n        output='screen',\n        parameters=[config]\n    )\n\n    # NODES FOR VISUALIZATION\n    front_left_segmented_image_node = Node(\n        package='image_view',\n        executable='image_view',\n        name='front_left_segmented_image',\n        remappings=[('image', 'carla/ego_vehicle/semantic_segmentation_front_left_1/image')],\n    )\n\n    front_right_segmented_image_node = Node(\n        package='image_view',\n        executable='image_view',\n        name='front_right_segmented_image',\n        remappings=[('image', 'carla/ego_vehicle/semantic_segmentation_front_right_1/image')],\n    )\n\n    segmentation_viewer_node = Node(\n        package='image_view',\n        executable='image_view',\n        name='segmentation_viewer',\n        remappings=[('image', 'BEV_image')],\n            parameters=[\n                {'autosize': True},\n        ]\n    )\n\n    # Create the launch description and populate\n    ld = LaunchDescription()\n\n    # Add the actions to the launch description\n    ld.add_action(use_sim_time)\n    ld.add_action(rosbag_play_node)\n    ld.add_action(semantic_grid_mapping_node)\n    ld.add_action(front_left_segmented_image_node)\n    ld.add_action(front_right_segmented_image_node)\n    ld.add_action(segmentation_viewer_node)\n\n    return ld\n\n"})}),"\n",(0,r.jsx)(n.p,{children:"Hence, we perform the following tasks:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Replay the rosbag"})," with a speed of 0.1. Note, that we set the speed to a very low value here, because your computer might be very slow. You can increase this value if your computer is fast enough to compute the semantic grid map at a higher speed. If necessary, you may also reduce the value."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsxs)(n.strong,{children:["Start the ",(0,r.jsx)(n.code,{children:"camera_based_semantic_grid_mapping "})," Node"]})," and load the parameters(camera image and info topic names, output image size etc.) that are necessary for the node."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Start a visualization node"})," to show the result"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["The node ",(0,r.jsx)(n.code,{children:"camera_based_semantic_grid_mapping "})," performs the following tasks on the data provided in the bag file:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"subscribe"})," to 8 ",(0,r.jsx)(n.code,{children:"CameraInfo"})," and 8 ",(0,r.jsx)(n.code,{children:"Image"})," topics"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"synchronize the subscribers"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:["apply ",(0,r.jsx)(n.code,{children:"inverse perspective mapping"})]})," on single images."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"stitches"}),"  the result into a BEV image"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"We can now start the launch file with:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 launch camera_based_semantic_grid_mapping_r2 semantic_grid_mapping.launch.py \n"})}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"image_view"})," node should show you the semantically segmented images of the right and left forward facing cameras. They are sufficient for testing the node, for now."]}),"\n",(0,r.jsx)("img",{src:"../images/visualization.PNG",alt:"Description of image"}),"\n",(0,r.jsx)(n.p,{children:"However, the Bird's Eye View (BEV) image is not shown. This is because the node doesn't yet subscribe to the correct images and cameras info topics yet."}),"\n",(0,r.jsxs)(n.p,{children:["In the following tasks, you will need to modify code in ",(0,r.jsx)(n.strong,{children:'"semantic_grid_mapping.py"'}),". After finishing all tasks, you will have completed the semantic grid mapping node."]}),"\n",(0,r.jsx)(n.h2,{id:"task-3-synchronize-the-subscribers",children:"Task 3: Synchronize the subscribers"}),"\n",(0,r.jsx)(n.p,{children:"As we want to combine images from multiple cameras to produce a BEV image, these images need to be from the same time step."}),"\n",(0,r.jsxs)(n.p,{children:["For the purpose of synchronizing the image topics, we will use ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"message_filters"})}),"."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"message_filters"})," is a utility library that provides filtering algorithms for commonly used messages."]}),"\n",(0,r.jsxs)(n.p,{children:["The following lines of code in ",(0,r.jsx)(n.strong,{children:'"semantic_grid_mapping.py"'})," are responsible for subscribing to the different ",(0,r.jsx)(n.code,{children:"CameraInfo"})," and ",(0,r.jsx)(n.code,{children:"Image"})," topics."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'PLACE_HOLDER_1 = "NONEXISTENT_CAMERA_IMAGE_TOPIC"\nPLACE_HOLDER_2 = "NONEXISTENT_CAMERA_INFO_TOPIC"\n# setup subscribers\nsubs = []  # array with all subscribers that should be synchronized\nfor image_topic, info_topic in zip(self.image_topics_in, self.info_topics_in):\n    ### START Task 3 CODE HERE ###\n    # create subscriber for topic\n    image_sub = message_filters.Subscriber(self, Image,PLACE_HOLDER_1, qos_profile=qos_profile)\n    # create a subscriber for camera info topic\n    info_sub = message_filters.Subscriber(self, CameraInfo, PLACE_HOLDER_2, qos_profile=qos_profile)\n    ### END Task 3 CODE HERE ###\n    # add subscribers to array\n    subs.append(image_sub)\n    subs.append(info_sub)\n\n# synchronized subscriber\nself.sync_sub = message_filters.ApproximateTimeSynchronizer(subs, queue_size=5., slop=0.01)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["In the above code snippet, we iterate over all ",(0,r.jsx)(n.code,{children:"Image"})," topics and their corresponding ",(0,r.jsx)(n.code,{children:"CameraInfo"}),". These are defined in ",(0,r.jsx)(n.code,{children:"config/params.yaml"})," und loaded in the node's function ",(0,r.jsx)(n.code,{children:"load_parameters()"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["Note that we use ",(0,r.jsx)(n.code,{children:"message_filters.Subscriber()"})," instead of normal ROS subscribers."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"message_filters"})," provides subscriber policies that allow subscribing to messages from multiple topics and outputting them in a single callback. Here, we will use ",(0,r.jsx)(n.code,{children:"message_filters.ApproximateTimeSynchronizer()"}),". The policy synchronizes the topics based on their time stamp (given in the header of the messages). It is capable of synchronizing topics even if messages' time stamps differ from each other. More information can be found in the ",(0,r.jsx)(n.a,{href:"https://wiki.ros.org/message_filters",children:"documentation"}),", ",(0,r.jsx)(n.a,{href:"https://github.com/ros2/message_filters.git",children:"ROS2 documentation"}),". There also exist an ",(0,r.jsx)(n.code,{children:"ExactTime Policy."})," , which is not covered here."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"message_filters.ApproximateTimeSynchronizer()"})," takes three parameters:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["an array of subscribers (we save our subscribers in the array ",(0,r.jsx)(n.strong,{children:"subs"}),")"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"queue_size"})," and  ",(0,r.jsx)(n.strong,{children:"slop"})," (we use  ",(0,r.jsx)(n.strong,{children:"queue_size=5"})," and  ",(0,r.jsx)(n.strong,{children:"slop=0.01"}),")"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"slop"})," parameter defines the maximum delay (in seconds) with which messages can be synchronized."]}),"\n",(0,r.jsxs)(n.p,{children:["Your task is to ",(0,r.jsx)(n.strong,{children:"replace"})," ",(0,r.jsx)(n.code,{children:"PLACE_HOLDER_1"})," and ",(0,r.jsx)(n.code,{children:"PLACE_HOLDER_2"})," with the correct variables to subscribe to the ",(0,r.jsx)(n.code,{children:"Image"}),"and ",(0,r.jsx)(n.code,{children:"CameraInfo"})," topics."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# create subscriber for topic\n    image_sub = message_filters.Subscriber(self, Image,PLACE_HOLDER_1, qos_profile=qos_profile)\n    # create a subscriber for camera info topic\n    info_sub = message_filters.Subscriber(self, CameraInfo, PLACE_HOLDER_2, qos_profile=qos_profile)\n"})}),"\n",(0,r.jsxs)(n.p,{children:["After this adjustment, you can ",(0,r.jsx)(n.strong,{children:"rerun the node"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 launch camera_based_semantic_grid_mapping_r2 semantic_grid_mapping.launch.py \n"})}),"\n",(0,r.jsx)(n.p,{children:"A visualization window for the BEV image should appear.\nHowever, the BEV image looks like this:"}),"\n",(0,r.jsx)("img",{src:"../images/bev_wrong.PNG",alt:"Description of image"}),"\n",(0,r.jsx)(n.p,{children:"Some parts of the code are still missing!"}),"\n",(0,r.jsx)(n.h2,{id:"task-4-extract-the-camera-intrinsic-matrix-from-the-camerainfo-message",children:"Task 4: Extract the camera intrinsic matrix from the CameraInfo message"}),"\n",(0,r.jsxs)(n.p,{children:["The goal of this task is to get the intrinsic matrices ",(0,r.jsx)(n.code,{children:"K"})," of the cameras."]}),"\n",(0,r.jsxs)(n.p,{children:["For this purpose, you have to modify the following lines in the function ",(0,r.jsx)(n.strong,{children:"compute_bev"})," by ",(0,r.jsx)(n.strong,{children:"replacing"})," ",(0,r.jsx)(n.code,{children:"PLACE_HOLDER_INTRINSIC"})," with your code."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# extract intrinsic matrix k (3x3) from CameraInfo topic\nPLACE_HOLDER_INTRINSIC = [100., 0., 0., 0., 100., 0., 0., 0., 100.] # comment this line in your solution\nK = np.reshape(PLACE_HOLDER_INTRINSIC, (3,3)) # replace the placeholder and use the actual camera intrinsics\n"})}),"\n",(0,r.jsx)(n.h4,{id:"hints",children:"hints:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The ",(0,r.jsx)(n.code,{children:"CameraInfo"})," message is provided in the variable ",(0,r.jsx)(n.code,{children:"cam_info_msg"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["In the ",(0,r.jsx)(n.strong,{children:"CameraInfo"})," message, the intrinsic matrix is named ",(0,r.jsx)(n.strong,{children:"k"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"task-5-calculate-the-camera-extrinsic-matrix",children:"Task 5: Calculate the camera extrinsic matrix"}),"\n",(0,r.jsxs)(n.p,{children:["The goal of this task is to compute extrinsic matrices of the cameras based on information in the ",(0,r.jsx)(n.code,{children:"CameraInfo"})," messages."]}),"\n",(0,r.jsxs)(n.p,{children:["The extrinsic matrix transforms the camera's coordinate frame to the vehicle's coordinates frame.\nFor this, we will use ros ",(0,r.jsx)(n.a,{href:"https://wiki.ros.org/tf2",children:(0,r.jsx)(n.strong,{children:"tf2"})}),". This library allows to maintain multiple coordinate frames and their relationship over time."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Part 1: Look up the transformation between the vehicle's base link and the camera frames"})}),"\n",(0,r.jsxs)(n.p,{children:["To get the transformation between coordinate frames, you will use the function ",(0,r.jsx)(n.a,{href:"http://docs.ros.org/en/noetic/api/tf2_ros/html/c++/classtf2__ros_1_1Buffer.html",children:(0,r.jsx)(n.code,{children:"tfBuffer.lookup_transform()"})})," which takes a target frame and a source frame as parameters and returns the transformation between them.\nReplace the ",(0,r.jsx)(n.code,{children:"None"})," placeholders in the function ",(0,r.jsx)(n.code,{children:"compute_bev()"})," in ",(0,r.jsx)(n.strong,{children:'"semanctic_grid_mapping.py"'}),", then uncomment the line to define ",(0,r.jsx)(n.code,{children:"transform"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# use tfBuffer to look up the transformation from the vehicle's base link frame to the camera frame.\n# transform = self.tfBuffer.lookup_transform(None, None, None) # uncomment and adjust\n"})}),"\n",(0,r.jsx)(n.h4,{id:"hints-1",children:"Hints:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The vehicle's base link frame name is saved in ",(0,r.jsx)(n.code,{children:"self.vehicle_base_link"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["The camera frame name can be found in the ",(0,r.jsx)(n.code,{children:"CameraInfo"})," topic."]}),"\n",(0,r.jsxs)(n.li,{children:["Look up the transformation at the common time ",(0,r.jsx)(n.code,{children:"common_time"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["The function ",(0,r.jsx)(n.code,{children:"lookup_transform()"})," can take four arguments: ",(0,r.jsx)(n.code,{children:"target_frame"}),", ",(0,r.jsx)(n.code,{children:"source_frame"}),", ",(0,r.jsx)(n.code,{children:"time"})," at which the value of the transform is desired, and an optional ",(0,r.jsx)(n.code,{children:"timeout"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Part 2: Convert a Transform into a homogeneous matrix"})}),"\n",(0,r.jsxs)(n.p,{children:["The function ",(0,r.jsx)(n.code,{children:"tfBuffer.lookup_transform()"})," returns a transform of the type ",(0,r.jsx)(n.a,{href:"http://docs.ros.org/en/noetic/api/geometry_msgs/html/msg/TransformStamped.html",children:(0,r.jsx)(n.code,{children:"geometry_msgs/TransformStamped"})}),".\nThe goal of this task is to convert this transform into a (4x4) ",(0,r.jsx)(n.strong,{children:"homogeneous transformation matrix"}),", the extrinsic matrix ",(0,r.jsx)(n.code,{children:"E"})," in our case."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"geometry_msgs/TransformStamped"})," contains a ",(0,r.jsx)(n.a,{href:"http://docs.ros.org/en/noetic/api/geometry_msgs/html/msg/Transform.html",children:(0,r.jsx)(n.code,{children:"geometry_msgs/Transform"})}),", which contains a ",(0,r.jsx)(n.strong,{children:"quaternion"})," and a ",(0,r.jsx)(n.strong,{children:"translation vector"}),".\n",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Quaternion",children:(0,r.jsx)(n.strong,{children:"Quaternions"})})," provide another way to represent rotations.\nIn this task, we will extract the quaternion from the variable ",(0,r.jsx)(n.code,{children:"transform"}),". Then, we will convert it to ",(0,r.jsx)(n.strong,{children:"(roll, pitch, yaw)"})," angles. At the end, we will convert ",(0,r.jsx)(n.strong,{children:"(roll, pitch, yaw)"})," into a rotation matrix."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Uncomment"})," and ",(0,r.jsx)(n.strong,{children:"adjust"})," the following lines to extract a ",(0,r.jsx)(n.strong,{children:"quaternion"})," out of the ",(0,r.jsx)(n.code,{children:"transform"})," and convert it into ",(0,r.jsx)(n.code,{children:"(roll, pitch, yaw)"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["The conversion uses ",(0,r.jsx)(n.a,{href:"https://github.com/DLu/tf_transformations/",children:(0,r.jsx)(n.code,{children:"tf_transformations"})}),". You can find the correct function for converting quaternions to ",(0,r.jsx)(n.code,{children:"(roll, pitch, yaw)"}),"- also called euler angles - ",(0,r.jsx)(n.a,{href:"https://github.com/DLu/tf_transformations/blob/main/tf_transformations/__init__.py",children:(0,r.jsx)(n.strong,{children:"here"})}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# extract quaternion from transform and transform it into a list\n# quaternion = transform.transform.REPLACE_ME # adjust and uncomment\n# quaternion = [REPLACE_ME.x, REPLACE_ME.y , REPLACE_ME.z, REPLACE_ME.w] # adjust and uncomment\n\n# convert quaternion to (roll,pitch,yaw)\n# roll, pitch, yaw = REPLACE_ME # uncomment and adjust\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Now, you can can convert ",(0,r.jsx)(n.code,{children:"(roll, pitch, yaw)"})," into a rotation matrix."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Replace"})," the ",(0,r.jsx)(n.code,{children:"None"})," placeholders with your code."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# compute rotation matrix\nRz = None # Replace with actual matrix\nRy = None # Replace with actual matrix\nRx = None # Replace with actual matrix\nR = None # Replace with actual matrix (combination of Rx, Ry, Rz)\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Refer to the documentation of ",(0,r.jsx)(n.a,{href:"http://docs.ros.org/en/noetic/api/geometry_msgs/html/msg/TransformStamped.html",children:(0,r.jsx)(n.code,{children:"geometry_msgs/TransformStamped"})})," and extract the translation vector by ",(0,r.jsx)(n.strong,{children:"adjusting"})," the ",(0,r.jsx)(n.code,{children:"REPLACE_ME"})," placeholder. Then, uncomment the next to line to save the ",(0,r.jsx)(n.code,{children:"x"}),", ",(0,r.jsx)(n.code,{children:"y"})," and ",(0,r.jsx)(n.code,{children:"z"})," components in a list. Last, convert the list to a NumPy array."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# extract translation from transform \n# t = transform.transform.REPLACE_ME # uncomment and adjust\n# t = [t.x, t.y, t.z]  # uncomment\n\n# convert t to a numpy array\n# t = REPLACE_ME # uncomment and adjust\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Combine"})," the determined (3x3) rotation matrix ",(0,r.jsx)(n.code,{children:"R"})," and the (3x1) translation vector ",(0,r.jsx)(n.code,{children:"t"})," into a (4x4) homogeneous transformation representing the extrinsic matrix ",(0,r.jsx)(n.code,{children:"E"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# first combine R and t\n# E = np.REPLACE_ME([REPLACE_ME, REPLACE_ME]) # uncomment and adjust\n# then add 1 row ([0., 0., 0., 1.]) to complete the transform\n# E = np.row_stack([E, REPLACE_ME]) # uncomment and adjust\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Then, use ",(0,r.jsx)(n.code,{children:"E"})," and ",(0,r.jsx)(n.strong,{children:"replace"})," the placeholder ",(0,r.jsx)(n.code,{children:"PLACE_HOLDER_EXTRINSIC"})," by commenting the following two lines."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"PLACE_HOLDER_EXTRINSIC = np.array([[1., 0., 0., 1.],[0., 1., 0., 1.],[0., 0., 1., 1.],[0., 0., 0., 1.]]) # comment when done with task 5\nE = PLACE_HOLDER_EXTRINSIC # comment when done with task 5\n"})}),"\n",(0,r.jsx)(n.p,{children:"Rerun the node :"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 launch camera_based_semantic_grid_mapping_r2 semantic_grid_mapping.launch.py \n"})}),"\n",(0,r.jsx)(n.h2,{id:"result",children:"Result"}),"\n",(0,r.jsxs)(n.p,{children:["After completing the ",(0,r.jsx)(n.code,{children:"compute_bev"})," function and restarting the node, you should see the following output."]}),"\n",(0,r.jsx)("img",{src:"../images/carla_bev.PNG",alt:"Description of image"}),"\n",(0,r.jsx)(n.p,{children:"Congratulations!"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Optional additional task"}),": Create an RVIZ configuration and a launch file that starts the semantic grid mapping node and RVIZ with your RVIZ configuration. Display all 8 images from the camera persepctive and the BEV image, if the performance of your computer permits!"]}),"\n",(0,r.jsx)(n.h2,{id:"wrap-up",children:"Wrap-up"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"You learned how to synchronize subscribers."}),"\n",(0,r.jsxs)(n.li,{children:["You learned how ",(0,r.jsx)(n.code,{children:"CameraInfo"})," messages can be used to extract the camera parameters."]}),"\n",(0,r.jsxs)(n.li,{children:["You learned how to perform coordinate transformations using ROS libraries like ",(0,r.jsx)(n.code,{children:"tf2"})," and ",(0,r.jsx)(n.code,{children:"tf_conversions"}),"."]}),"\n",(0,r.jsx)(n.li,{children:"You have completed a simple Python ROS package for camera-based semantic grid mapping."}),"\n",(0,r.jsx)(n.li,{children:"Feel free to explore the rest of the code for a deeper understanding of the semantic grid mapping package."}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"ros1-instructions",children:"ROS1 Instructions"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{src:"https://img.shields.io/badge/ROS1-blue",alt:"ROS1"})}),"\n",(0,r.jsx)(n.h2,{id:"perform-camera-based-semantic-grid-mapping-using-geometry-based-inverse-perspective-mapping-1",children:"Perform Camera-based Semantic Grid Mapping using geometry-based inverse perspective mapping"}),"\n",(0,r.jsxs)(n.p,{children:["In this workshop, we will perform ",(0,r.jsx)(n.strong,{children:"semantic grid mapping"})," based on images taken by vehicle-mounted cameras.\nThe approach will make use of inverse perspective mapping (IPM)."]}),"\n",(0,r.jsx)(n.p,{children:"We will use a recording from a simulation containing images from eight cameras."}),"\n",(0,r.jsx)(n.p,{children:"The learning goals of this workshop are ..."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Inspecting a rosbag which contains camera data"}),"\n",(0,r.jsx)(n.li,{children:"Learn about the ROS message format for camera images and camera infos"}),"\n",(0,r.jsx)(n.li,{children:"Learn about synchronized subscribers"}),"\n",(0,r.jsx)(n.li,{children:"Learn how to use the tf2 library"}),"\n",(0,r.jsx)(n.li,{children:"Learn how to visualize the output of semantic grid mapping"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"introduction-to-this-workshop-1",children:"Introduction to this workshop"}),"\n",(0,r.jsx)(n.p,{children:"We prepared a rosbag with camera data for you to use."}),"\n",(0,r.jsxs)(n.p,{children:["Download the file ",(0,r.jsx)(n.code,{children:"semantic_8_cams.bag"})," from ",(0,r.jsx)(n.a,{href:"https://rwth-aachen.sciebo.de/s/bHan8XNDB9ijlCz",children:(0,r.jsx)(n.strong,{children:"here (683.6 MB)"})}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["Save this file to your local directory ",(0,r.jsx)(n.code,{children:"${REPOSITORY}/bag"}),". This directory will be mounted into the docker container to the path ",(0,r.jsx)(n.code,{children:"/home/rosuser/ws/bag"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["After the download, navigate to the local directory ",(0,r.jsx)(n.code,{children:"${REPOSITORY}/docker"})," on your host and execute ",(0,r.jsx)(n.code,{children:"./ros1_run.sh"})," to start the ACDC docker container."]}),"\n",(0,r.jsxs)(n.p,{children:["Inside the container, you can navigate to ",(0,r.jsx)(n.code,{children:"/home/rosuser/ws/bag"})," and execute ",(0,r.jsx)(n.code,{children:"rosbag info semantic_8_cams.bag"})," to inspect the rosbag:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"path:        semantic_8_cams.bag\nversion:     2.0\nduration:    7.5s\nstart:       Jan 01 1970 01:00:26.59 (26.59)\nend:         Jan 01 1970 01:00:34.04 (34.04)\nsize:        683.6 MB\nmessages:    4622\ncompression: none [718/718 chunks]\ntypes:       rosgraph_msgs/Clock    [a9c97c1d230cfc112e270351a944ee47]\n             rosgraph_msgs/Log      [acffd30cd6b6de30f120938c17c593fb]\n             sensor_msgs/CameraInfo [c9a58c1b0b154e0e6da7578cb991d214]\n             sensor_msgs/Image      [060021388200f6f0f447d0fcd9c64743]\n             tf2_msgs/TFMessage     [94810edda583a504dfda3829e70d7eec]\ntopics:      /carla/ego_vehicle/rgb_view/camera_info                               149 msgs    : sensor_msgs/CameraInfo\n             /carla/ego_vehicle/rgb_view/image                                     149 msgs    : sensor_msgs/Image     \n             /carla/ego_vehicle/semantic_segmentation_back_left_1/camera_info      149 msgs    : sensor_msgs/CameraInfo\n             /carla/ego_vehicle/semantic_segmentation_back_left_1/image            149 msgs    : sensor_msgs/Image     \n             /carla/ego_vehicle/semantic_segmentation_back_left_2/camera_info      149 msgs    : sensor_msgs/CameraInfo\n             /carla/ego_vehicle/semantic_segmentation_back_left_2/image            149 msgs    : sensor_msgs/Image     \n             /carla/ego_vehicle/semantic_segmentation_back_right_1/camera_info     149 msgs    : sensor_msgs/CameraInfo\n             /carla/ego_vehicle/semantic_segmentation_back_right_1/image           149 msgs    : sensor_msgs/Image     \n             /carla/ego_vehicle/semantic_segmentation_back_right_2/camera_info     149 msgs    : sensor_msgs/CameraInfo\n             /carla/ego_vehicle/semantic_segmentation_back_right_2/image           149 msgs    : sensor_msgs/Image     \n             /carla/ego_vehicle/semantic_segmentation_front_left_1/camera_info     149 msgs    : sensor_msgs/CameraInfo\n             /carla/ego_vehicle/semantic_segmentation_front_left_1/image           149 msgs    : sensor_msgs/Image     \n             /carla/ego_vehicle/semantic_segmentation_front_left_2/camera_info     149 msgs    : sensor_msgs/CameraInfo\n             /carla/ego_vehicle/semantic_segmentation_front_left_2/image           149 msgs    : sensor_msgs/Image     \n             /carla/ego_vehicle/semantic_segmentation_front_right_1/camera_info    149 msgs    : sensor_msgs/CameraInfo\n             /carla/ego_vehicle/semantic_segmentation_front_right_1/image          149 msgs    : sensor_msgs/Image     \n             /carla/ego_vehicle/semantic_segmentation_front_right_2/camera_info    149 msgs    : sensor_msgs/CameraInfo\n             /carla/ego_vehicle/semantic_segmentation_front_right_2/image          149 msgs    : sensor_msgs/Image     \n             /clock                                                                150 msgs    : rosgraph_msgs/Clock   \n             /rosout                                                                 2 msgs    : rosgraph_msgs/Log     \n             /tf                                                                  1788 msgs    : tf2_msgs/TFMessage\n"})}),"\n",(0,r.jsxs)(n.p,{children:["You can see that the rosbag has a duration of 7.5 seconds and contains segmented images of type ",(0,r.jsx)(n.code,{children:"sensor_msgs/Image"})," and corresponding ",(0,r.jsx)(n.code,{children:"sensor_msgs/CameraInfo"})," messages.\nWe will use these camera images in this assignment in order to apply semantic grid mapping."]}),"\n",(0,r.jsxs)(n.h3,{id:"ros-sensor_msgsimage-message",children:["ROS ",(0,r.jsx)(n.code,{children:"sensor_msgs/Image"})," Message"]}),"\n",(0,r.jsxs)(n.p,{children:["The message definition ",(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/noetic/api/sensor_msgs/html/msg/Image.html",children:"sensor_msgs/Image"})," is ROS' image message format. This message type provides a way to send images ",(0,r.jsx)(n.code,{children:"sensor_msgs/Image"})," that can be used seamlessly with many different ROS visualization and image processing tools. Please read the documentation about the ",(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/noetic/api/sensor_msgs/html/msg/Image.html",children:"detailed message format"})," and its content."]}),"\n",(0,r.jsxs)(n.h3,{id:"ros-sensor_msgscamerainfo",children:["ROS ",(0,r.jsx)(n.code,{children:"sensor_msgs/CameraInfo"})]}),"\n",(0,r.jsxs)(n.p,{children:["The message definition ",(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/noetic/api/sensor_msgs/html/msg/CameraInfo.html",children:"sensor_msgs/CameraInfo"})," is ROS' standard camera info message format. It is sent together with ",(0,r.jsx)(n.code,{children:"sensor_msgs/Image"})," or ",(0,r.jsx)(n.code,{children:"sensor_msgs/CompressedImage"})," to provide additional information about the current camera image such as ",(0,r.jsx)(n.strong,{children:"camera calibration parameters"}),". Please read the documentation about the ",(0,r.jsx)(n.a,{href:"https://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/CameraInfo.html",children:"detailed message format"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"task-1-explore-the-semantic-grid-mapping-package-and-build-and-source-the-package-1",children:"Task 1: Explore the semantic grid mapping package and build and source the package"}),"\n",(0,r.jsxs)(n.p,{children:["The code for the camera-based semantic grid mapping node can be found in the directory ",(0,r.jsx)(n.code,{children:"workshops/section_2/camera_based_semantic_grid_mapping"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["The main source code is located in the directory ",(0,r.jsx)(n.code,{children:"src"}),". The launch file and parameters are located in directory ",(0,r.jsx)(n.code,{children:"launch"}),". Feel free to read all the code, parameters and launch files."]}),"\n",(0,r.jsxs)(n.p,{children:["Now, in the container, navigate to ",(0,r.jsx)(n.code,{children:"catkin_workspace"})," and build the package with with ",(0,r.jsx)(n.code,{children:"catkin build"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"catkin build camera_based_semantic_grid_mapping\n"})}),"\n",(0,r.jsx)(n.p,{children:"and source the workspace"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"source devel/setup.bash\n"})}),"\n",(0,r.jsx)(n.p,{children:"Perfect! Now you will be able to perform semantic grid mapping using camera images with this package. Let's go to the next task."}),"\n",(0,r.jsx)(n.h2,{id:"task-2-replay-rosbag-and-run-the-camera-based-grid-mapping-node-1",children:"Task 2: Replay rosbag and run the camera-based grid mapping node"}),"\n",(0,r.jsx)(n.p,{children:"We have already prepared a launch file for you to execute the camera-based semantic grid mapping package. Please read through the following lines of code carefully."}),"\n",(0,r.jsxs)(n.p,{children:["Contents of the file ",(0,r.jsx)(n.code,{children:"start_all.launch"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<launch>\n\n    <param name ="/use_sim_time" value="true"/>\n    \n    \x3c!-- ROSBAG PLAY --\x3e\n    <node pkg="rosbag" \n          type="play"\n          name="player"\n          output="screen"\n          args="--rate 0.1 -l --clock $/home/rosuser/ws/bag/semantic_8_cams.bag">\n    </node>\n\n    \x3c!--- Semantic Grid Mapping Node --\x3e\n    <rosparam\n      command="load"\n      file="$(find camera_based_semantic_grid_mapping)/launch/params.yaml"\n    />\n\n    <node\n        name="camera_based_semantic_grid_mapping"\n        pkg="camera_based_semantic_grid_mapping"\n        type="semantic_grid_mapping.py"\n        output="screen">\n    </node>\n\n    \x3c!--- NODES FOR VISUALIZATION --\x3e\n    <node pkg="image_view"\n          type="image_view"\n          name="front_left_segmented_image"\n          args="image:=/carla/ego_vehicle/semantic_segmentation_front_left_1/image">\n    </node>\n    \x3c!--- NODES FOR VISUALIZATION --\x3e\n    <node pkg="image_view"\n          type="image_view"\n          name="front_right_segmented_image"\n          args="image:=/carla/ego_vehicle/semantic_segmentation_front_right_1/image">\n    </node>\n    \n    <node pkg="image_view"\n          type="image_view"\n          name="segmentation_viewer"\n          args="image:=/BEV_image">\n    </node>\n   \n\n\n</launch>\n'})}),"\n",(0,r.jsx)(n.p,{children:"Hence, we perform the following tasks:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Replay the rosbag"})," with a speed of 0.1. Note, that we set the speed to a very low value here, because your computer might be very slow. You can increase this value if your computer is fast enough to compute the semantic grid map at a higher speed. If necessary, you may also reduce the value."]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Load the parameters"})," that are necessary for the ",(0,r.jsx)(n.code,{children:"camera_based_semantic_grid_mapping"})," node (camera image and info topic names, output image size etc.)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsxs)(n.strong,{children:["Start the ",(0,r.jsx)(n.code,{children:"camera_based_semantic_grid_mapping "})," Node"]})}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Start a visualization node"})," to show the result"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["The node ",(0,r.jsx)(n.code,{children:"camera_based_semantic_grid_mapping "})," performs the following tasks on the data provided in the bag file:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"subscribe"})," to 8 ",(0,r.jsx)(n.code,{children:"CameraInfo"})," and 8 ",(0,r.jsx)(n.code,{children:"Image"})," topics"]}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.strong,{children:"synchronize the subscribers"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsxs)(n.strong,{children:["apply ",(0,r.jsx)(n.code,{children:"inverse perspective mapping"})]})," on single images."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"stitches"}),"  the result into a BEV image"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"We can now start the launch file with:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"roslaunch camera_based_semantic_grid_mapping start_all.launch\n"})}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"image_view"})," node should show you the semantically segmented images of the right and left forward facing cameras. They are sufficient for testing the node, for now."]}),"\n",(0,r.jsx)("img",{src:"../images/visualization.PNG",alt:"Description of image"}),"\n",(0,r.jsx)(n.p,{children:"However, the Bird's Eye View (BEV) image is not shown. This is because the node doesn't yet subscribe to the correct images and cameras info topics yet."}),"\n",(0,r.jsxs)(n.p,{children:["In the following tasks, you will need to modify code in ",(0,r.jsx)(n.strong,{children:'"semantic_grid_mapping.py"'}),". After finishing all tasks, you will have completed the semantic grid mapping node."]}),"\n",(0,r.jsx)(n.h2,{id:"task-3-synchronize-the-subscribers-1",children:"Task 3: Synchronize the subscribers"}),"\n",(0,r.jsx)(n.p,{children:"As we want to combine images from multiple cameras to produce a BEV image, these images need to be from the same time step."}),"\n",(0,r.jsxs)(n.p,{children:["For the purpose of synchronizing the image topics, we will use ",(0,r.jsx)(n.strong,{children:(0,r.jsx)(n.code,{children:"message_filters"})}),"."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"message_filters"})," is a utility library that provides filtering algorithms for commonly used messages."]}),"\n",(0,r.jsxs)(n.p,{children:["The following lines of code in ",(0,r.jsx)(n.strong,{children:'"semantic_grid_mapping.py"'})," are responsible for subscribing to the different ",(0,r.jsx)(n.code,{children:"CameraInfo"})," and ",(0,r.jsx)(n.code,{children:"Image"})," topics."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'PLACE_HOLDER_1 = "NONEXISTENT_CAMERA_IMAGE_TOPIC"\nPLACE_HOLDER_2 = "NONEXISTENT_CAMERA_INFO_TOPIC"\n# setup subscribers\nsubs = []  # array with all subscribers that should be synchronized\nfor image_topic, info_topic in zip(self.image_topics_in, self.info_topics_in):\n    ### START Task 3 CODE HERE ###\n    # create subscriber for topic\n    image_sub = message_filters.Subscriber(PLACE_HOLDER_1, Image, queue_size=1)\n    # create a subscriber for camera info topic\n    info_sub = message_filters.Subscriber(PLACE_HOLDER_2, CameraInfo, queue_size=1)\n    ### END Task 3 CODE HERE ###\n    # add subscribers to array\n    subs.append(image_sub)\n    subs.append(info_sub)\n\n# synchronized subscriber\nself.sync_sub = message_filters.ApproximateTimeSynchronizer(subs, queue_size=5., slop=0.01)\n'})}),"\n",(0,r.jsxs)(n.p,{children:["In the above code snippet, we iterate over all ",(0,r.jsx)(n.code,{children:"Image"})," topics and their corresponding ",(0,r.jsx)(n.code,{children:"CameraInfo"}),". These are defined in ",(0,r.jsx)(n.code,{children:"launch/params.yaml"})," und loaded in the node's function ",(0,r.jsx)(n.code,{children:"load_parameters()"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["Note that we use ",(0,r.jsx)(n.code,{children:"message_filters.Subscriber()"})," instead of normal ROS subscribers."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"message_filters"})," provides subscriber policies that allow subscribing to messages from multiple topics and outputting them in a single callback. Here, we will use ",(0,r.jsx)(n.code,{children:"message_filters.ApproximateTimeSynchronizer()"}),". The policy synchronizes the topics based on their time stamp (given in the header of the messages). It is capable of synchronizing topics even if messages' time stamps differ from each other. More information can be found in the ",(0,r.jsx)(n.a,{href:"https://wiki.ros.org/message_filters",children:"documentation"}),". There also exist an ",(0,r.jsx)(n.code,{children:"ExactTime Policy."})," , which is not covered here."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"message_filters.ApproximateTimeSynchronizer()"})," takes three parameters:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["an array of subscribers (we save our subscribers in the array ",(0,r.jsx)(n.strong,{children:"subs"}),")"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"queue_size"})," and  ",(0,r.jsx)(n.strong,{children:"slop"})," (we use  ",(0,r.jsx)(n.strong,{children:"queue_size=5"})," and  ",(0,r.jsx)(n.strong,{children:"slop=0.01"}),")"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["The ",(0,r.jsx)(n.code,{children:"slop"})," parameter defines the maximum delay (in seconds) with which messages can be synchronized."]}),"\n",(0,r.jsxs)(n.p,{children:["Your task is to ",(0,r.jsx)(n.strong,{children:"replace"})," ",(0,r.jsx)(n.code,{children:"PLACE_HOLDER_1"})," and ",(0,r.jsx)(n.code,{children:"PLACE_HOLDER_2"})," with the correct variables to subscribe to the ",(0,r.jsx)(n.code,{children:"Image"}),"and ",(0,r.jsx)(n.code,{children:"CameraInfo"})," topics."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# create subscriber for topic\nimage_sub = message_filters.Subscriber(PLACE_HOLDER_1, Image, queue_size=1)\n# create a subscriber for camera info topic\ninfo_sub = message_filters.Subscriber(PLACE_HOLDER_2, CameraInfo, queue_size=1)\n"})}),"\n",(0,r.jsxs)(n.p,{children:["After this adjustment, you can ",(0,r.jsx)(n.strong,{children:"rerun the node"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"roslaunch camera_based_semantic_grid_mapping start_all.launch\n"})}),"\n",(0,r.jsx)(n.p,{children:"A visualization window for the BEV image should appear.\nHowever, the BEV image looks like this:"}),"\n",(0,r.jsx)("img",{src:"../images/bev_wrong.PNG",alt:"Description of image"}),"\n",(0,r.jsx)(n.p,{children:"Some parts of the code are still missing!"}),"\n",(0,r.jsx)(n.h2,{id:"task-4-extract-the-camera-intrinsic-matrix-from-the-camerainfo-message-1",children:"Task 4: Extract the camera intrinsic matrix from the CameraInfo message"}),"\n",(0,r.jsxs)(n.p,{children:["The goal of this task is to get the intrinsic matrices ",(0,r.jsx)(n.code,{children:"K"})," of the cameras."]}),"\n",(0,r.jsxs)(n.p,{children:["For this purpose, you have to modify the following lines in the function ",(0,r.jsx)(n.strong,{children:"compute_bev"})," by ",(0,r.jsx)(n.strong,{children:"replacing"})," ",(0,r.jsx)(n.code,{children:"PLACE_HOLDER_INTRINSIC"})," with your code."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# extract intrinsic matrix K (3x3) from CameraInfo topic\nPLACE_HOLDER_INTRINSIC = [100., 0., 0., 0., 100., 0., 0., 0., 100.] # comment this line in your solution\nK = np.reshape(PLACE_HOLDER_INTRINSIC, (3,3)) # replace the placeholder and use the actual camera intrinsics\n"})}),"\n",(0,r.jsx)(n.h4,{id:"hints-2",children:"hints:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The ",(0,r.jsx)(n.code,{children:"CameraInfo"})," message is provided in the variable ",(0,r.jsx)(n.code,{children:"cam_info_msg"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["In the ",(0,r.jsx)(n.strong,{children:"CameraInfo"})," message, the intrinsic matrix is named ",(0,r.jsx)(n.strong,{children:"K"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"task-5-calculate-the-camera-extrinsic-matrix-1",children:"Task 5: Calculate the camera extrinsic matrix"}),"\n",(0,r.jsxs)(n.p,{children:["The goal of this task is to compute extrinsic matrices of the cameras based on information in the ",(0,r.jsx)(n.code,{children:"CameraInfo"})," messages."]}),"\n",(0,r.jsxs)(n.p,{children:["The extrinsic matrix transforms the camera's coordinate frame to the vehicle's coordinates frame.\nFor this, we will use ros ",(0,r.jsx)(n.a,{href:"https://wiki.ros.org/tf2",children:(0,r.jsx)(n.strong,{children:"tf2"})}),". This library allows to maintain multiple coordinate frames and their relationship over time."]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Part 1: Look up the transformation between the vehicle's base link and the camera frames"})}),"\n",(0,r.jsxs)(n.p,{children:["To get the transformation between coordinate frames, you will use the function ",(0,r.jsx)(n.a,{href:"http://docs.ros.org/en/noetic/api/tf2_ros/html/c++/classtf2__ros_1_1Buffer.html",children:(0,r.jsx)(n.code,{children:"tfBuffer.lookup_transform()"})})," which takes a target frame and a source frame as parameters and returns the transformation between them.\nReplace the ",(0,r.jsx)(n.code,{children:"None"})," placeholders in the function ",(0,r.jsx)(n.code,{children:"compute_bev()"})," in ",(0,r.jsx)(n.strong,{children:'"semanctic_grid_mapping.py"'}),", then uncomment the line to define ",(0,r.jsx)(n.code,{children:"transform"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# use tfBuffer to look up the transformation from the vehicle's base link frame to the camera frame.\n# transform = self.tfBuffer.lookup_transform(None, None, None) # uncomment and adjust\n"})}),"\n",(0,r.jsx)(n.h4,{id:"hints-3",children:"Hints:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["The vehicle's base link frame name is saved in ",(0,r.jsx)(n.code,{children:"self.vehicle_base_link"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["The camera frame name can be found in the ",(0,r.jsx)(n.code,{children:"CameraInfo"})," topic."]}),"\n",(0,r.jsxs)(n.li,{children:["Look up the transformation at the common time ",(0,r.jsx)(n.code,{children:"common_time"}),"."]}),"\n",(0,r.jsxs)(n.li,{children:["The function ",(0,r.jsx)(n.code,{children:"lookup_transform()"})," can take four arguments: ",(0,r.jsx)(n.code,{children:"target_frame"}),", ",(0,r.jsx)(n.code,{children:"source_frame"}),", ",(0,r.jsx)(n.code,{children:"time"})," at which the value of the transform is desired, and an optional ",(0,r.jsx)(n.code,{children:"timeout"}),"."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Part 2: Convert a Transform into a homogeneous matrix"})}),"\n",(0,r.jsxs)(n.p,{children:["The function ",(0,r.jsx)(n.code,{children:"tfBuffer.lookup_transform()"})," returns a transform of the type ",(0,r.jsx)(n.a,{href:"http://docs.ros.org/en/noetic/api/geometry_msgs/html/msg/TransformStamped.html",children:(0,r.jsx)(n.code,{children:"geometry_msgs/TransformStamped"})}),".\nThe goal of this task is to convert this transform into a (4x4) ",(0,r.jsx)(n.strong,{children:"homogeneous transformation matrix"}),", the extrinsic matrix ",(0,r.jsx)(n.code,{children:"E"})," in our case."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.code,{children:"geometry_msgs/TransformStamped"})," contains a ",(0,r.jsx)(n.a,{href:"http://docs.ros.org/en/noetic/api/geometry_msgs/html/msg/Transform.html",children:(0,r.jsx)(n.code,{children:"geometry_msgs/Transform"})}),", which contains a ",(0,r.jsx)(n.strong,{children:"quaternion"})," and a ",(0,r.jsx)(n.strong,{children:"translation vector"}),".\n",(0,r.jsx)(n.a,{href:"https://en.wikipedia.org/wiki/Quaternion",children:(0,r.jsx)(n.strong,{children:"Quaternions"})})," provide another way to represent rotations.\nIn this task, we will extract the quaternion from the variable ",(0,r.jsx)(n.code,{children:"transform"}),". Then, we will convert it to ",(0,r.jsx)(n.strong,{children:"(roll, pitch, yaw)"})," angles. At the end, we will convert ",(0,r.jsx)(n.strong,{children:"(roll, pitch, yaw)"})," into a rotation matrix."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Uncomment"})," and ",(0,r.jsx)(n.strong,{children:"adjust"})," the following lines to extract a ",(0,r.jsx)(n.strong,{children:"quaternion"})," out of the ",(0,r.jsx)(n.code,{children:"transform"})," and convert it into ",(0,r.jsx)(n.code,{children:"(roll, pitch, yaw)"}),"."]}),"\n",(0,r.jsxs)(n.p,{children:["The conversion uses ",(0,r.jsx)(n.a,{href:"https://wiki.ros.org/tf_conversions",children:(0,r.jsx)(n.code,{children:"tf_conversions"})}),". You can find the correct function for converting quaternions to ",(0,r.jsx)(n.code,{children:"(roll, pitch, yaw)"}),"- also called euler angles - ",(0,r.jsx)(n.a,{href:"https://github.com/ros/geometry/blob/noetic-devel/tf/src/tf/transformations.py",children:(0,r.jsx)(n.strong,{children:"here"})}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# extract quaternion from transform and transform it into a list\n# quaternion = transform.transform.REPLACE_ME # adjust and uncomment\n# quaternion = [REPLACE_ME.x, REPLACE_ME.y , REPLACE_ME.z, REPLACE_ME.w] # adjust and uncomment\n\n# convert quaternion to (roll,pitch,yaw)\n# roll, pitch, yaw = REPLACE_ME # uncomment and adjust\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Now, you can can convert ",(0,r.jsx)(n.code,{children:"(roll, pitch, yaw)"})," into a rotation matrix."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Replace"})," the ",(0,r.jsx)(n.code,{children:"None"})," placeholders with your code."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# compute rotation matrix\nRz = None # Replace with actual matrix\nRy = None # Replace with actual matrix\nRx = None # Replace with actual matrix\nR = None # Replace with actual matrix (combination of Rx, Ry, Rz)\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Refer to the documentation of ",(0,r.jsx)(n.a,{href:"http://docs.ros.org/en/noetic/api/geometry_msgs/html/msg/TransformStamped.html",children:(0,r.jsx)(n.code,{children:"geometry_msgs/TransformStamped"})})," and extract the translation vector by ",(0,r.jsx)(n.strong,{children:"adjusting"})," the ",(0,r.jsx)(n.code,{children:"REPLACE_ME"})," placeholder. Then, uncomment the next to line to save the ",(0,r.jsx)(n.code,{children:"x"}),", ",(0,r.jsx)(n.code,{children:"y"})," and ",(0,r.jsx)(n.code,{children:"z"})," components in a list. Last, convert the list to a NumPy array."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# extract translation from transform \n# t = transform.transform.REPLACE_ME # uncomment and adjust\n# t = [t.x, t.y, t.z]  # uncomment\n\n# convert t to a numpy array\n# t = REPLACE_ME # uncomment and adjust\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Combine"})," the determined (3x3) rotation matrix ",(0,r.jsx)(n.code,{children:"R"})," and the (3x1) translation vector ",(0,r.jsx)(n.code,{children:"t"})," into a (4x4) homogeneous transformation representing the extrinsic matrix ",(0,r.jsx)(n.code,{children:"E"}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# first combine R and t\n# E = np.REPLACE_ME([REPLACE_ME, REPLACE_ME]) # uncomment and adjust\n# then add 1 row ([0., 0., 0., 1.]) to complete the transform\n# E = np.row_stack([E, REPLACE_ME]) # uncomment and adjust\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Then, use ",(0,r.jsx)(n.code,{children:"E"})," and ",(0,r.jsx)(n.strong,{children:"replace"})," the placeholder ",(0,r.jsx)(n.code,{children:"PLACE_HOLDER_EXTRINSIC"})," by commenting the following two lines."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"PLACE_HOLDER_EXTRINSIC = np.array([[1., 0., 0., 1.],[0., 1., 0., 1.],[0., 0., 1., 1.],[0., 0., 0., 1.]]) # comment when done with task 5\nE = PLACE_HOLDER_EXTRINSIC # comment when done with task 5\n"})}),"\n",(0,r.jsx)(n.p,{children:"Rerun the node :"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"roslaunch camera_based_semantic_grid_mapping start_all.launch\n"})}),"\n",(0,r.jsx)(n.h2,{id:"result-1",children:"Result"}),"\n",(0,r.jsxs)(n.p,{children:["After completing the ",(0,r.jsx)(n.code,{children:"compute_bev"})," function and restarting the node, you should see the following output."]}),"\n",(0,r.jsx)("img",{src:"../images/carla_bev.PNG",alt:"Description of image"}),"\n",(0,r.jsx)(n.p,{children:"Congratulations!"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Optional additional task"}),": Create an RVIZ configuration and a launch file that starts the semantic grid mapping node and RVIZ with your RVIZ configuration. Display all 8 images from the camera persepctive and the BEV image, if the porformance of your computer permits!"]}),"\n",(0,r.jsx)(n.h2,{id:"wrap-up-1",children:"Wrap-up"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"You learned how to synchronize subscribers."}),"\n",(0,r.jsxs)(n.li,{children:["You learned how ",(0,r.jsx)(n.code,{children:"CameraInfo"})," messages can be used to extract the camera parameters."]}),"\n",(0,r.jsxs)(n.li,{children:["You learned how to perform coordinate transformations using ROS libraries like ",(0,r.jsx)(n.code,{children:"tf2"})," and ",(0,r.jsx)(n.code,{children:"tf_conversions"}),"."]}),"\n",(0,r.jsx)(n.li,{children:"You have completed a simple Python ROS package for camera-based semantic grid mapping."}),"\n",(0,r.jsx)(n.li,{children:"Feel free to explore the rest of the code for a deeper understanding of the semantic grid mapping package."}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(l,{...e})}):l(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>o});var a=s(6540);const r={},i=a.createContext(r);function t(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);