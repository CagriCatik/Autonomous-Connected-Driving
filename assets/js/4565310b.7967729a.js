"use strict";(self.webpackChunkacd=self.webpackChunkacd||[]).push([[4632],{5978:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>c,metadata:()=>s,toc:()=>r});const s=JSON.parse('{"id":"task/sensor_data_processing/Object-Detection","title":"Object Detection","description":"ROS1","source":"@site/docs/task/02_sensor_data_processing/05_Object-Detection.md","sourceDirName":"task/02_sensor_data_processing","slug":"/task/sensor_data_processing/Object-Detection","permalink":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Object-Detection","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/task/02_sensor_data_processing/05_Object-Detection.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{},"sidebar":"taskSidebar","previous":{"title":"Localization","permalink":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Localization"},"next":{"title":"Perform Deep Learning based semantic image segmentation applied on camera images","permalink":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Semantic-Image-Segmentation"}}');var t=i(4848),o=i(8453);const c={},a="Object Detection",d={},r=[{value:"Definitions",id:"definitions",level:2},{value:"ika ROS Object Definition",id:"ika-ros-object-definition",level:3},{value:"ika ROS Object Lists Definition",id:"ika-ros-object-lists-definition",level:3},{value:"Measurement Data",id:"measurement-data",level:2},{value:"Rosbag Inspection",id:"rosbag-inspection",level:3},{value:"Rosbag Visualization",id:"rosbag-visualization",level:3},{value:"Recap: Navigation in RVIZ",id:"recap-navigation-in-rviz",level:4},{value:"Lidar Detection",id:"lidar-detection",level:2},{value:"Visualize Object List",id:"visualize-object-list",level:3},{value:"Task 1",id:"task-1",level:2},{value:"Task 2",id:"task-2",level:2},{value:"Wrap-up",id:"wrap-up",level:2}];function l(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"object-detection",children:"Object Detection"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.img,{src:"https://img.shields.io/badge/ROS1-blue",alt:"ROS1"})}),"\n",(0,t.jsx)("img",{src:"../images/section_2/object_detection/header_object_detection.png",alt:"Description of image"}),"\n",(0,t.jsxs)(n.p,{children:["In this assignment, we will focus on ",(0,t.jsx)(n.strong,{children:"3D object detection"})," for raw LiDAR within ROS. In particular, we consider a recording from our test vehicle which is equipped with a ",(0,t.jsx)(n.a,{href:"https://icave2.cse.buffalo.edu/resources/sensor-modeling/VLP32CManual.pdf",children:"Velodyne VLP-32C"})," and apply a state-of-the art 3D object detection model to predict bounding boxes."]}),"\n",(0,t.jsx)(n.p,{children:"In this exercise you will learn"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["how ROS ",(0,t.jsx)(n.strong,{children:"object definitions"})," are defined"]}),"\n",(0,t.jsxs)(n.li,{children:["to visualize ",(0,t.jsx)(n.strong,{children:"LiDAR point clouds"})," within ",(0,t.jsx)(n.strong,{children:"RViz"})]}),"\n",(0,t.jsxs)(n.li,{children:["how to launch a ",(0,t.jsx)(n.strong,{children:"ROS node"})," that applies a detection algorithm on the raw sensor data"]}),"\n",(0,t.jsxs)(n.li,{children:["to use ",(0,t.jsx)(n.strong,{children:"RVIZ"})," to visualize ",(0,t.jsx)(n.strong,{children:"detected objects"})]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"definitions",children:"Definitions"}),"\n",(0,t.jsxs)(n.p,{children:["The ika definitions for the ROS messages and other internal definitions are defined in the package ",(0,t.jsx)(n.a,{href:"https://github.com/ika-rwth-aachen/acdc/blob/main/catkin_workspace/src/dependencies/definitions",children:(0,t.jsx)(n.em,{children:"definitions"})}),". The ROS message files are located in the subdirectory ",(0,t.jsx)(n.a,{href:"https://github.com/ika-rwth-aachen/acdc/tree/main/catkin_workspace/src/dependencies/definitions/msg",children:"definitions/msg"})," and all other internal definitions can be found in ",(0,t.jsx)(n.code,{children:"~/ws/catkin_workspace/src/dependencies/definitions/include/definitions/utility"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"ika-ros-object-definition",children:"ika ROS Object Definition"}),"\n",(0,t.jsxs)(n.p,{children:["The files ",(0,t.jsx)(n.a,{href:"https://github.com/ika-rwth-aachen/acdc/blob/main/catkin_workspace/src/dependencies/definitions/msg/IkaObject.msg",children:"IkaObject.msg"})," and ",(0,t.jsx)(n.a,{href:"https://github.com/ika-rwth-aachen/acdc/blob/main/catkin_workspace/src/dependencies/definitions/include/definitions/utility/object_definitions.h",children:"object_definitions.h"})," define the properties of an ",(0,t.jsx)(n.strong,{children:"3D object"}),". By the term ",(0,t.jsx)(n.em,{children:"object"}),", we refer to a surround traffic participant which can be assigned to a class (e.g. car, pedestrian, truck) and has an estimated state (e.g. position, velocity, orientation). The ika object definitions distinguish between several classes as shown in the following snipped (excerpt from ",(0,t.jsx)(n.a,{href:"https://github.com/ika-rwth-aachen/acdc/blob/main/catkin_workspace/src/dependencies/definitions/include/definitions/utility/object_definitions.h#L96",children:"object_definitions.h"}),"\n):"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-c++",children:"enum ika_object_types {\n  UNCLASSIFIED = 0,\n  PEDESTRIAN = 1,\n  BICYCLE = 2,\n  MOTORBIKE = 3,\n  CAR = 4,\n  TRUCK = 5,\n  VAN = 6,\n  BUS = 7,\n  ANIMAL = 8,\n  ROAD_OBSTACLE = 9,\n  TRAILER = 10,\n  TYPES_COUNT = 11\n};\n\n"})}),"\n",(0,t.jsxs)(n.p,{children:["For simplicity, we will focus in this assignment on the classes ",(0,t.jsx)(n.code,{children:"CAR"}),", ",(0,t.jsx)(n.code,{children:"PEDESTRIAN"}),", ",(0,t.jsx)(n.code,{children:"TRUCK"})," and ",(0,t.jsx)(n.code,{children:"BIKE"}),". Note that the ",(0,t.jsx)(n.a,{href:"https://en.cppreference.com/w/cpp/language/enum",children:"enumeration"})," above assigns each type to an unique ID. For example, type ",(0,t.jsx)(n.code,{children:"CAR"})," has ID 4. This association can then be accessed in the code with ",(0,t.jsx)(n.code,{children:"definitions::ika_object_types::CAR"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["The following code snippet presents an excerpt from the file ",(0,t.jsx)(n.a,{href:"https://github.com/ika-rwth-aachen/acdc/blob/main/catkin_workspace/src/dependencies/definitions/msg/IkaObject.msg",children:"IkaObject.msg"}),". As shown below, each object has a ",(0,t.jsx)(n.strong,{children:"state vector"})," which holds current values of the object, such as the ",(0,t.jsx)(n.strong,{children:"position"}),", ",(0,t.jsx)(n.strong,{children:"velocity"}),", ",(0,t.jsx)(n.strong,{children:"acceleration"})," and the ",(0,t.jsx)(n.strong,{children:"3D bounding box"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"float32[] fMean               # Statevector, containing attributes depend on chosen motion model\nfloat32[] fCovariance         # Covariance-Matrix, containing attributes depend on chosen motion model\n"})}),"\n",(0,t.jsxs)(n.p,{children:["As each quantity might be estimated by the AV's sensors and its perception algorithms, which are not perfect, we need to consider uncertainties as well. The ",(0,t.jsx)(n.strong,{children:"state covariance vector"})," stores such uncertainties, which are necessary in later processing steps such as the object fusion."]}),"\n",(0,t.jsxs)(n.p,{children:["In this assignment we will focus purely on 3D object detection. That means, we will just consider the state vector ",(0,t.jsx)(n.code,{children:"fMean"}),", which basically describe an object's bounding box."]}),"\n",(0,t.jsx)(n.h3,{id:"ika-ros-object-lists-definition",children:"ika ROS Object Lists Definition"}),"\n",(0,t.jsxs)(n.p,{children:["Sending many single object messages from various sources over the ROS network might be very tedious to handle and organize. Further, sensor's have often a fixed sample rate in which they detect and send several objects. Hence, so called ",(0,t.jsx)(n.em,{children:"object lists"})," are used to handle and send several object messages at once. The ",(0,t.jsx)(n.a,{href:"https://github.com/ika-rwth-aachen/acdc/blob/main/catkin_workspace/src/dependencies/definitions/msg/IkaObjectList.msg",children:(0,t.jsx)(n.code,{children:"IkaObjectList.msg"})})," defines such a list. The implementation for the ",(0,t.jsx)(n.code,{children:"IkaObjectList"})," is shown below"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"std_msgs/Header header\n\n# List meta information\nuint8 IdSource    #see definitions/utility/object_definitions.h for enum of sensors\n\n# Actually objects\nIkaObject[] objects\n"})}),"\n",(0,t.jsxs)(n.p,{children:["As you can see, the message ",(0,t.jsx)(n.code,{children:"IkaObjectList"})," has a field ",(0,t.jsx)(n.code,{children:"objects"})," which is a list (with undefined length) and contains elements of the type ",(0,t.jsx)(n.code,{children:"IkaObject"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"measurement-data",children:"Measurement Data"}),"\n",(0,t.jsx)(n.p,{children:"We provide real sensor data from our institute's test vehicle which is equipped with a Velodyne VLP-32C. This laser scanner has 32 layers and scans its environment with 10 Hz. The recordings were captured at Campus Melaten in Aachen, Germandy and can be downloaded as follows:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"wget -O lidar_campus_melaten.bag https://rwth-aachen.sciebo.de/s/udlMYloXpCdVtyp/download\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Alternatively, you can download the bag file for this assignment here ",(0,t.jsx)(n.a,{href:"https://rwth-aachen.sciebo.de/s/udlMYloXpCdVtyp",children:(0,t.jsx)(n.strong,{children:"Link"})})," (approx. 1.5 GB)."]}),"\n",(0,t.jsxs)(n.p,{children:["Save the file to the local directory ",(0,t.jsx)(n.code,{children:"${REPOSITORY}/bag"})," on you host which is mounted to ",(0,t.jsx)(n.code,{children:"~/bag"})," in the docker container. Now, we want to work with the ",(0,t.jsx)(n.code,{children:"rosbag"})," commands inside the container"]}),"\n",(0,t.jsx)(n.h3,{id:"rosbag-inspection",children:"Rosbag Inspection"}),"\n",(0,t.jsxs)(n.p,{children:["Inspect the bag file with the command ",(0,t.jsx)(n.code,{children:"rosbag info"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"rosuser@:/home/rosuser/bag# rosbag info lidar_campus_melaten.bag \npath:        lidar_campus_melaten.bag \nversion:     2.0\nduration:    1:59s (119s)\nstart:       Feb 05 2020 15:25:31.41 (1580916331.41)\nend:         Feb 05 2020 15:27:31.37 (1580916451.37)\nsize:        1.5 GB\nmessages:    1200\ncompression: none [1199/1199 chunks]\ntypes:       sensor_msgs/PointCloud2 [1158d486dd51d683ce2f1be655c3c181]\n             tf2_msgs/TFMessage      [94810edda583a504dfda3829e70d7eec]\ntopics:      /points2     1199 msgs    : sensor_msgs/PointCloud2\n             /tf_static      1 msg     : tf2_msgs/TFMessage\n\n"})}),"\n",(0,t.jsxs)(n.p,{children:["As you can see the bag file contains the topic ",(0,t.jsx)(n.code,{children:"/points2"})," which contains messages of type ",(0,t.jsx)(n.code,{children:"sensor_msgs/PointCloud2"}),". In contrast to the ika ROS message definition, this message type is a standard message which was defined by the ROS community. The message definition can be found ",(0,t.jsx)(n.a,{href:"http://docs.ros.org/noetic/api/sensor_msgs/html/msg/PointCloud2.html",children:"here"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"rosbag-visualization",children:"Rosbag Visualization"}),"\n",(0,t.jsx)(n.p,{children:"Now, we want to visualize the point clouds using RVIZ."}),"\n",(0,t.jsxs)(n.p,{children:["Instead of starting a ",(0,t.jsx)(n.code,{children:"roscore"}),", ",(0,t.jsx)(n.code,{children:"rviz"})," and ",(0,t.jsx)(n.code,{children:"rosbag play"}),", we suggest to use a launch file to execute all three commands at once. We prepared such a launch file for you here ",(0,t.jsx)(n.code,{children:"workshops/section_2/lidar_detection/launch/start_rosbag_play_rviz.launch"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-xml",children:'<launch>\n    \x3c!-- Rosbag --\x3e\n    <param name="use_sim_time" value="true"/>\n    <node \n        pkg="rosbag"\n        type="play"\n        args="--clock -l -r 0.5 -d 1 /home/rosuser/bag/lidar_campus_melaten.bag"\n        name="player"\n        output="screen">\n    </node>\n\n    \x3c!-- RViz --\x3e\n    <node\n        type="rviz"\n        name="rviz"\n        pkg="rviz"\n        args="-d $(find lidar_detection)/rviz/point_cloud.rviz">\n    </node>    \n</launch>\n'})}),"\n",(0,t.jsxs)(n.p,{children:["As you can see, ",(0,t.jsx)(n.code,{children:"rosbag play"})," and ",(0,t.jsx)(n.code,{children:"rviz"})," are evoked. The launch file basically bundles all necessary commands to run the nodes with their parameters and executes them simultaneously. Furthermore, ",(0,t.jsx)(n.code,{children:"roscore"})," is called implicitly and it is not necessary to manually execute it."]}),"\n",(0,t.jsxs)(n.p,{children:["Since the launch file is in the package ",(0,t.jsx)(n.code,{children:"lidar_detection"}),", you can simply start it with:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"roslaunch lidar detection start_rosbag_play_rviz.launch\n"})}),"\n",(0,t.jsx)(n.p,{children:"The RViz window should pop up and you should see something like this"}),"\n",(0,t.jsx)("img",{src:"../images/section_2/object_detection/rviz.png",alt:"Description of image"}),"\n",(0,t.jsxs)(n.p,{children:["For a better visualization you may alter the display settings. Have a look on tab ",(0,t.jsx)(n.code,{children:"PointCloud2"}),". Here you could increase ",(0,t.jsx)(n.code,{children:"Size"})," to different values. Feel free to play with the settings such as ",(0,t.jsx)(n.code,{children:"Style"}),", ",(0,t.jsx)(n.code,{children:"Decay Time"})," or ",(0,t.jsx)(n.code,{children:"Color Transformer"}),"."]}),"\n",(0,t.jsx)(n.h4,{id:"recap-navigation-in-rviz",children:"Recap: Navigation in RVIZ"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Left mouse button: Click and drag to rotate around the Z axis"}),"\n",(0,t.jsx)(n.li,{children:"Middle mouse button: Click and drag to move the camera along the XY plane"}),"\n",(0,t.jsx)(n.li,{children:"Right mouse button: Click and drag to zoom the image"}),"\n",(0,t.jsx)(n.li,{children:"Scrollwheel: Zoom the image"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Congratulations!"})," We can terminate the previous launch file and focus on the actual lidar detection alorithms in the following."]}),"\n",(0,t.jsx)(n.h2,{id:"lidar-detection",children:"Lidar Detection"}),"\n",(0,t.jsxs)(n.p,{children:["We already provide a ROS package ",(0,t.jsx)(n.code,{children:"lidar_detection"})," for the detection of 3D objects, where the actual inference of the deep learning model is performed first, before generating the bounding boxes and object lists in a second step. The code is located in ",(0,t.jsx)(n.code,{children:"~/ws/catkin_workspace/src/workshops/section_2/lidar_detection"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"section_2/\n\u2514\u2500\u2500 lidar_detection\n \xa0\xa0 \u251c\u2500\u2500 CMakeLists.txt\n \xa0\xa0 \u251c\u2500\u2500 include\n    \u2502   \u251c\u2500\u2500 definitions.h\n    \u2502   \u251c\u2500\u2500 detector.h\n    \u2502   \u251c\u2500\u2500 lidar_detection.h\n \xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 list_creator.h\n \xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 pillar_utils.h\n \xa0\xa0 \u251c\u2500\u2500 launch\n \xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 static_params.yaml\n \xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start_all.launch\n \xa0\xa0 \u2502\xa0\xa0 \u251c\u2500\u2500 start_lidar_detection.launch\n \xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 start_rosbag_play_rviz.launch\n \xa0\xa0 \u251c\u2500\u2500 model\n    \u2502   \u251c\u2500\u2500 lidar_detection.yml\n \xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 FrozenGraphs\n    \u2502        \u2514\u2500\u2500lidar_detection\n    \u2502            \u2514\u2500\u2500lidar_detection.pb\n \xa0\xa0 \u251c\u2500\u2500 nodelet_plugins.xml\n \xa0\xa0 \u251c\u2500\u2500 package.xml\n \xa0\xa0 \u251c\u2500\u2500 rviz\n \xa0\xa0 \u2502\xa0\xa0 \u2514\u2500\u2500 point_cloud.rviz\n \xa0\xa0 \u2514\u2500\u2500 src\n \xa0\xa0     \u251c\u2500\u2500 definitions.cpp\n        \u251c\u2500\u2500 detector.cpp\n        \u251c\u2500\u2500 lidar_detection.cpp\n        \u251c\u2500\u2500 list_creator.cpp\n \xa0\xa0     \u2514\u2500\u2500 pillar_utils.cpp\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Don't be overwhelmed by the amount of code and all the details of this implementation. We do not require you to understand every line of code. As previously mentioned, the 3D Object Detection algorithm is called PointPillars which is based on a Deep Neural Network. We do not explain more details of this model here but we refer interested readers to ",(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/1812.05784",children:"https://arxiv.org/abs/1812.05784"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["Now, it's time to launch the ",(0,t.jsx)(n.code,{children:"lidar_detection"})," node. Navigate to ",(0,t.jsx)(n.code,{children:"~/ws/catkin_workspace"})," and build/source your workspace again to ensure that everything is well prepared."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"catkin build\nsource devel/setup.bash\n"})}),"\n",(0,t.jsxs)(n.p,{children:["After this you can launch the ",(0,t.jsx)(n.code,{children:"lidar_detection"})," node as well as the previous point cloud visualization. We provide a combined launch file which launches both tasks together."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"roslaunch lidar_detection start_all.launch\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note"})," that the bag file is being replayed with a rate of ",(0,t.jsx)(n.code,{children:"0.1"})," as the inference of the neural net for object detection is quite time demanding without using a GPU. Depending on the performance of you computer you might increase that factor."]}),"\n",(0,t.jsxs)(n.p,{children:["Now, the ",(0,t.jsx)(n.code,{children:"lidar_detection"})," node should be running and the RVIZ window should have popped up. Let's configure RVIZ to visualize the ",(0,t.jsx)(n.code,{children:"ikaObjectLists"})," messages."]}),"\n",(0,t.jsx)(n.h3,{id:"visualize-object-list",children:"Visualize Object List"}),"\n",(0,t.jsxs)(n.p,{children:["Click on ",(0,t.jsx)(n.em,{children:"ADD"})," --\x3e ",(0,t.jsx)(n.em,{children:"By Topic"})," --\x3e ",(0,t.jsxs)(n.em,{children:["Select ",(0,t.jsx)(n.code,{children:"/lidar_detection/object_list/IkaObjectList"})]})," --\x3e ",(0,t.jsx)(n.em,{children:"Ok"})]}),"\n",(0,t.jsx)("img",{src:"../images/section_2/object_detection/rviz_select.png",alt:"Description of image"}),"\n",(0,t.jsx)(n.p,{children:"Now, the detections should be visible and you should see something similar to this."}),"\n",(0,t.jsx)("img",{src:"../images/section_2/object_detection/task.png",alt:"Description of image"}),"\n",(0,t.jsxs)(n.p,{children:["Unfortunately, there's something wrong. The bounding boxes do not fit the objects very well and all objects are classified as ",(0,t.jsx)(n.code,{children:"UNKNOWN"}),", although we can clearly see vehicles and pedestrians in the point clouds. Let's fix this issue in the two upcoming tasks."]}),"\n",(0,t.jsx)(n.h2,{id:"task-1",children:"Task 1"}),"\n",(0,t.jsxs)(n.p,{children:["As you have seen in the previous task, the output of the object detection has some flaws. We forgot to set the correct size of the object boxes and the heading angle. In this task you will fix that problem and assign the correct values to the ",(0,t.jsx)(n.code,{children:"IkaObject"})," message."]}),"\n",(0,t.jsxs)(n.p,{children:["Open the file ",(0,t.jsx)(n.code,{children:"list_creator.cpp"})," and navigate to the following code snipped."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-c++",children:"// set object position\nobject.fMean[(int)definitions::ca_model::posX] = bounding_box.center(0);\nobject.fMean[(int)definitions::ca_model::posY] = bounding_box.center(1);\n\n ...  \n\n\n// START TASK 1 CODE  \n\n// set object dimensions and fHeading\nobject.fMean[(int)definitions::ca_model::length] = 2;\nobject.fMean[(int)definitions::ca_model::width] = 2;\nobject.fMean[(int)definitions::ca_model::height] = 2;\n\n...\n     \n// set yaw angle\nobject.fMean[(int)definitions::ca_model::heading] = 0;\n\n// END TASK 1 CODE \n"})}),"\n",(0,t.jsx)(n.p,{children:"You can see that two different variables are used:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"object"}),": Instance of ",(0,t.jsx)(n.a,{href:"#ObjectStateDefinition",children:"IkaObject.msg"})]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"bounding_box"}),": Instance of struct ",(0,t.jsx)(n.code,{children:"BoundingBoxCenter"}),". The definition of this struct is given in file ",(0,t.jsx)(n.code,{children:"definitions.h"}),":"]}),"\n"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-c++",children:"struct BoundingBoxCenter\n{\n  Eigen::Vector2d center;\n  float z;\n  float length;\n  float width;\n  float height;\n  float yaw;\n\n  float score;\n  int class_idx;\n  std::string class_name;\n};\n"})}),"\n",(0,t.jsxs)(n.p,{children:["You may have noticed, that position of the object is correctly assigned, but we did not assign the dimensions and heading angle of the object correctly. However, the detection algorithm stores its estimates in ",(0,t.jsx)(n.code,{children:"bounding_box"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["Now, fix the problem and assign the quantities of ",(0,t.jsx)(n.code,{children:"bounding_box"})," to ",(0,t.jsx)(n.code,{children:"object"}),". Modify the code just in between the brackets to avoid any problems."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note:"})," After your implementation, you have to rebuild the ",(0,t.jsx)(n.code,{children:"lidar_detection"})," package, before starting everything again:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Compile the code: ",(0,t.jsx)(n.code,{children:"catkin build"})]}),"\n",(0,t.jsxs)(n.li,{children:["Launch the node: ",(0,t.jsx)(n.code,{children:"roslaunch lidar_detection start_all.launch"})]}),"\n",(0,t.jsxs)(n.li,{children:["Reconfigure RVIZ to display the ",(0,t.jsx)(n.code,{children:"ikaObjectList"})]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Note: You may also store the current RVIZ configuration with ",(0,t.jsx)(n.code,{children:"File"})," -> ",(0,t.jsx)(n.code,{children:"Save Config"})," and load the config via the launch file or with ",(0,t.jsx)(n.code,{children:"Open Config"})," in order to avoid the third step."]}),"\n",(0,t.jsx)(n.p,{children:"In case you correctly implemented the code block, you can see that the detected objects are associated with the estimated bounding box dimensions."}),"\n",(0,t.jsx)(n.h2,{id:"task-2",children:"Task 2"}),"\n",(0,t.jsxs)(n.p,{children:["In the previous task, you corrected the dimensions and orientation of the bounding box. But the object boxes in RVIZ still do not show a correct classification of the detected objects. All objects are still associated with class ",(0,t.jsx)(n.code,{children:"UNKNOWN"}),". Now, lets tackle this problem:"]}),"\n",(0,t.jsxs)(n.p,{children:["Inspect the file ",(0,t.jsx)(n.code,{children:"detector.cpp"})," and search for the ",(0,t.jsx)(n.code,{children:"TASK 2 CODE"})," brackets. This snippets calculates the ",(0,t.jsx)(n.code,{children:"class_idx"})," of a bounding box, which should represent the index of the most probable class value, based on all predicted ",(0,t.jsx)(n.code,{children:"class_scores"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-c++",children:"// START TASK 2 CODE\n\nint class_idx = -1;\n\n// END TASK 2 CODE\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Your task is now to compute the correct value for ",(0,t.jsx)(n.code,{children:"class_idx"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"Hints:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["You can use ",(0,t.jsx)(n.code,{children:"std::max_element"})," to find the position of the maximum of an array."]}),"\n",(0,t.jsxs)(n.li,{children:["In this example the position of the maximum can be used as the new ",(0,t.jsx)(n.code,{children:"class_idx"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["After your implementation, you have to rebuild the ",(0,t.jsx)(n.code,{children:"lidar_detection"})," package, before starting it again:"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["Compile the code: ",(0,t.jsx)(n.code,{children:"catkin build"})]}),"\n",(0,t.jsxs)(n.li,{children:["Launch the node: ",(0,t.jsx)(n.code,{children:"roslaunch lidar_detection start_all.launch"})]}),"\n",(0,t.jsxs)(n.li,{children:["Reconfigure RVIZ to display the ",(0,t.jsx)(n.code,{children:"ikaObjectList"})]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Note: You may also store the current RVIZ configuration with ",(0,t.jsx)(n.code,{children:"File"})," -> ",(0,t.jsx)(n.code,{children:"Save Config"})," and load the config via the launch file or with ",(0,t.jsx)(n.code,{children:"Open Config"})," in order to avoid the third step."]}),"\n",(0,t.jsx)(n.p,{children:"In case you correctly implemented the function, you can see that the detected objects are associated with the estimated classes."}),"\n",(0,t.jsx)("img",{src:"../images/section_2/object_detection/result.png",alt:"Description of image"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Note"}),": You will notice that due to the correct class assignment, much less false positive detections occur. This is because the correct score threshold can be applied which improves the overall performance a lot!"]}),"\n",(0,t.jsx)(n.h2,{id:"wrap-up",children:"Wrap-up"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"You learned the basics about 3D object detection"}),"\n",(0,t.jsxs)(n.li,{children:["You learned about the ",(0,t.jsx)(n.strong,{children:"IkaObject"})," and ",(0,t.jsx)(n.strong,{children:"IkaObjectLists"})," message definitions"]}),"\n",(0,t.jsxs)(n.li,{children:["You learned how to launch a ",(0,t.jsx)(n.strong,{children:"ROS node"})]}),"\n",(0,t.jsxs)(n.li,{children:["You learned to use ",(0,t.jsx)(n.strong,{children:"RVIZ"})," to visualize ",(0,t.jsx)(n.strong,{children:"LiDAR point clouds"})]}),"\n",(0,t.jsxs)(n.li,{children:["You learned to use ",(0,t.jsx)(n.strong,{children:"RVIZ"})," to visualize ",(0,t.jsx)(n.strong,{children:"detected objects"})]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(l,{...e})}):l(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>c,x:()=>a});var s=i(6540);const t={},o=s.createContext(t);function c(e){const n=s.useContext(o);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:c(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);