"use strict";(self.webpackChunkacd=self.webpackChunkacd||[]).push([[2572],{5916:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"introToolsSidebar":[{"type":"link","label":"Getting Started","href":"/Autonomous-Connected-Driving/docs/theory/introduction-tools/getting_started","docId":"theory/introduction-tools/getting_started","unlisted":false},{"type":"category","label":"Introduction","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Concept","href":"/Autonomous-Connected-Driving/docs/theory/introduction-tools/introduction/concept","docId":"theory/introduction-tools/introduction/concept","unlisted":false},{"type":"link","label":"Motivation","href":"/Autonomous-Connected-Driving/docs/theory/introduction-tools/introduction/motivation","docId":"theory/introduction-tools/introduction/motivation","unlisted":false},{"type":"link","label":"A-Model","href":"/Autonomous-Connected-Driving/docs/theory/introduction-tools/introduction/a_model","docId":"theory/introduction-tools/introduction/a_model","unlisted":false},{"type":"link","label":"Preface","href":"/Autonomous-Connected-Driving/docs/theory/introduction-tools/introduction/topics","docId":"theory/introduction-tools/introduction/topics","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/introduction"},{"type":"category","label":"Tools","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Tools Overview","href":"/Autonomous-Connected-Driving/docs/theory/introduction-tools/tools/tools_overview","docId":"theory/introduction-tools/tools/tools_overview","unlisted":false},{"type":"link","label":"Setup for Operating System","href":"/Autonomous-Connected-Driving/docs/theory/introduction-tools/tools/setup_os","docId":"theory/introduction-tools/tools/setup_os","unlisted":false},{"type":"link","label":"Setup ROS Coding Environment","href":"/Autonomous-Connected-Driving/docs/theory/introduction-tools/tools/setup_ros","docId":"theory/introduction-tools/tools/setup_ros","unlisted":false},{"type":"link","label":"Setup Jupyterlab","href":"/Autonomous-Connected-Driving/docs/theory/introduction-tools/tools/setup_jupyterlab","docId":"theory/introduction-tools/tools/setup_jupyterlab","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/tools"}],"sensorSidebar":[{"type":"link","label":"Getting Started","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/getting_started","docId":"theory/sensor-data-processing/getting_started","unlisted":false},{"type":"category","label":"Introduction","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Sensor Data Processing","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/introduction/","docId":"theory/sensor-data-processing/introduction/introduction","unlisted":false},{"type":"link","label":"Goals and Challenges of Environment Perception","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/introduction/goals_challenges","docId":"theory/sensor-data-processing/introduction/goals_challenges","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/introduction-1"},{"type":"category","label":"Image Segmentation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/image_segmentation/introduction","docId":"theory/sensor-data-processing/image_segmentation/introduction","unlisted":false},{"type":"link","label":"Deep Learning","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/image_segmentation/deep_learning","docId":"theory/sensor-data-processing/image_segmentation/deep_learning","unlisted":false},{"type":"link","label":"Training","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/image_segmentation/training","docId":"theory/sensor-data-processing/image_segmentation/training","unlisted":false},{"type":"link","label":"Evaluation","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/image_segmentation/evaluation","docId":"theory/sensor-data-processing/image_segmentation/evaluation","unlisted":false},{"type":"link","label":"Boosting Performance","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/image_segmentation/boosting_performance","docId":"theory/sensor-data-processing/image_segmentation/boosting_performance","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/image-segmentation"},{"type":"category","label":"Semantic Point Cloud Segmentation","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/semantic_point_cloud_segmentation/introduction","docId":"theory/sensor-data-processing/semantic_point_cloud_segmentation/introduction","unlisted":false},{"type":"link","label":"Deep Learning","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/semantic_point_cloud_segmentation/deep_learning","docId":"theory/sensor-data-processing/semantic_point_cloud_segmentation/deep_learning","unlisted":false},{"type":"link","label":"Training","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/semantic_point_cloud_segmentation/training","docId":"theory/sensor-data-processing/semantic_point_cloud_segmentation/training","unlisted":false},{"type":"link","label":"Evaluation","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/semantic_point_cloud_segmentation/evaluation","docId":"theory/sensor-data-processing/semantic_point_cloud_segmentation/evaluation","unlisted":false},{"type":"link","label":"Boosting Performance","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/semantic_point_cloud_segmentation/boosting_performance","docId":"theory/sensor-data-processing/semantic_point_cloud_segmentation/boosting_performance","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/semantic-point-cloud-segmentation"},{"type":"category","label":"Object Detection","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/object_detection/introduction","docId":"theory/sensor-data-processing/object_detection/introduction","unlisted":false},{"type":"link","label":"Introduction","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/object_detection/deep_learning","docId":"theory/sensor-data-processing/object_detection/deep_learning","unlisted":false},{"type":"link","label":"Introduction","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/object_detection/training","docId":"theory/sensor-data-processing/object_detection/training","unlisted":false},{"type":"link","label":"Introduction","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/object_detection/evaluation","docId":"theory/sensor-data-processing/object_detection/evaluation","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/object-detection"},{"type":"category","label":"Point Cloud OGM","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Point Cloud Occupancy Grid Mapping","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/point_cloud_ogm/introduction","docId":"theory/sensor-data-processing/point_cloud_ogm/introduction","unlisted":false},{"type":"link","label":"Occupancy Grid Mapping from 3D Point Clouds: Geometric Inverse Sensor Models","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/point_cloud_ogm/geometric_inverse_sensor","docId":"theory/sensor-data-processing/point_cloud_ogm/geometric_inverse_sensor","unlisted":false},{"type":"link","label":"Point Cloud Occupancy Grid Mapping Using Deep Inverse Sensor Models","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/point_cloud_ogm/deep_inverse_sensor","docId":"theory/sensor-data-processing/point_cloud_ogm/deep_inverse_sensor","unlisted":false},{"type":"link","label":"Training and Evaluating Neural Networks for Occupancy Grid Maps","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/point_cloud_ogm/training_evaluation","docId":"theory/sensor-data-processing/point_cloud_ogm/training_evaluation","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/point-cloud-ogm"},{"type":"category","label":"Camera Based Semantic Grid Mapping","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/camera_based_semantic_grid_mapping/introduction","docId":"theory/sensor-data-processing/camera_based_semantic_grid_mapping/introduction","unlisted":false},{"type":"link","label":"Challenges","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/camera_based_semantic_grid_mapping/challenges","docId":"theory/sensor-data-processing/camera_based_semantic_grid_mapping/challenges","unlisted":false},{"type":"link","label":"Inverse Perspective Mapping","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/camera_based_semantic_grid_mapping/ipm","docId":"theory/sensor-data-processing/camera_based_semantic_grid_mapping/ipm","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/camera-based-semantic-grid-mapping"},{"type":"category","label":"Localization","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction to Localization","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/localization/introduction","docId":"theory/sensor-data-processing/localization/introduction","unlisted":false},{"type":"link","label":"Localization Challenges","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/localization/challenges","docId":"theory/sensor-data-processing/localization/challenges","unlisted":false},{"type":"link","label":"Global Localization","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/localization/global_localization","docId":"theory/sensor-data-processing/localization/global_localization","unlisted":false},{"type":"link","label":"Relative Localization","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/localization/relative_localization","docId":"theory/sensor-data-processing/localization/relative_localization","unlisted":false},{"type":"link","label":"Combination of Localization","href":"/Autonomous-Connected-Driving/docs/theory/sensor-data-processing/localization/combination_localization_approaches","docId":"theory/sensor-data-processing/localization/combination_localization_approaches","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/localization"}],"objectSidebar":[{"type":"link","label":"Getting Started","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/getting_started","docId":"theory/object-fusion-tracking/getting_started","unlisted":false},{"type":"category","label":"Introduction","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/introduction/","docId":"theory/object-fusion-tracking/introduction/introduction","unlisted":false},{"type":"link","label":"Challenges and Fundamentals","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/introduction/fundamentals","docId":"theory/object-fusion-tracking/introduction/fundamentals","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/introduction-2"},{"type":"category","label":"Object Prediction","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/prediction/introduction","docId":"theory/object-fusion-tracking/prediction/introduction","unlisted":false},{"type":"link","label":"Mathematical Notation","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/prediction/mathematical_notion","docId":"theory/object-fusion-tracking/prediction/mathematical_notion","unlisted":false},{"type":"link","label":"Object Description","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/prediction/object_description","docId":"theory/object-fusion-tracking/prediction/object_description","unlisted":false},{"type":"link","label":"Importance of Prediction","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/prediction/importance_prediction","docId":"theory/object-fusion-tracking/prediction/importance_prediction","unlisted":false},{"type":"link","label":"Prediction Equations","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/prediction/prediction_equations","docId":"theory/object-fusion-tracking/prediction/prediction_equations","unlisted":false},{"type":"link","label":"Motion Model and Process Noise Matrix","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/prediction/motion_model","docId":"theory/object-fusion-tracking/prediction/motion_model","unlisted":false},{"type":"link","label":"Integration with ROS","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/prediction/ros_integration","docId":"theory/object-fusion-tracking/prediction/ros_integration","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/object-prediction"},{"type":"category","label":"Object Association ","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction to Object Association","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/association/introduction","docId":"theory/object-fusion-tracking/association/introduction","unlisted":false},{"type":"link","label":"Context and Workflow","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/association/context_workflow","docId":"theory/object-fusion-tracking/association/context_workflow","unlisted":false},{"type":"link","label":"Object Association Approaches","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/association/object_association","docId":"theory/object-fusion-tracking/association/object_association","unlisted":false},{"type":"link","label":"Implementation in ROS","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/association/implementation_ros","docId":"theory/object-fusion-tracking/association/implementation_ros","unlisted":false},{"type":"link","label":"Advanced Topics","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/association/advanced_topics","docId":"theory/object-fusion-tracking/association/advanced_topics","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/object-association-"},{"type":"category","label":"Object Fusion","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Object Fusion","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/fusion/object_fusion","docId":"theory/object-fusion-tracking/fusion/object_fusion","unlisted":false},{"type":"link","label":"Introduction to Object Fusion","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/fusion/introduction","docId":"theory/object-fusion-tracking/fusion/introduction","unlisted":false},{"type":"link","label":"Key Concepts","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/fusion/key_concepts","docId":"theory/object-fusion-tracking/fusion/key_concepts","unlisted":false},{"type":"link","label":"Implementation Steps","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/fusion/implementation_steps","docId":"theory/object-fusion-tracking/fusion/implementation_steps","unlisted":false},{"type":"link","label":"Practical Considerations","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/fusion/practical_considerations","docId":"theory/object-fusion-tracking/fusion/practical_considerations","unlisted":false},{"type":"link","label":"Advanced Topics","href":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/fusion/advanced_topics","docId":"theory/object-fusion-tracking/fusion/advanced_topics","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/object-fusion"}],"vehicleSidebar":[{"type":"link","label":"Getting Started","href":"/Autonomous-Connected-Driving/docs/theory/vehicle-guidance/getting_started","docId":"theory/vehicle-guidance/getting_started","unlisted":false},{"type":"category","label":"Introduction","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Vehicle Guidance","href":"/Autonomous-Connected-Driving/docs/theory/vehicle-guidance/introduction/","docId":"theory/vehicle-guidance/introduction/introduction","unlisted":false},{"type":"link","label":"Goals, Challenges & Fundamentals","href":"/Autonomous-Connected-Driving/docs/theory/vehicle-guidance/introduction/goals_challenges_fundamentals","docId":"theory/vehicle-guidance/introduction/goals_challenges_fundamentals","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/introduction-3"},{"type":"category","label":"Navigation Level","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Vehicle Guidance on Navigation Level","href":"/Autonomous-Connected-Driving/docs/theory/vehicle-guidance/navigation_level/introduction_nav_level","docId":"theory/vehicle-guidance/navigation_level/introduction_nav_level","unlisted":false},{"type":"link","label":"Lanelet2","href":"/Autonomous-Connected-Driving/docs/theory/vehicle-guidance/navigation_level/lanelet2","docId":"theory/vehicle-guidance/navigation_level/lanelet2","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/navigation-level"},{"type":"category","label":"Guidance Level","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction to Guidance-Level Motion Planning","href":"/Autonomous-Connected-Driving/docs/theory/vehicle-guidance/guidance_level/introduction","docId":"theory/vehicle-guidance/guidance_level/introduction","unlisted":false},{"type":"link","label":"The Direct Multiple Shooting Approach","href":"/Autonomous-Connected-Driving/docs/theory/vehicle-guidance/guidance_level/direct_multiple_shooting","docId":"theory/vehicle-guidance/guidance_level/direct_multiple_shooting","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/guidance-level"},{"type":"category","label":"Stabilization Level","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Stabilization Level in Vehicle Control","href":"/Autonomous-Connected-Driving/docs/theory/vehicle-guidance/stabilization_level/introduction","docId":"theory/vehicle-guidance/stabilization_level/introduction","unlisted":false},{"type":"link","label":"Low-, High-, and Bi-Level Stabilization in Vehicle Control","href":"/Autonomous-Connected-Driving/docs/theory/vehicle-guidance/stabilization_level/levels","docId":"theory/vehicle-guidance/stabilization_level/levels","unlisted":false},{"type":"link","label":"Trajectory Control Using Feedback PID Controllers","href":"/Autonomous-Connected-Driving/docs/theory/vehicle-guidance/stabilization_level/trajectory_control","docId":"theory/vehicle-guidance/stabilization_level/trajectory_control","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/stabilization-level"}],"connectedSidebar":[{"type":"link","label":"Getting Started","href":"/Autonomous-Connected-Driving/docs/theory/connected-driving/getting_started","docId":"theory/connected-driving/getting_started","unlisted":false},{"type":"category","label":"Introduction ","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction to Connected Driving","href":"/Autonomous-Connected-Driving/docs/theory/connected-driving/introduction/","docId":"theory/connected-driving/introduction/introduction","unlisted":false},{"type":"link","label":"Connectivity in Automated Driving","href":"/Autonomous-Connected-Driving/docs/theory/connected-driving/introduction/categories","docId":"theory/connected-driving/introduction/categories","unlisted":false},{"type":"link","label":"Challenges of Automated and Connected Driving","href":"/Autonomous-Connected-Driving/docs/theory/connected-driving/introduction/challenges","docId":"theory/connected-driving/introduction/challenges","unlisted":false},{"type":"link","label":"Key Terminology in Connected Driving","href":"/Autonomous-Connected-Driving/docs/theory/connected-driving/introduction/key_term","docId":"theory/connected-driving/introduction/key_term","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/introduction-"},{"type":"category","label":"Collective Cloud Functions","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Introduction to Collective Cloud Functions","href":"/Autonomous-Connected-Driving/docs/theory/connected-driving/collective_cloud_functions/fundamentals","docId":"theory/connected-driving/collective_cloud_functions/fundamentals","unlisted":false},{"type":"link","label":"Understanding MQTT Protocol for Connected Mobility","href":"/Autonomous-Connected-Driving/docs/theory/connected-driving/collective_cloud_functions/mqtt","docId":"theory/connected-driving/collective_cloud_functions/mqtt","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/collective-cloud-functions"},{"type":"category","label":"V2I Communication","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Infrastructure-to-Vehicle Communication","href":"/Autonomous-Connected-Driving/docs/theory/connected-driving/v2i_communication/introduction","docId":"theory/connected-driving/v2i_communication/introduction","unlisted":false},{"type":"link","label":"SPAT and MAP Extended Messages in Connected Driving","href":"/Autonomous-Connected-Driving/docs/theory/connected-driving/v2i_communication/spatem","docId":"theory/connected-driving/v2i_communication/spatem","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/v2i-communication"}],"cppSidebar":[{"type":"link","label":"Getting Started","href":"/Autonomous-Connected-Driving/docs/cpp/getting_started","docId":"cpp/getting_started","unlisted":false},{"type":"category","label":"Introduction","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Syntax","href":"/Autonomous-Connected-Driving/docs/cpp/Introduction/syntax","docId":"cpp/Introduction/syntax","unlisted":false},{"type":"link","label":"Statements","href":"/Autonomous-Connected-Driving/docs/cpp/Introduction/statement","docId":"cpp/Introduction/statement","unlisted":false},{"type":"link","label":"Output for Text","href":"/Autonomous-Connected-Driving/docs/cpp/Introduction/print_text","docId":"cpp/Introduction/print_text","unlisted":false},{"type":"link","label":"Output for Numbers","href":"/Autonomous-Connected-Driving/docs/cpp/Introduction/print_numbers","docId":"cpp/Introduction/print_numbers","unlisted":false},{"type":"link","label":"New Lines","href":"/Autonomous-Connected-Driving/docs/cpp/Introduction/new_lines","docId":"cpp/Introduction/new_lines","unlisted":false},{"type":"link","label":"Pointers","href":"/Autonomous-Connected-Driving/docs/cpp/Introduction/pointers","docId":"cpp/Introduction/pointers","unlisted":false}]}],"pySidebar":[{"type":"link","label":"Getting Started","href":"/Autonomous-Connected-Driving/docs/python/getting_started","docId":"python/getting_started","unlisted":false},{"type":"link","label":"Intro","href":"/Autonomous-Connected-Driving/docs/python/Intro","docId":"python/Intro","unlisted":false},{"type":"link","label":"Syntax","href":"/Autonomous-Connected-Driving/docs/python/Syntax","docId":"python/Syntax","unlisted":false},{"type":"link","label":"Comments","href":"/Autonomous-Connected-Driving/docs/python/Comments","docId":"python/Comments","unlisted":false},{"type":"link","label":"Variables","href":"/Autonomous-Connected-Driving/docs/python/Variables","docId":"python/Variables","unlisted":false}],"rosSidebar":[{"type":"link","label":"Getting Started with ROS","href":"/Autonomous-Connected-Driving/docs/ros/getting_started","docId":"ros/getting_started","unlisted":false},{"type":"link","label":"ROS Introduction","href":"/Autonomous-Connected-Driving/docs/ros/introduction","docId":"ros/introduction","unlisted":false},{"type":"link","label":"ROS JupyROS","href":"/Autonomous-Connected-Driving/docs/ros/ros_jupyros","docId":"ros/ros_jupyros","unlisted":false},{"type":"link","label":"ROS Setup","href":"/Autonomous-Connected-Driving/docs/ros/ros_setup","docId":"ros/ros_setup","unlisted":false},{"type":"link","label":"ROS Foundations","href":"/Autonomous-Connected-Driving/docs/ros/ros_foundations","docId":"ros/ros_foundations","unlisted":false},{"type":"link","label":"ROS Filesystem","href":"/Autonomous-Connected-Driving/docs/ros/filesystem","docId":"ros/filesystem","unlisted":false},{"type":"link","label":"ROS Master","href":"/Autonomous-Connected-Driving/docs/ros/master","docId":"ros/master","unlisted":false},{"type":"link","label":"ROS Node","href":"/Autonomous-Connected-Driving/docs/ros/nodes","docId":"ros/nodes","unlisted":false},{"type":"link","label":"parameter_server","href":"/Autonomous-Connected-Driving/docs/ros/parameter_server","docId":"ros/parameter_server","unlisted":false},{"type":"link","label":"messages","href":"/Autonomous-Connected-Driving/docs/ros/messages","docId":"ros/messages","unlisted":false},{"type":"link","label":"topics","href":"/Autonomous-Connected-Driving/docs/ros/topics","docId":"ros/topics","unlisted":false},{"type":"link","label":"services","href":"/Autonomous-Connected-Driving/docs/ros/services","docId":"ros/services","unlisted":false},{"type":"link","label":"launch","href":"/Autonomous-Connected-Driving/docs/ros/launch","docId":"ros/launch","unlisted":false},{"type":"link","label":"gui_tools","href":"/Autonomous-Connected-Driving/docs/ros/gui_tools","docId":"ros/gui_tools","unlisted":false},{"type":"link","label":"ros_bags","href":"/Autonomous-Connected-Driving/docs/ros/ros_bags","docId":"ros/ros_bags","unlisted":false}],"ros2Sidebar":[{"type":"link","label":"Getting Started","href":"/Autonomous-Connected-Driving/docs/ros2/getting_started","docId":"ros2/getting_started","unlisted":false},{"type":"link","label":"Introduction to ROS2","href":"/Autonomous-Connected-Driving/docs/ros2/intro","docId":"ros2/intro","unlisted":false},{"type":"link","label":"ROS 2 Installation Guide","href":"/Autonomous-Connected-Driving/docs/ros2/installation","docId":"ros2/installation","unlisted":false},{"type":"link","label":"Using Docker for ROS2 Development","href":"/Autonomous-Connected-Driving/docs/ros2/docker","docId":"ros2/docker","unlisted":false},{"type":"link","label":"Building a ROS2 Workspace Using colcon","href":"/Autonomous-Connected-Driving/docs/ros2/build","docId":"ros2/build","unlisted":false},{"type":"link","label":"Creating Your First ROS2 Publisher Node","href":"/Autonomous-Connected-Driving/docs/ros2/publisher","docId":"ros2/publisher","unlisted":false},{"type":"link","label":"Creating Your First ROS2 Subscriber Node","href":"/Autonomous-Connected-Driving/docs/ros2/subscriber","docId":"ros2/subscriber","unlisted":false},{"type":"link","label":"Understanding ROS 2 Packages","href":"/Autonomous-Connected-Driving/docs/ros2/package","docId":"ros2/package","unlisted":false},{"type":"link","label":"CMakeLists.txt in ROS2","href":"/Autonomous-Connected-Driving/docs/ros2/cmakelists","docId":"ros2/cmakelists","unlisted":false},{"type":"link","label":"How to Start ROS2 Nodes","href":"/Autonomous-Connected-Driving/docs/ros2/start_ros2_nodes","docId":"ros2/start_ros2_nodes","unlisted":false}],"taskSidebar":[{"type":"link","label":"Getting Started","href":"/Autonomous-Connected-Driving/docs/task/getting_started","docId":"task/getting_started","unlisted":false},{"type":"category","label":"Introduction & Tools","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"ROS Setup","href":"/Autonomous-Connected-Driving/docs/task/intro/ROS-Setup","docId":"task/intro/ROS-Setup","unlisted":false},{"type":"link","label":"ROS Foundations","href":"/Autonomous-Connected-Driving/docs/task/intro/ROS-Foundations","docId":"task/intro/ROS-Foundations","unlisted":false},{"type":"link","label":"ROS GUI Tools","href":"/Autonomous-Connected-Driving/docs/task/intro/ROS-GUI-Tools","docId":"task/intro/ROS-GUI-Tools","unlisted":false},{"type":"link","label":"ROS Nodes in C++","href":"/Autonomous-Connected-Driving/docs/task/intro/ROS-Nodes-in-Cpp","docId":"task/intro/ROS-Nodes-in-Cpp","unlisted":false},{"type":"link","label":"ROS Bags","href":"/Autonomous-Connected-Driving/docs/task/intro/ROS-Bags","docId":"task/intro/ROS-Bags","unlisted":false},{"type":"link","label":"ROS Application","href":"/Autonomous-Connected-Driving/docs/task/intro/ROS-Application","docId":"task/intro/ROS-Application","unlisted":false},{"type":"link","label":"ROS2 Outlook","href":"/Autonomous-Connected-Driving/docs/task/intro/ROS2-Outlook","docId":"task/intro/ROS2-Outlook","unlisted":false},{"type":"link","label":"ROS2 Tutorial in C++","href":"/Autonomous-Connected-Driving/docs/task/intro/ROS2-Tutorial-in-Cpp","docId":"task/intro/ROS2-Tutorial-in-Cpp","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/introduction--tools"},{"type":"category","label":"Sensor Data Processing","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Camera-based Semantic Grid Mapping","href":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Camera-based-Semantic-Grid-Mapping","docId":"task/sensor_data_processing/Camera-based-Semantic-Grid-Mapping","unlisted":false},{"type":"link","label":"Deep Learning-based Point Cloud Occupancy Grid Mapping","href":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Deep-OGM","docId":"task/sensor_data_processing/Deep-OGM","unlisted":false},{"type":"link","label":"Geometric Point Cloud Occupancy Grid Mapping","href":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Geometric-OGM","docId":"task/sensor_data_processing/Geometric-OGM","unlisted":false},{"type":"link","label":"Localization","href":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Localization","docId":"task/sensor_data_processing/Localization","unlisted":false},{"type":"link","label":"Object Detection","href":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Object-Detection","docId":"task/sensor_data_processing/Object-Detection","unlisted":false},{"type":"link","label":"Semantic Image Segmentation Applied on Camera Images","href":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Semantic-Image-Segmentation","docId":"task/sensor_data_processing/Semantic-Image-Segmentation","unlisted":false},{"type":"link","label":"Perform Deep Learning Based Semantic Point Cloud Segmentation","href":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Semantic-Point-Cloud-Segmentation","docId":"task/sensor_data_processing/Semantic-Point-Cloud-Segmentation","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/sensor-data-processing"},{"type":"category","label":"Object Fusion Tracking","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Object Prediction","href":"/Autonomous-Connected-Driving/docs/task/object_fusion_tracking/Object-Prediction","docId":"task/object_fusion_tracking/Object-Prediction","unlisted":false},{"type":"link","label":"Object Association","href":"/Autonomous-Connected-Driving/docs/task/object_fusion_tracking/Object-Association","docId":"task/object_fusion_tracking/Object-Association","unlisted":false},{"type":"link","label":"Object Fusion","href":"/Autonomous-Connected-Driving/docs/task/object_fusion_tracking/Object-Fusion","docId":"task/object_fusion_tracking/Object-Fusion","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/object-fusion-tracking"},{"type":"category","label":"Vehicle Guidance","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Trajectory Control","href":"/Autonomous-Connected-Driving/docs/task/vehicle_guidance/Trajectory-Control","docId":"task/vehicle_guidance/Trajectory-Control","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/vehicle-guidance"},{"type":"category","label":"Connected Driving","collapsible":true,"collapsed":true,"items":[{"type":"link","label":"Cloud-Based Object Fusion","href":"/Autonomous-Connected-Driving/docs/task/connected_driving/Cloud-Based-Object-Fusion","docId":"task/connected_driving/Cloud-Based-Object-Fusion","unlisted":false},{"type":"link","label":"SPaT/MAP Processing for Trajectory Planning","href":"/Autonomous-Connected-Driving/docs/task/connected_driving/SPaT-Processing","docId":"task/connected_driving/SPaT-Processing","unlisted":false}],"href":"/Autonomous-Connected-Driving/docs/category/connected-driving"}]},"docs":{"cpp/getting_started":{"id":"cpp/getting_started","title":"Getting Started","description":"1. Introduction and Basics","sidebar":"cppSidebar"},"cpp/Introduction/new_lines":{"id":"cpp/Introduction/new_lines","title":"New Lines","description":"","sidebar":"cppSidebar"},"cpp/Introduction/pointers":{"id":"cpp/Introduction/pointers","title":"Pointers","description":"---","sidebar":"cppSidebar"},"cpp/Introduction/print_numbers":{"id":"cpp/Introduction/print_numbers","title":"Output for Numbers","description":"---","sidebar":"cppSidebar"},"cpp/Introduction/print_text":{"id":"cpp/Introduction/print_text","title":"Output for Text","description":"---","sidebar":"cppSidebar"},"cpp/Introduction/statement":{"id":"cpp/Introduction/statement","title":"Statements","description":"---","sidebar":"cppSidebar"},"cpp/Introduction/syntax":{"id":"cpp/Introduction/syntax","title":"Syntax","description":"---","sidebar":"cppSidebar"},"python/Comments":{"id":"python/Comments","title":"Comments","description":"","sidebar":"pySidebar"},"python/getting_started":{"id":"python/getting_started","title":"Getting Started","description":"1. Introduction and Basics","sidebar":"pySidebar"},"python/Intro":{"id":"python/Intro","title":"Intro","description":"","sidebar":"pySidebar"},"python/Syntax":{"id":"python/Syntax","title":"Syntax","description":"","sidebar":"pySidebar"},"python/Variables":{"id":"python/Variables","title":"Variables","description":"","sidebar":"pySidebar"},"ros/filesystem":{"id":"ros/filesystem","title":"ROS Filesystem","description":"The ROS1 filesystem forms the backbone of the ROS (Robot Operating System) architecture, defining the structure and organization of files and directories used within a ROS workspace. Understanding the ROS1 filesystem is critical for managing projects, developing packages, and collaborating effectively in robotics development. This document provides a comprehensive overview of the ROS1 filesystem, including its key components and best practices.","sidebar":"rosSidebar"},"ros/getting_started":{"id":"ros/getting_started","title":"Getting Started with ROS","description":"Welcome to the world of ROS (Robot Operating System), a flexible and modular framework for robotics development. This guide will help you set up your environment, understand the core concepts, and start creating and managing robotics applications with confidence.","sidebar":"rosSidebar"},"ros/gui_tools":{"id":"ros/gui_tools","title":"gui_tools","description":"","sidebar":"rosSidebar"},"ros/introduction":{"id":"ros/introduction","title":"ROS Introduction","description":"The Robot Operating System (ROS1) is a flexible and modular framework for writing robot software. It provides the tools, libraries, and conventions needed to develop complex and robust robotic applications. ROS1 was first introduced in 2007 and has since been widely adopted in both academia and industry as a standard platform for robotics research and development.","sidebar":"rosSidebar"},"ros/launch":{"id":"ros/launch","title":"launch","description":"","sidebar":"rosSidebar"},"ros/master":{"id":"ros/master","title":"ROS Master","description":"The ROS1 Master is a pivotal component of the ROS (Robot Operating System) communication framework. Acting as the central registry and coordination point, it enables seamless communication between ROS nodes in a distributed robotic system. The Master is responsible for the discovery and management of nodes, topics, services, and parameters, ensuring that every component in the ROS network is aware of the others.","sidebar":"rosSidebar"},"ros/messages":{"id":"ros/messages","title":"messages","description":"","sidebar":"rosSidebar"},"ros/nodes":{"id":"ros/nodes","title":"ROS Node","description":"In ROS (Robot Operating System) 1, a node is a fundamental building block of the ROS computational graph. Nodes are processes that perform computation and communicate with other nodes in a distributed system. This modularity allows developers to build complex robotic systems by composing multiple nodes, each responsible for a specific task, such as perception, control, or actuation.","sidebar":"rosSidebar"},"ros/parameter_server":{"id":"ros/parameter_server","title":"parameter_server","description":"","sidebar":"rosSidebar"},"ros/ros_bags":{"id":"ros/ros_bags","title":"ros_bags","description":"","sidebar":"rosSidebar"},"ros/ros_foundations":{"id":"ros/ros_foundations","title":"ROS Foundations","description":"The Robot Operating System (ROS) is an open-source framework designed for developing robotic software. ROS1 (the original version of ROS) provides tools, libraries, and conventions to simplify the task of creating complex robot behavior across various robotic platforms.","sidebar":"rosSidebar"},"ros/ros_jupyros":{"id":"ros/ros_jupyros","title":"ROS JupyROS","description":"Introduction","sidebar":"rosSidebar"},"ros/ros_setup":{"id":"ros/ros_setup","title":"ROS Setup","description":"This documentation provides a detailed guide for setting up and configuring ROS1 (Robot Operating System) on your machine. Follow these instructions to ensure a successful installation and configuration.","sidebar":"rosSidebar"},"ros/services":{"id":"ros/services","title":"services","description":"","sidebar":"rosSidebar"},"ros/topics":{"id":"ros/topics","title":"topics","description":"","sidebar":"rosSidebar"},"ros2/build":{"id":"ros2/build","title":"Building a ROS2 Workspace Using colcon","description":"Building a ROS2 workspace is a crucial step in developing ROS2 applications. The process involves setting up the workspace, adding packages, and using colcon to compile the code. This documentation provides a comprehensive guide for building a ROS2 workspace using colcon, suitable for both beginners and advanced users.","sidebar":"ros2Sidebar"},"ros2/cmakelists":{"id":"ros2/cmakelists","title":"CMakeLists.txt in ROS2","description":"In ROS2, CMakeLists.txt plays a pivotal role in defining the build configuration of your package. It is a CMake script used by the ament build system to compile and link your code, manage dependencies, and define installation rules. A well-configured CMakeLists.txt ensures smooth development and deployment of your ROS2 package.","sidebar":"ros2Sidebar"},"ros2/docker":{"id":"ros2/docker","title":"Using Docker for ROS2 Development","description":"Docker simplifies the setup and management of development environments by providing isolated, portable, and consistent containers. Using Docker for ROS2 development ensures that your environment is easy to replicate and maintain, especially when collaborating across different systems.","sidebar":"ros2Sidebar"},"ros2/getting_started":{"id":"ros2/getting_started","title":"Getting Started","description":"This guide will help you get up to speed with Robot Operating System 2 (ROS2). Whether you are a beginner or an experienced robotics enthusiast, this step-by-step guide is designed to cover the essentials and beyond.","sidebar":"ros2Sidebar"},"ros2/installation":{"id":"ros2/installation","title":"ROS 2 Installation Guide","description":"This comprehensive guide provides step-by-step instructions to install ROS 2 on your machine, tailored to common platforms like Ubuntu. This guide caters to both beginners and advanced users by ensuring clarity and technical depth.","sidebar":"ros2Sidebar"},"ros2/intro":{"id":"ros2/intro","title":"Introduction to ROS2","description":"The Robot Operating System 2 (ROS2) is a powerful, modular framework designed to support the development of robotic systems. Building on the foundations of its predecessor, ROS1, ROS2 provides a robust and scalable architecture suitable for modern robotics applications. This documentation introduces ROS2, its features, and key differences from ROS1, making it accessible to both beginners and advanced users.","sidebar":"ros2Sidebar"},"ros2/package":{"id":"ros2/package","title":"Understanding ROS 2 Packages","description":"ROS 2 (Robot Operating System 2) packages are fundamental units of software organization and deployment in ROS 2 applications. They allow developers to group together nodes, launch files, message and service definitions, and other resources required for robotic applications. This documentation provides a comprehensive guide to understanding the structure of ROS 2 packages and creating your own packages, catering to both beginners and advanced users.","sidebar":"ros2Sidebar"},"ros2/publisher":{"id":"ros2/publisher","title":"Creating Your First ROS2 Publisher Node","description":"This guide will walk you through creating your first ROS2 publisher node. Publishers are essential components of the ROS2 communication system, as they send messages to specific topics for other nodes to receive. This tutorial is designed for both beginners and advanced users, offering clarity and technical depth.","sidebar":"ros2Sidebar"},"ros2/start_ros2_nodes":{"id":"ros2/start_ros2_nodes","title":"How to Start ROS2 Nodes","description":"This guide provides a comprehensive understanding of starting ROS2 nodes, verifying their connectivity, and debugging communication. Designed for both beginners and advanced users, this documentation ensures clarity and technical depth for users at all levels.","sidebar":"ros2Sidebar"},"ros2/subscriber":{"id":"ros2/subscriber","title":"Creating Your First ROS2 Subscriber Node","description":"In ROS 2 (Robot Operating System 2), the Subscriber node is an essential component used to receive and process messages published by a Publisher node. This documentation provides a detailed guide for setting up and implementing a ROS 2 Subscriber node, catering to both beginners and advanced users. By the end of this guide, you will understand how to create a Subscriber node, process incoming messages, and integrate it into a larger ROS 2 application.","sidebar":"ros2Sidebar"},"task/connected_driving/Cloud-Based-Object-Fusion":{"id":"task/connected_driving/Cloud-Based-Object-Fusion","title":"Cloud-Based Object Fusion","description":"ROS1","sidebar":"taskSidebar"},"task/connected_driving/SPaT-Processing":{"id":"task/connected_driving/SPaT-Processing","title":"SPaT/MAP Processing for Trajectory Planning","description":"ROS1","sidebar":"taskSidebar"},"task/getting_started":{"id":"task/getting_started","title":"Getting Started","description":"","sidebar":"taskSidebar"},"task/intro/ROS-Application":{"id":"task/intro/ROS-Application","title":"ROS Application","description":"ROS1","sidebar":"taskSidebar"},"task/intro/ROS-Bags":{"id":"task/intro/ROS-Bags","title":"ROS Bags","description":"ROS1","sidebar":"taskSidebar"},"task/intro/ROS-Foundations":{"id":"task/intro/ROS-Foundations","title":"ROS Foundations","description":"ROS1","sidebar":"taskSidebar"},"task/intro/ROS-GUI-Tools":{"id":"task/intro/ROS-GUI-Tools","title":"ROS GUI Tools","description":"ROS1","sidebar":"taskSidebar"},"task/intro/ROS-Nodes-in-Cpp":{"id":"task/intro/ROS-Nodes-in-Cpp","title":"ROS Nodes in C++","description":"ROS1","sidebar":"taskSidebar"},"task/intro/ROS-Setup":{"id":"task/intro/ROS-Setup","title":"ROS Setup","description":"ROS1","sidebar":"taskSidebar"},"task/intro/ROS2-Outlook":{"id":"task/intro/ROS2-Outlook","title":"ROS2 Outlook","description":"ROS1","sidebar":"taskSidebar"},"task/intro/ROS2-Tutorial-in-Cpp":{"id":"task/intro/ROS2-Tutorial-in-Cpp","title":"ROS2 Tutorial in C++","description":"ROS2","sidebar":"taskSidebar"},"task/object_fusion_tracking/Object-Association":{"id":"task/object_fusion_tracking/Object-Association","title":"Object Association","description":"ROS1","sidebar":"taskSidebar"},"task/object_fusion_tracking/Object-Fusion":{"id":"task/object_fusion_tracking/Object-Fusion","title":"Object Fusion","description":"ROS1","sidebar":"taskSidebar"},"task/object_fusion_tracking/Object-Prediction":{"id":"task/object_fusion_tracking/Object-Prediction","title":"Object Prediction","description":"ROS1","sidebar":"taskSidebar"},"task/sensor_data_processing/Camera-based-Semantic-Grid-Mapping":{"id":"task/sensor_data_processing/Camera-based-Semantic-Grid-Mapping","title":"Camera-based Semantic Grid Mapping","description":"ROS2  Instructions","sidebar":"taskSidebar"},"task/sensor_data_processing/Deep-OGM":{"id":"task/sensor_data_processing/Deep-OGM","title":"Deep Learning-based Point Cloud Occupancy Grid Mapping","description":"ROS1","sidebar":"taskSidebar"},"task/sensor_data_processing/Geometric-OGM":{"id":"task/sensor_data_processing/Geometric-OGM","title":"Geometric Point Cloud Occupancy Grid Mapping","description":"ROS1","sidebar":"taskSidebar"},"task/sensor_data_processing/Localization":{"id":"task/sensor_data_processing/Localization","title":"Localization","description":"Combining GNSS and LiDAR-Odometry for Frequent Pose Estimation","sidebar":"taskSidebar"},"task/sensor_data_processing/Object-Detection":{"id":"task/sensor_data_processing/Object-Detection","title":"Object Detection","description":"ROS1","sidebar":"taskSidebar"},"task/sensor_data_processing/Semantic-Image-Segmentation":{"id":"task/sensor_data_processing/Semantic-Image-Segmentation","title":"Semantic Image Segmentation Applied on Camera Images","description":"ROS2","sidebar":"taskSidebar"},"task/sensor_data_processing/Semantic-Point-Cloud-Segmentation":{"id":"task/sensor_data_processing/Semantic-Point-Cloud-Segmentation","title":"Perform Deep Learning Based Semantic Point Cloud Segmentation","description":"ROS2","sidebar":"taskSidebar"},"task/vehicle_guidance/Trajectory-Control":{"id":"task/vehicle_guidance/Trajectory-Control","title":"Trajectory Control","description":"ROS1","sidebar":"taskSidebar"},"theory/connected-driving/collective_cloud_functions/fundamentals":{"id":"theory/connected-driving/collective_cloud_functions/fundamentals","title":"Introduction to Collective Cloud Functions","description":"Collective cloud functions are pivotal in the realm of automated driving, serving as the backbone for centralized data processing and distribution. These functions facilitate the seamless integration of data from multiple sources, ensuring that connected vehicles and infrastructure components operate with enhanced intelligence and coordination. The distinguishing features of collective cloud functions are:","sidebar":"connectedSidebar"},"theory/connected-driving/collective_cloud_functions/mqtt":{"id":"theory/connected-driving/collective_cloud_functions/mqtt","title":"Understanding MQTT Protocol for Connected Mobility","description":"In the rapidly evolving landscape of connected mobility, efficient and reliable communication protocols are paramount. The Message Queuing Telemetry Transport (MQTT) protocol stands out as a lightweight, scalable solution tailored for the Internet of Things (IoT). This documentation delves into the intricacies of MQTT, exploring its core features, architectural components, quality of service levels, and its seamless integration with the Robot Operating System (ROS). Whether you\'re a beginner embarking on your IoT journey or an advanced developer seeking to optimize your connected mobility systems, this guide offers comprehensive insights to enhance your understanding and application of MQTT.","sidebar":"connectedSidebar"},"theory/connected-driving/getting_started":{"id":"theory/connected-driving/getting_started","title":"Getting Started","description":"Introduction to Connected Driving","sidebar":"connectedSidebar"},"theory/connected-driving/introduction/categories":{"id":"theory/connected-driving/introduction/categories","title":"Connectivity in Automated Driving","description":"Connectivity plays a pivotal role in automated driving, enabling a multitude of functions that enhance efficiency, safety, and overall functionality. These connectivity-enabled functions can be categorized into Cooperative Functions, Collective Functions, and Supportive Functions. This documentation delves into each category, providing comprehensive insights supported by concepts from the ACDC course material.","sidebar":"connectedSidebar"},"theory/connected-driving/introduction/challenges":{"id":"theory/connected-driving/introduction/challenges","title":"Challenges of Automated and Connected Driving","description":"Automated and connected driving technologies are at the forefront of transforming the future of mobility. These innovations promise significant advancements in safety, efficiency, comfort, and environmental sustainability. By harnessing cutting-edge technologies, automated and connected driving systems aim to mitigate human error, optimize traffic flow, and reduce the environmental impact of transportation. However, the path to fully realizing these benefits is laden with numerous challenges spanning technological, infrastructural, regulatory, and implementation domains. Addressing these challenges is essential for the successful integration and widespread adoption of automated and connected driving systems.","sidebar":"connectedSidebar"},"theory/connected-driving/introduction/introduction":{"id":"theory/connected-driving/introduction/introduction","title":"Introduction to Connected Driving","description":"Connected Driving represents a pivotal advancement in the evolution of automated vehicles, integrating sophisticated communication technologies to enhance safety, efficiency, and user experience. This module delves into the fundamental concepts, objectives, and advantages of incorporating connected functionalities into autonomous systems. By understanding these core elements, learners can effectively contribute to the development and implementation of next-generation transportation solutions.","sidebar":"connectedSidebar"},"theory/connected-driving/introduction/key_term":{"id":"theory/connected-driving/introduction/key_term","title":"Key Terminology in Connected Driving","description":"Connected driving leverages advanced communication technologies to enhance vehicular interactions and transportation infrastructure. This documentation provides a comprehensive overview of essential concepts and terminologies associated with connected driving, catering to both beginners and advanced users.","sidebar":"connectedSidebar"},"theory/connected-driving/v2i_communication/introduction":{"id":"theory/connected-driving/v2i_communication/introduction","title":"Infrastructure-to-Vehicle Communication","description":"Infrastructure-to-Vehicle (I2V) communication is a pivotal component in the landscape of connected and automated driving. By facilitating the exchange of critical data between road infrastructure, vehicles, and other traffic participants, I2V aims to significantly enhance road safety and traffic efficiency. This documentation delves into the standardized message formats, benefits, and challenges associated with I2V communication, as well as its potential to complement and augment vehicle sensor systems.","sidebar":"connectedSidebar"},"theory/connected-driving/v2i_communication/spatem":{"id":"theory/connected-driving/v2i_communication/spatem","title":"SPAT and MAP Extended Messages in Connected Driving","description":"In the rapidly evolving landscape of automated and connected driving, effective communication between vehicles and traffic infrastructure is paramount. Two pivotal components in this communication framework are the Signal Phase and Timing (SPAT) Extended Message and the MAP Extended Message. These messages enable seamless Vehicle-to-Infrastructure (V2I) interactions, which are essential for optimizing traffic flow, enhancing road safety, and paving the way for fully autonomous transportation systems.","sidebar":"connectedSidebar"},"theory/introduction-tools/getting_started":{"id":"theory/introduction-tools/getting_started","title":"Getting Started","description":"","sidebar":"introToolsSidebar"},"theory/introduction-tools/introduction/a_model":{"id":"theory/introduction-tools/introduction/a_model","title":"A-Model","description":"The Automated and Connected Driving Challenges (ACDC) course is designed to provide comprehensive education on the methodologies and tools essential for understanding and advancing automated and connected vehicle systems. This documentation serves as a guide to understanding the course\'s structure, objectives, and key components such as the A-Model framework and the integration of the Robot Operating System (ROS).","sidebar":"introToolsSidebar"},"theory/introduction-tools/introduction/concept":{"id":"theory/introduction-tools/introduction/concept","title":"Concept","description":"","sidebar":"introToolsSidebar"},"theory/introduction-tools/introduction/motivation":{"id":"theory/introduction-tools/introduction/motivation","title":"Motivation","description":"Automated and connected driving offers transformative potential in the transportation domain. This documentation explores the motivations behind these innovations, focusing on their safety, efficiency, environmental, and accessibility benefits. Additionally, it delves into the SAE levels of driving automation, a standardized framework to guide progress in automated driving development.","sidebar":"introToolsSidebar"},"theory/introduction-tools/introduction/topics":{"id":"theory/introduction-tools/introduction/topics","title":"Preface","description":"This Knowledge-Base is designed to serve as an all-encompassing guide, providing in-depth insights into the concepts, tools, and methodologies that underpin the Automated and Connected Driving (ACD) curriculum. Whether you are a beginner stepping into the world of autonomous systems or an experienced professional seeking to deepen your expertise, this resource offers structured, detailed, and actionable content to guide your learning journey.","sidebar":"introToolsSidebar"},"theory/introduction-tools/tools/setup_jupyterlab":{"id":"theory/introduction-tools/tools/setup_jupyterlab","title":"Setup Jupyterlab","description":"","sidebar":"introToolsSidebar"},"theory/introduction-tools/tools/setup_os":{"id":"theory/introduction-tools/tools/setup_os","title":"Setup for Operating System","description":"This part provides a structured and user-friendly guide to setting up the required environment for the ACDC programming exercises, focusing on ROS and Jupyter Notebook tasks. It caters to users on different operating systems, offering detailed explanations, installation steps, and troubleshooting tips.","sidebar":"introToolsSidebar"},"theory/introduction-tools/tools/setup_ros":{"id":"theory/introduction-tools/tools/setup_ros","title":"Setup ROS Coding Environment","description":"This guide provides a detailed walkthrough for setting up the ROS coding environment required for the ACDC course. It includes instructions for installing and configuring essential tools like Docker, Visual Studio Code (VS Code), and related scripts, ensuring a seamless development experience with ROS and ROS 2.","sidebar":"introToolsSidebar"},"theory/introduction-tools/tools/tools_overview":{"id":"theory/introduction-tools/tools/tools_overview","title":"Tools Overview","description":"Automated and Connected Driving Challenges (ACDC) require the integration of various software tools to address complex tasks. This documentation provides an in-depth overview of the tools used in the ACDC course, focusing on their functionalities, applications, and role in learning and development. The tools covered include edX, Jupyter Notebooks, GitHub, Robot Operating System (ROS), Docker, and various programming languages.","sidebar":"introToolsSidebar"},"theory/object-fusion-tracking/association/advanced_topics":{"id":"theory/object-fusion-tracking/association/advanced_topics","title":"Advanced Topics","description":"As multi-sensor data fusion systems become increasingly complex and integral to robotic applications, addressing advanced topics and optimization strategies is essential for enhancing performance, scalability, and robustness. This chapter delves into sophisticated techniques for fine-tuning object association thresholds, integrating advanced fusion strategies, optimizing scalability and performance, enhancing system robustness, and implementing comprehensive testing and validation methodologies. These advanced considerations ensure that object association mechanisms remain effective in dynamic and challenging environments, thereby elevating the overall efficacy of robotic perception and tracking systems.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/association/context_workflow":{"id":"theory/object-fusion-tracking/association/context_workflow","title":"Context and Workflow","description":"Understanding the context and workflow of multi-sensor fusion is essential for developing robust and accurate perception systems in robotics and autonomous applications. This chapter provides a comprehensive explanation of the multi-sensor fusion pipeline, emphasizing two critical components: Temporal Alignment and Object Association. These components ensure that data from various sensors are synchronized and accurately linked, facilitating coherent and reliable environmental understanding.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/association/implementation_ros":{"id":"theory/object-fusion-tracking/association/implementation_ros","title":"Implementation in ROS","description":"Implementing object association techniques within the Robot Operating System (ROS) framework facilitates seamless integration, real-time processing, and robust communication between various robotic components. This chapter provides a comprehensive guide to implementing Intersection over Union (IoU) and Mahalanobis Distance-based object association within ROS. It covers the selection of appropriate ROS versions and programming languages, setup requirements, step-by-step implementation procedures, and practical code examples to aid both beginners and advanced users in developing efficient object association modules.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/association/introduction":{"id":"theory/object-fusion-tracking/association/introduction","title":"Introduction to Object Association","description":"Object association is a critical component in the realm of multi-sensor data fusion, playing a pivotal role in enhancing the accuracy and reliability of perception systems in robotics. By effectively correlating data from various sensors, object association ensures that the information integrated into robotic systems is coherent, consistent, and actionable. This document delves into the significance of object association, its application within robotic frameworks like ROS (Robot Operating System), and provides an in-depth exploration of two fundamental techniques: Intersection over Union (IoU) and Mahalanobis Distance, particularly within the Kalman filter framework.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/association/object_association":{"id":"theory/object-fusion-tracking/association/object_association","title":"Object Association Approaches","description":"Object association is a fundamental aspect of multi-sensor data fusion, playing a crucial role in ensuring accurate state estimation and reliable data integration within robotic systems. This chapter provides an in-depth exploration of various object association techniques, emphasizing their importance, methodologies, and practical implementations. Specifically, it delves into the Intersection over Union (IoU) and Mahalanobis Distance methods, outlining their definitions, assumptions, formulas, procedures, and illustrative examples.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/fusion/advanced_topics":{"id":"theory/object-fusion-tracking/fusion/advanced_topics","title":"Advanced Topics","description":"Object Fusion Tracking is a critical component in various domains such as autonomous vehicles, robotics, and surveillance systems. It involves integrating data from multiple sensors to accurately detect, track, and predict the movement of objects in an environment. By fusing information from different sensor modalities\u2014such as radar, lidar, and cameras\u2014fusion tracking systems enhance the robustness and reliability of object detection and tracking, especially in complex and dynamic environments.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/fusion/implementation_steps":{"id":"theory/object-fusion-tracking/fusion/implementation_steps","title":"Implementation Steps","description":"Implementing object association and fusion within the Kalman filter framework involves a series of methodical steps that transform raw sensor data into accurate and reliable state estimates. This chapter delineates the comprehensive process of mapping measurement variables, calculating innovation, computing innovation covariance, determining the Kalman gain, and updating the state and covariance matrices. Each section provides detailed explanations, mathematical formulations, and practical examples to facilitate a thorough understanding and effective implementation of these critical steps.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/fusion/introduction":{"id":"theory/object-fusion-tracking/fusion/introduction","title":"Introduction to Object Fusion","description":"Object fusion is a pivotal element in the landscape of automated driving systems, serving as the bridge that integrates disparate sensor data into a coherent and accurate representation of the vehicle\'s surroundings. By amalgamating information from various sensors such as cameras, LiDARs, radars, and ultrasonic devices, object fusion enhances the reliability and precision of object detection and state estimation. This chapter explores the significance of object fusion in automated driving, elucidates its role within the Kalman filter framework as a measurement update process, and outlines the primary objective of combining sensor-level and global objects to minimize error and augment state precision.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/fusion/key_concepts":{"id":"theory/object-fusion-tracking/fusion/key_concepts","title":"Key Concepts","description":"Understanding the foundational concepts underpinning object association and fusion within the Kalman filter framework is essential for developing accurate and reliable multi-sensor data fusion systems. This chapter elucidates critical concepts, including sensor-level and global objects, error covariances, measurement matrices, innovations, innovation covariances, Kalman gains, and the state and covariance update processes. Each section provides comprehensive definitions, characteristics, roles, and practical examples to facilitate both novice and experienced users in mastering these fundamental principles.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/fusion/object_fusion":{"id":"theory/object-fusion-tracking/fusion/object_fusion","title":"Object Fusion","description":"In the realm of automated driving systems, accurate perception of the environment is paramount for safe and efficient navigation. Object fusion, also referred to as the measurement update in Kalman filters, plays a critical role in synthesizing data from multiple sensors to form a coherent understanding of surrounding objects. This process involves integrating sensor-level objects ($\\\\hatS$)\u2014the raw measurements from individual sensors\u2014with their corresponding **global objects** ($\\\\hat{x}G$)\u2014the predicted states maintained by the system.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/fusion/practical_considerations":{"id":"theory/object-fusion-tracking/fusion/practical_considerations","title":"Practical Considerations","description":"Object Fusion Tracking is a critical component in various domains such as autonomous vehicles, robotics, and surveillance systems. It involves integrating data from multiple sensors to accurately detect, track, and predict the movement of objects in an environment. By fusing information from different sensor modalities\u2014such as radar, lidar, and cameras\u2014fusion tracking systems enhance the robustness and reliability of object detection and tracking, especially in complex and dynamic environments.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/getting_started":{"id":"theory/object-fusion-tracking/getting_started","title":"Getting Started","description":"","sidebar":"objectSidebar"},"theory/object-fusion-tracking/introduction/fundamentals":{"id":"theory/object-fusion-tracking/introduction/fundamentals","title":"Challenges and Fundamentals","description":"Automated and connected driving systems are at the forefront of modern automotive technology, promising enhanced safety, efficiency, and convenience. Central to these systems is the ability to perceive and interpret the surrounding environment accurately. This environmental representation is achieved by integrating data from multiple sensors, such as cameras, radar, and lidar, each offering unique strengths and encountering distinct limitations.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/introduction/introduction":{"id":"theory/object-fusion-tracking/introduction/introduction","title":"Introduction","description":"Object Fusion and Tracking is a pivotal component in the data processing pipeline of automated driving systems. This stage serves as a bridge between sensor data processing and environment perception, seamlessly integrating raw sensor inputs to create a coherent and dynamic model of the surrounding environment. The primary focus of this section is to delve into the key objectives, fundamental methodologies, and inherent challenges associated with object fusion and tracking. By understanding these aspects, learners will gain practical insights and hands-on experience essential for contributing to the rapidly evolving field of automated driving research.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/prediction/importance_prediction":{"id":"theory/object-fusion-tracking/prediction/importance_prediction","title":"Importance of Prediction","description":"Object prediction serves as the temporal bridge between successive sensor measurements. Its significance lies in ensuring that object tracking remains accurate and consistent despite the inherent delays and motion dynamics within the system. This section explores the key reasons for implementing prediction mechanisms and the potential consequences of neglecting them in automated driving systems.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/prediction/introduction":{"id":"theory/object-fusion-tracking/prediction/introduction","title":"Introduction","description":"Object prediction is a fundamental component in the realms of object tracking and environment modeling, particularly within automated driving systems. It serves as the cornerstone for object association and data fusion by synchronizing inputs from diverse sensors with a unified global environment model. This documentation delves into the intricacies of object prediction using the Kalman filter, emphasizing its implementation and seamless integration into ROS-based (Robot Operating System) frameworks.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/prediction/mathematical_notion":{"id":"theory/object-fusion-tracking/prediction/mathematical_notion","title":"Mathematical Notation","description":"Understanding the mathematical foundation is crucial for grasping the mechanics of object prediction using Kalman filters. This section elucidates the symbols, indices, and coordinate systems employed throughout the documentation, providing a clear and consistent framework for interpreting the subsequent equations and implementations.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/prediction/motion_model":{"id":"theory/object-fusion-tracking/prediction/motion_model","title":"Motion Model and Process Noise Matrix","description":"The motion model and process noise matrix are pivotal in defining how an object\'s state evolves over time and how uncertainties are accounted for within the Kalman filter framework. This section delves into the specifics of these components, elucidating their roles, formulations, and significance in object prediction.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/prediction/object_description":{"id":"theory/object-fusion-tracking/prediction/object_description","title":"Object Description","description":"This section outlines the fundamental components involved in object prediction, namely the State Vector and the Error Covariance Matrix. Understanding these components is essential for implementing the Kalman filter effectively, as they form the basis for estimating and quantifying the state of tracked objects within the global environment model.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/prediction/prediction_equations":{"id":"theory/object-fusion-tracking/prediction/prediction_equations","title":"Prediction Equations","description":"The Kalman filter employs a set of mathematical equations to predict the future state of an object based on its current state and the underlying motion model. This section delineates the core prediction equations used in object prediction, providing both the theoretical foundations and practical implementations essential for accurate state estimation.","sidebar":"objectSidebar"},"theory/object-fusion-tracking/prediction/ros_integration":{"id":"theory/object-fusion-tracking/prediction/ros_integration","title":"Integration with ROS","description":"Integrating the Kalman filter-based object prediction into a ROS (Robot Operating System) environment facilitates efficient communication and real-time processing, essential for autonomous driving systems.","sidebar":"objectSidebar"},"theory/sensor-data-processing/camera_based_semantic_grid_mapping/challenges":{"id":"theory/sensor-data-processing/camera_based_semantic_grid_mapping/challenges","title":"Challenges","description":"Semantic grid mapping is a pivotal technique in modern autonomous systems, providing detailed and structured representations of a vehicle\'s environment. This documentation delves into the major challenges and potential solutions associated with deep learning-based, geometry-based, and hybrid approaches to semantic grid mapping. It caters to a range of technical proficiencies, from beginners to advanced users, ensuring clarity, technical depth, and contextual relevance throughout.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/camera_based_semantic_grid_mapping/introduction":{"id":"theory/sensor-data-processing/camera_based_semantic_grid_mapping/introduction","title":"Introduction","description":"Camera-based Semantic Grid Mapping is a cutting-edge technique employed in autonomous vehicle systems to interpret and represent the vehicle\'s surrounding environment in a structured and semantically rich grid format. Unlike traditional occupancy grid maps that merely indicate whether a space is occupied or free, semantic grid maps provide detailed categorization of each occupied cell. This categorization includes the identification of various object types such as roads, buildings, pedestrians, vehicles, and other static or dynamic entities. By leveraging camera images, this approach enhances the vehicle\'s ability to understand its environment, facilitating more informed and sophisticated decision-making processes essential for safe and efficient navigation.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/camera_based_semantic_grid_mapping/ipm":{"id":"theory/sensor-data-processing/camera_based_semantic_grid_mapping/ipm","title":"Inverse Perspective Mapping","description":"Camera-based Semantic Grid Mapping is an essential technique in the fields of computer vision and autonomous systems. It facilitates the conversion of raw image data into structured spatial representations, enabling machines to understand and navigate their environments effectively. A cornerstone of this transformation process is Inverse Perspective Mapping (IPM). IPM is instrumental in translating image coordinates from a camera\'s perspective into real-world grid maps, providing a top-down view that is crucial for tasks such as navigation, obstacle detection, and path planning.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/getting_started":{"id":"theory/sensor-data-processing/getting_started","title":"Getting Started","description":"Sensor data processing lies at the heart of modern perception systems, enabling machines to interpret and interact with their environments effectively. This process involves collecting raw sensor inputs, transforming them into structured information, and preparing them for higher-level tasks like segmentation, mapping, and object detection. Whether for autonomous vehicles, robotics, or intelligent surveillance, sensor data processing is the essential first step in building perception pipelines.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/image_segmentation/boosting_performance":{"id":"theory/sensor-data-processing/image_segmentation/boosting_performance","title":"Boosting Performance","description":"Image segmentation is a fundamental task in computer vision, involving the partitioning of an image into meaningful regions corresponding to different objects or classes. Enhancing the performance of image segmentation models is crucial for achieving higher accuracy, robustness, and generalization, especially in real-world applications such as autonomous driving, medical imaging, and satellite imagery analysis.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/image_segmentation/deep_learning":{"id":"theory/sensor-data-processing/image_segmentation/deep_learning","title":"Deep Learning","description":"Training deep learning models for semantic image segmentation is a meticulous process that involves designing appropriate network architectures, selecting effective loss functions, optimizing model parameters, and fine-tuning hyperparameters. This section provides a comprehensive overview of the training process, focusing on the encoder-decoder architecture, loss functions, optimization techniques, hyperparameter tuning, and practical applications within the context of automated driving.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/image_segmentation/evaluation":{"id":"theory/sensor-data-processing/image_segmentation/evaluation","title":"Evaluation","description":"Image segmentation is a cornerstone task in computer vision, aiming to partition an image into meaningful regions that correspond to different objects or classes. Evaluating the performance of image segmentation models is crucial to ascertain their accuracy, robustness, and applicability across various domains such as autonomous driving, medical imaging, and satellite imagery analysis. This documentation provides an in-depth exploration of one of the most pivotal evaluation metrics for semantic image segmentation: Mean Intersection over Union (Mean IoU or MIoU). We will delve into its definition, computation, significance, and practical application in benchmarking segmentation models.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/image_segmentation/introduction":{"id":"theory/sensor-data-processing/image_segmentation/introduction","title":"Introduction","description":"Semantic image segmentation is a cornerstone of computer vision in automated driving. It involves assigning a semantic class, such as \\"road,\\" \\"pedestrian,\\" or \\"car,\\" to every pixel in an image. This granular level of classification enables a vehicle to gain a comprehensive understanding of its surroundings, which is vital for safe navigation and informed decision-making. By accurately interpreting the visual environment, autonomous vehicles can effectively differentiate between various objects and surfaces, allowing for nuanced responses to dynamic driving conditions. This document delves into the fundamentals, challenges, and contemporary approaches to semantic segmentation, with a particular emphasis on modern deep learning techniques that drive advancements in this field.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/image_segmentation/training":{"id":"theory/sensor-data-processing/image_segmentation/training","title":"Training","description":"Training deep learning models for semantic image segmentation is a meticulous process that involves designing appropriate network architectures, selecting effective loss functions, optimizing model parameters, and fine-tuning hyperparameters. This documentation provides a comprehensive overview of the training process, focusing on the encoder-decoder architecture, loss functions, optimization techniques, hyperparameter tuning, and practical applications within the context of automated driving.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/introduction/goals_challenges":{"id":"theory/sensor-data-processing/introduction/goals_challenges","title":"Goals and Challenges of Environment Perception","description":"Environment perception is a critical component of sensor data processing in autonomous vehicles. It involves detecting and characterizing elements within the vehicle\'s surroundings to enable safe and intelligent decision-making. This document outlines the goals and challenges of environment perception, emphasizing the use of advanced algorithms, particularly neural networks, to tackle these tasks.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/introduction/introduction":{"id":"theory/sensor-data-processing/introduction/introduction","title":"Sensor Data Processing","description":"Sensor data processing is a cornerstone of autonomous vehicle functionality. It enables vehicles to perceive and interpret their environment, similar to human perception, ensuring safe and efficient operation. This document introduces the goals, categories, and challenges of sensor data processing, focusing on environment perception through electromagnetic and pressure wave detection.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/localization/challenges":{"id":"theory/sensor-data-processing/localization/challenges","title":"Localization Challenges","description":"Localization is a critical component in the development of automated vehicles. It involves determining the precise position and orientation (pose) of a vehicle within its environment. Accurate localization enables automated vehicles to navigate, plan maneuvers, and stabilize their movements effectively. However, achieving robust and precise localization presents numerous challenges, especially as the requirements escalate with higher levels of automation.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/localization/combination_localization_approaches":{"id":"theory/sensor-data-processing/localization/combination_localization_approaches","title":"Combination of Localization","description":"Localization is a foundational element in the realm of automated driving, serving as the mechanism through which a vehicle determines its precise position and orientation within its environment. Accurate localization facilitates critical functions such as navigation, path planning, and seamless interaction with High-Definition (HD) maps. This documentation delves into the synergistic combination of global localization techniques, like GNSS-based systems, and relative localization methods, such as those leveraging Inertial Measurement Units (IMUs). By integrating these approaches, the robustness and accuracy of vehicle pose estimation are significantly enhanced. Additionally, this document explores various sensor fusion strategies that amalgamate data from multiple sensors, addressing challenges related to diverse environmental conditions and sensor reliability to meet the stringent requirements of modern localization systems in automated driving.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/localization/global_localization":{"id":"theory/sensor-data-processing/localization/global_localization","title":"Global Localization","description":"Global localization is a fundamental component in the realm of automated vehicles, enabling precise determination of a vehicle\'s position within a fixed reference system. Accurate localization is crucial for tasks such as route planning, guidance, and control within digital maps. This documentation provides a comprehensive overview of global localization, exploring various methodologies, reference systems, and their applications in automated driving. Whether you are a beginner seeking to understand the basics or an advanced user aiming to deepen your technical knowledge, this guide offers clear explanations and contextual relevance to cater to your needs.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/localization/introduction":{"id":"theory/sensor-data-processing/localization/introduction","title":"Introduction to Localization","description":"Localization is a critical component in the realm of automated vehicles, enabling them to understand and navigate their environment effectively. While perception systems allow these vehicles to interpret their surroundings, localization ensures that the vehicle accurately determines its own position and orientation within that environment. This documentation delves into the fundamentals of localization, its significance in vehicle guidance, various localization methods, and the sensors that facilitate this essential task.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/localization/relative_localization":{"id":"theory/sensor-data-processing/localization/relative_localization","title":"Relative Localization","description":"Localization is a cornerstone technology in the realm of autonomous systems, robotics, and vehicular navigation. It involves determining the position and orientation (collectively known as \\"pose\\") of a device or vehicle within a given environment. While global localization leverages external references like GPS to ascertain position within a universal frame, relative localization focuses on estimating pose relative to an initial or prior position without relying on external global signals.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/object_detection/deep_learning":{"id":"theory/sensor-data-processing/object_detection/deep_learning","title":"Introduction","description":"","sidebar":"sensorSidebar"},"theory/sensor-data-processing/object_detection/evaluation":{"id":"theory/sensor-data-processing/object_detection/evaluation","title":"Introduction","description":"","sidebar":"sensorSidebar"},"theory/sensor-data-processing/object_detection/introduction":{"id":"theory/sensor-data-processing/object_detection/introduction","title":"Introduction","description":"","sidebar":"sensorSidebar"},"theory/sensor-data-processing/object_detection/training":{"id":"theory/sensor-data-processing/object_detection/training","title":"Introduction","description":"","sidebar":"sensorSidebar"},"theory/sensor-data-processing/point_cloud_ogm/deep_inverse_sensor":{"id":"theory/sensor-data-processing/point_cloud_ogm/deep_inverse_sensor","title":"Point Cloud Occupancy Grid Mapping Using Deep Inverse Sensor Models","description":"Occupancy Grid Mapping (OGM) is a fundamental technique in robotics and autonomous systems, providing a spatial representation of an environment by partitioning it into a grid and assigning a probability to each cell indicating whether it is occupied or free. Traditionally, geometry-based methods have been employed to generate occupancy maps from 3D point clouds obtained from sensors like LiDAR. These methods, while effective, often struggle with accuracy and adaptability, especially in complex or dynamic environments.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/point_cloud_ogm/geometric_inverse_sensor":{"id":"theory/sensor-data-processing/point_cloud_ogm/geometric_inverse_sensor","title":"Occupancy Grid Mapping from 3D Point Clouds: Geometric Inverse Sensor Models","description":"Occupancy grid mapping is a cornerstone technique in robotic perception and environmental modeling, enabling autonomous systems to interpret and navigate through their surroundings effectively. By transforming raw sensor data into a probabilistic grid-based representation of the environment, robots can make informed decisions, avoid obstacles, and perform complex tasks with increased autonomy and reliability.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/point_cloud_ogm/introduction":{"id":"theory/sensor-data-processing/point_cloud_ogm/introduction","title":"Point Cloud Occupancy Grid Mapping","description":"Occupancy Grid Mapping (OGM) is a pivotal technique in the realm of autonomous vehicle (AV) perception, serving as a bridge between raw sensor data and actionable environmental understanding. By transforming unstructured 3D sensor inputs into a structured and interpretable map, OGM facilitates critical tasks such as navigation, obstacle avoidance, and decision-making. This methodology discretizes a 3D environment into a grid of cells, each classified as free (drivable), occupied (non-drivable), or unknown. Such a representation not only provides a clear spatial understanding but also supports higher-level functionalities like trajectory planning, object detection, and semantic mapping.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/point_cloud_ogm/training_evaluation":{"id":"theory/sensor-data-processing/point_cloud_ogm/training_evaluation","title":"Training and Evaluating Neural Networks for Occupancy Grid Maps","description":"Occupancy Grid Maps (OGMs) are pivotal in robotics and autonomous systems for environment representation and navigation. They provide a discretized spatial map where each cell indicates the probability of being occupied or free. Leveraging neural networks to predict OGMs from 3D point cloud data has shown promising results, enhancing the accuracy and efficiency of spatial understanding in dynamic environments.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/semantic_point_cloud_segmentation/boosting_performance":{"id":"theory/sensor-data-processing/semantic_point_cloud_segmentation/boosting_performance","title":"Boosting Performance","description":"Semantic point cloud segmentation is a critical technology underpinning a myriad of applications, including autonomous driving, robotics, and augmented reality. By enabling machines to understand and interpret three-dimensional spatial data, semantic segmentation facilitates accurate object recognition, environment mapping, and interaction. Enhancing the performance of segmentation models involves overcoming challenges such as limited annotated data and class imbalance. This documentation provides an in-depth exploration of effective data augmentation strategies and specialized loss functions designed to elevate the performance of semantic point cloud segmentation models.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/semantic_point_cloud_segmentation/deep_learning":{"id":"theory/sensor-data-processing/semantic_point_cloud_segmentation/deep_learning","title":"Deep Learning","description":"Point cloud segmentation using deep learning is an innovative approach that harnesses neural networks to process complex, high-dimensional point cloud data. This documentation outlines the fundamental concepts, challenges, methodologies, and practical implementations of point cloud segmentation using deep learning techniques.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/semantic_point_cloud_segmentation/evaluation":{"id":"theory/sensor-data-processing/semantic_point_cloud_segmentation/evaluation","title":"Evaluation","description":"Semantic point cloud segmentation evaluation is pivotal in assessing the performance and efficacy of models tasked with partitioning a 3D scene into semantically meaningful components. Accurate evaluation ensures that segmentation models not only perform well on specific datasets but also generalize effectively to diverse real-world scenarios. This documentation delves into the Mean Intersection over Union (MIoU) metric, a cornerstone in segmentation evaluation, and explores the Semantic KITTI dataset, a benchmark standard in the field.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/semantic_point_cloud_segmentation/introduction":{"id":"theory/sensor-data-processing/semantic_point_cloud_segmentation/introduction","title":"Introduction","description":"This guide explores the extension of semantic image segmentation principles into the three-dimensional (3D) realm, specifically focusing on the semantic segmentation of 3D point clouds acquired through LiDAR sensors. Whether you are a beginner embarking on your journey into 3D computer vision or an advanced practitioner seeking deeper insights, this documentation provides a seamless, well-structured, and user-friendly exploration of the topic.","sidebar":"sensorSidebar"},"theory/sensor-data-processing/semantic_point_cloud_segmentation/training":{"id":"theory/sensor-data-processing/semantic_point_cloud_segmentation/training","title":"Training","description":"Semantic point cloud segmentation is a pivotal task in the realm of computer vision and machine learning, underpinning a multitude of applications such as autonomous driving, robotics, and augmented reality. This process entails the classification of each point within a 3D point cloud into predefined semantic categories, thereby enabling machines to comprehend and interpret their three-dimensional environments with precision. By assigning meaningful labels to individual points, systems can perform tasks ranging from obstacle detection and navigation to scene understanding and interaction.","sidebar":"sensorSidebar"},"theory/vehicle-guidance/getting_started":{"id":"theory/vehicle-guidance/getting_started","title":"Getting Started","description":"","sidebar":"vehicleSidebar"},"theory/vehicle-guidance/guidance_level/direct_multiple_shooting":{"id":"theory/vehicle-guidance/guidance_level/direct_multiple_shooting","title":"The Direct Multiple Shooting Approach","description":"The Direct Multiple Shooting Approach is an advanced methodology for addressing Optimal Control Problems (OCPs) within the realm of motion planning for autonomous vehicles. Building upon the foundational principles of direct approaches, this technique discretizes the control function into distinct intervals and performs forward integration of the system dynamics within each interval. This strategic balance between flexibility and robustness renders the Direct Multiple Shooting Approach particularly well-suited for handling complex vehicle models and nonlinear systems, ensuring precise and reliable motion planning.","sidebar":"vehicleSidebar"},"theory/vehicle-guidance/guidance_level/introduction":{"id":"theory/vehicle-guidance/guidance_level/introduction","title":"Introduction to Guidance-Level Motion Planning","description":"The guidance-level task in autonomous driving is pivotal for achieving precise motion planning and control, enabling a vehicle to transition seamlessly from an initial state to a desired goal state. Building upon foundational concepts of vehicle guidance, such as the Optimal Control Problem (OCP), this section delves into three primary methods for solving the OCP: dynamic programming, direct approaches, and indirect approaches. Central to this exploration is the model of vehicle guidance by Donges, which provides a comprehensive framework for understanding and implementing these methodologies.","sidebar":"vehicleSidebar"},"theory/vehicle-guidance/introduction/goals_challenges_fundamentals":{"id":"theory/vehicle-guidance/introduction/goals_challenges_fundamentals","title":"Goals, Challenges & Fundamentals","description":"Vehicle guidance is a fundamental component of automated driving systems, ensuring the safe, efficient, and robust operation of autonomous vehicles (AVs). It encompasses the vehicle\u2019s ability to navigate complex environments, make tactical decisions, and execute precise control actions. This framework is structured across three hierarchical levels:","sidebar":"vehicleSidebar"},"theory/vehicle-guidance/introduction/introduction":{"id":"theory/vehicle-guidance/introduction/introduction","title":"Vehicle Guidance","description":"Vehicle guidance is a pivotal component within the software architecture of automated driving systems. It ensures the safe and efficient navigation of autonomous vehicles by leveraging data from various upstream modules, such as perception and environment modeling. Beyond mere collision avoidance, vehicle guidance aims to enhance driving comfort and operational efficiency. Functioning as a tactical process, it translates high-level strategic navigation plans into actionable instructions, effectively bridging the gap between overarching decision-making and granular vehicle control.","sidebar":"vehicleSidebar"},"theory/vehicle-guidance/navigation_level/introduction_nav_level":{"id":"theory/vehicle-guidance/navigation_level/introduction_nav_level","title":"Vehicle Guidance on Navigation Level","description":"Vehicle guidance is essential for enabling autonomous systems to navigate complex environments effectively. It encompasses methods and algorithms designed to solve navigation problems, optimizing for safety, efficiency, and adaptability. This document explores the theoretical foundations and practical applications of vehicle guidance, with a particular focus on dynamic programming methods grounded in Bellman\u2019s principle of optimality.","sidebar":"vehicleSidebar"},"theory/vehicle-guidance/navigation_level/lanelet2":{"id":"theory/vehicle-guidance/navigation_level/lanelet2","title":"Lanelet2","description":"The Lanelet2 framework stands at the forefront of digital map representation and route planning technologies tailored for autonomous driving applications. Building upon and extending the capabilities of traditional navigation systems, Lanelet2 introduces lanelets\u2014modular and extensible data structures that offer high-granularity representations of road networks. This framework seamlessly integrates with motion planning and guidance-level algorithms, such as the Dijkstra algorithm, facilitating efficient route computation and trajectory planning. As a cornerstone for autonomous driving systems, Lanelet2 provides robust solutions to complex routing and navigation challenges, enabling safer and more reliable autonomous vehicle (AV) operations.","sidebar":"vehicleSidebar"},"theory/vehicle-guidance/stabilization_level/introduction":{"id":"theory/vehicle-guidance/stabilization_level/introduction","title":"Stabilization Level in Vehicle Control","description":"The stabilization level is a pivotal component of vehicle control systems in autonomous driving, responsible for ensuring that the vehicle adheres to the planned trajectory with minimal deviation. It compensates for external disturbances and inaccuracies in both the vehicle\'s state and the planned trajectory. Operating closest to the vehicle\'s physical interface, the stabilization level bridges the gap between high-level trajectory planning and low-level actuator commands, ensuring smooth and precise vehicle maneuvers.","sidebar":"vehicleSidebar"},"theory/vehicle-guidance/stabilization_level/levels":{"id":"theory/vehicle-guidance/stabilization_level/levels","title":"Low-, High-, and Bi-Level Stabilization in Vehicle Control","description":"Vehicle stabilization approaches are essential for ensuring that autonomous vehicles adhere closely to their planned trajectories, minimizing deviations caused by various disturbances. This document delves into three primary stabilization methodologies\u2014low-level stabilization, high-level stabilization, and bi-level stabilization\u2014each tailored to address specific types of disturbances and operational scenarios within automated driving systems.","sidebar":"vehicleSidebar"},"theory/vehicle-guidance/stabilization_level/trajectory_control":{"id":"theory/vehicle-guidance/stabilization_level/trajectory_control","title":"Trajectory Control Using Feedback PID Controllers","description":"Trajectory control at the stabilization level is a cornerstone of autonomous driving systems, ensuring that vehicles accurately follow pre-planned paths with minimal deviation. This is achieved through the integration of feedback PID controllers with feedforward controls. The primary objective is to calculate and correct deviations in both longitudinal and lateral movements, maintaining precise adherence to the desired trajectory. This is implemented using a closed-loop simulation within the ROS (Robot Operating System) framework, leveraging odometry calculations, trajectory data interpolation, and sophisticated control system design.","sidebar":"vehicleSidebar"}}}}')}}]);