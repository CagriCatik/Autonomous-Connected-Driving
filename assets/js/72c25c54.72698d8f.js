"use strict";(self.webpackChunkacd=self.webpackChunkacd||[]).push([[6408],{7098:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"theory/object-fusion-tracking/introduction/fundamentals","title":"Challenges and Fundamentals","description":"Automated and connected driving systems are at the forefront of modern automotive technology, promising enhanced safety, efficiency, and convenience. Central to these systems is the ability to perceive and interpret the surrounding environment accurately. This environmental representation is achieved by integrating data from multiple sensors, such as cameras, radar, and lidar, each offering unique strengths and encountering distinct limitations.","source":"@site/docs/theory/03_object-fusion-tracking/01_introduction/02_fundamentals.md","sourceDirName":"theory/03_object-fusion-tracking/01_introduction","slug":"/theory/object-fusion-tracking/introduction/fundamentals","permalink":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/introduction/fundamentals","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/theory/03_object-fusion-tracking/01_introduction/02_fundamentals.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"objectSidebar","previous":{"title":"Introduction","permalink":"/Autonomous-Connected-Driving/docs/theory/object-fusion-tracking/introduction/"},"next":{"title":"Object Prediction","permalink":"/Autonomous-Connected-Driving/docs/category/object-prediction"}}');var i=s(4848),o=s(8453);const a={},r="Challenges and Fundamentals",l={},c=[{value:"Fundamentals of the Multi-Instance Kalman Filter",id:"fundamentals-of-the-multi-instance-kalman-filter",level:2},{value:"1. Object Prediction",id:"1-object-prediction",level:3},{value:"2. Object Association",id:"2-object-association",level:3},{value:"3. Object Fusion",id:"3-object-fusion",level:3},{value:"Challenges in Sensor Fusion",id:"challenges-in-sensor-fusion",level:2},{value:"Sensor Characteristics",id:"sensor-characteristics",level:3},{value:"Environmental Factors",id:"environmental-factors",level:3},{value:"Computational Constraints",id:"computational-constraints",level:3},{value:"Steps to Implement a Multi-Instance Kalman Filter",id:"steps-to-implement-a-multi-instance-kalman-filter",level:2},{value:"Step 1: Object Prediction",id:"step-1-object-prediction",level:3},{value:"Input Transformation",id:"input-transformation",level:4},{value:"Temporal Synchronization",id:"temporal-synchronization",level:4},{value:"Code Snippet: Predicting Object States",id:"code-snippet-predicting-object-states",level:4},{value:"Step 2: Object Association",id:"step-2-object-association",level:3},{value:"Matching Algorithms",id:"matching-algorithms",level:4},{value:"Handling Unmatched Objects",id:"handling-unmatched-objects",level:4},{value:"Code Snippet: Associating Objects",id:"code-snippet-associating-objects",level:4},{value:"Step 3: Object Fusion",id:"step-3-object-fusion",level:3},{value:"Weighted Averaging",id:"weighted-averaging",level:4},{value:"Mitigating Uncertainty",id:"mitigating-uncertainty",level:4},{value:"Code Snippet: Fusing Object States",id:"code-snippet-fusing-object-states",level:4},{value:"Advanced Techniques and Optimizations",id:"advanced-techniques-and-optimizations",level:2},{value:"Adaptive Noise Covariance",id:"adaptive-noise-covariance",level:3},{value:"Handling Non-linear Dynamics",id:"handling-non-linear-dynamics",level:3},{value:"Parallel Processing",id:"parallel-processing",level:3},{value:"Code Snippet: Adaptive Noise Covariance",id:"code-snippet-adaptive-noise-covariance",level:3},{value:"Applications in Automated Driving",id:"applications-in-automated-driving",level:2},{value:"1. Environment Modeling",id:"1-environment-modeling",level:3},{value:"2. Behavior Prediction",id:"2-behavior-prediction",level:3},{value:"3. Path Planning",id:"3-path-planning",level:3},{value:"Implementation in ROS Environment",id:"implementation-in-ros-environment",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Step-by-Step Guide",id:"step-by-step-guide",level:3},{value:"Code Examples",id:"code-examples",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"challenges-and-fundamentals",children:"Challenges and Fundamentals"})}),"\n",(0,i.jsx)(n.p,{children:"Automated and connected driving systems are at the forefront of modern automotive technology, promising enhanced safety, efficiency, and convenience. Central to these systems is the ability to perceive and interpret the surrounding environment accurately. This environmental representation is achieved by integrating data from multiple sensors, such as cameras, radar, and lidar, each offering unique strengths and encountering distinct limitations."}),"\n",(0,i.jsxs)(n.p,{children:["A ",(0,i.jsx)(n.strong,{children:"multi-instance Kalman filter"})," stands out as an advanced tool for ",(0,i.jsx)(n.strong,{children:"object fusion and tracking"}),", enabling precise and dynamic modeling of the environment. By effectively combining data from various sensors, it facilitates reliable object detection, tracking, and prediction, which are critical for decision-making processes in automated vehicles."]}),"\n",(0,i.jsx)(n.p,{children:"This documentation delves into the principles, challenges, and methodologies involved in implementing and optimizing a multi-instance Kalman filter for object tracking in automated and connected driving systems. It is designed to cater to both beginners and advanced users, providing clear explanations, technical depth, and practical code examples."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"fundamentals-of-the-multi-instance-kalman-filter",children:"Fundamentals of the Multi-Instance Kalman Filter"}),"\n",(0,i.jsxs)(n.p,{children:["The multi-instance Kalman filter is an extension of the traditional Kalman filter, tailored to handle multiple objects and integrate data from diverse sensors. It constructs a ",(0,i.jsx)(n.strong,{children:"global environment model"})," by processing inputs from various sources, ensuring a coherent and accurate representation of the surroundings."]}),"\n",(0,i.jsx)(n.h3,{id:"1-object-prediction",children:"1. Object Prediction"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Object prediction"})," involves forecasting the future states of objects based on their current states and motion models. This step ensures that the global environment model remains up-to-date with the latest sensor data."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Alignment of Object Lists"}),": Align sensor-generated object lists with the global environment model to maintain consistency."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Coordinate Transformation"}),": Convert sensor data into a unified coordinate system, typically the vehicle's frame of reference."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"State Prediction"}),": Predict the current state of each object in the global model to match the timestamp of the latest sensor data."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Key Challenge"}),": Achieving spatiotemporal alignment despite varying sensor update rates. Different sensors may operate at different frequencies, requiring precise synchronization to ensure accurate predictions."]}),"\n",(0,i.jsx)(n.h3,{id:"2-object-association",children:"2. Object Association"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Object association"})," is the process of matching detected objects from sensor data to existing objects in the global environment model. This step is crucial for maintaining continuity in tracking and avoiding duplication or loss of objects."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Correspondence Determination"}),": Identify which sensor-level objects correspond to which global-level objects."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Handling New and Occluded Objects"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"New Detections"}),": Introduce new objects into the global model when they are detected by sensors but do not exist in the current model."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Occlusions"}),": Manage objects that become temporarily invisible due to physical obstructions, ensuring they are retained in the model until they reappear or are confirmed as no longer present."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Key Challenge"}),": Efficiently processing large object lists while managing false positives and maintaining real-time performance. As the number of objects increases, the association process must remain swift and accurate."]}),"\n",(0,i.jsx)(n.h3,{id:"3-object-fusion",children:"3. Object Fusion"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Object fusion"})," combines the states of associated objects from all relevant sensor readings to produce a coherent and reliable estimate of each object's state."]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"State Combination"}),": Merge position, velocity, and other relevant attributes from multiple sensors."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Weighting Measurements"}),": Assign weights to each sensor's measurements based on their reliability and accuracy. For instance, lidar may offer precise distance measurements, while cameras provide better lateral accuracy."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Global Tracking"}),": Output a unified list of tracked objects with updated states, ensuring consistency across the environment model."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Key Challenge"}),": Balancing sensor-specific biases while mitigating uncertainty. Different sensors may have varying degrees of accuracy and reliability, necessitating careful weighting and fusion strategies to achieve optimal results."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"challenges-in-sensor-fusion",children:"Challenges in Sensor Fusion"}),"\n",(0,i.jsx)(n.p,{children:"Integrating data from multiple sensors to create a reliable environment model is fraught with challenges. These challenges stem from the inherent characteristics of sensors, environmental factors, and computational constraints."}),"\n",(0,i.jsx)(n.h3,{id:"sensor-characteristics",children:"Sensor Characteristics"}),"\n",(0,i.jsx)(n.p,{children:"Each sensor type employed in automated driving systems possesses unique strengths and limitations:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Radar"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Strengths"}),": Precise longitudinal (distance) measurements, robust performance in various weather conditions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Limitations"}),": Less accurate lateral (side-to-side) measurements, lower resolution compared to other sensors."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Cameras"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Strengths"}),": Superior lateral accuracy, rich visual information (color, texture)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Limitations"}),": Less reliable for depth perception, performance can degrade in low-light or adverse weather conditions."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Lidar"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Strengths"}),": High-resolution distance measurements, excellent for object shape and size detection."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Limitations"}),": Susceptible to adverse weather (rain, fog), generally more expensive and computationally intensive."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"environmental-factors",children:"Environmental Factors"}),"\n",(0,i.jsx)(n.p,{children:"The dynamic and unpredictable nature of real-world environments introduces additional complexities:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Occlusion"}),": Objects may become partially or fully obscured from certain sensors due to physical obstructions like buildings, other vehicles, or environmental elements. This inconsistency in visibility complicates tracking continuity."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"False Detections"}),": Sensor noise and errors can result in ghost objects or false positives, leading to inaccuracies in the environment model."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"computational-constraints",children:"Computational Constraints"}),"\n",(0,i.jsx)(n.p,{children:"Automated driving systems require real-time processing to ensure timely decision-making. Sensor fusion algorithms, especially those handling multiple data streams and objects, must be optimized to process large volumes of data efficiently without introducing latency."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"steps-to-implement-a-multi-instance-kalman-filter",children:"Steps to Implement a Multi-Instance Kalman Filter"}),"\n",(0,i.jsx)(n.p,{children:"Implementing a multi-instance Kalman filter involves several methodical steps to ensure accurate object tracking and sensor fusion. Below is a detailed guide outlining each step, accompanied by relevant code snippets to facilitate practical understanding."}),"\n",(0,i.jsx)(n.h3,{id:"step-1-object-prediction",children:"Step 1: Object Prediction"}),"\n",(0,i.jsx)(n.p,{children:"Object prediction forecasts the future states of objects based on their current states and motion dynamics. This ensures the global environment model is synchronized with the latest sensor data."}),"\n",(0,i.jsx)(n.h4,{id:"input-transformation",children:"Input Transformation"}),"\n",(0,i.jsx)(n.p,{children:"Before prediction, sensor data must be transformed into a unified coordinate system to maintain consistency across different sensor inputs."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Coordinate Systems"}),": Typically, all sensor data is converted to the vehicle's local coordinate system to simplify computations and integration."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Transformation Process"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Translation"}),": Adjust object positions based on sensor mounting positions."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Rotation"}),": Align sensor orientations to the vehicle's frame."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"temporal-synchronization",children:"Temporal Synchronization"}),"\n",(0,i.jsx)(n.p,{children:"Different sensors may provide data at varying update rates. Temporal synchronization adjusts the global environment model to align with the timestamp of the latest sensor reading."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Time Alignment"}),": Predict the state of each object to the current timestamp using their velocity and acceleration."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Interpolation"}),": In cases where exact synchronization is challenging, interpolate states to approximate alignment."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"code-snippet-predicting-object-states",children:"Code Snippet: Predicting Object States"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass ObjectState:\n    def __init__(self, position, velocity, uncertainty):\n        self.position = np.array(position)  # [x, y, z]\n        self.velocity = np.array(velocity)  # [vx, vy, vz]\n        self.uncertainty = np.array(uncertainty)  # Covariance matrix\n\ndef predict_global_model(global_model, delta_time, process_noise):\n    """\n    Predicts the next state of each object in the global model.\n\n    Parameters:\n    - global_model: List of ObjectState instances representing current objects.\n    - delta_time: Time elapsed since last prediction.\n    - process_noise: Process noise covariance matrix.\n\n    Returns:\n    - Updated global_model with predicted states.\n    """\n    for obj in global_model:\n        # Predict new position based on velocity\n        obj.position += obj.velocity * delta_time\n        \n        # Update uncertainty with process noise\n        obj.uncertainty += process_noise * delta_time\n        \n    return global_model\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Explanation"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"ObjectState"})," class encapsulates the state of an object, including its position, velocity, and uncertainty."]}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"predict_global_model"})," function iterates through each object in the global model, updating its position based on velocity and increasing uncertainty to account for process noise over the elapsed time (",(0,i.jsx)(n.code,{children:"delta_time"}),")."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"step-2-object-association",children:"Step 2: Object Association"}),"\n",(0,i.jsx)(n.p,{children:"Object association matches detected objects from sensor data to existing objects in the global environment model. Accurate association is vital to maintain tracking continuity and prevent duplication."}),"\n",(0,i.jsx)(n.h4,{id:"matching-algorithms",children:"Matching Algorithms"}),"\n",(0,i.jsx)(n.p,{children:"Efficient matching algorithms are essential for handling large numbers of objects without compromising real-time performance."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Hungarian Algorithm"}),": An optimal matching algorithm that minimizes the total cost of association, suitable for handling many-to-one and one-to-many matching scenarios."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Nearest Neighbor"}),": A simpler approach where each sensor object is matched to the closest global object based on a predefined distance metric."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Joint Probabilistic Data Association (JPDA)"}),": Considers the probabilistic associations between multiple objects and measurements, useful in cluttered environments."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"handling-unmatched-objects",children:"Handling Unmatched Objects"}),"\n",(0,i.jsx)(n.p,{children:"Not all sensor detections will correspond to existing global objects, and not all global objects will have corresponding sensor detections."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"New Objects"}),": When a sensor detects an object that does not match any existing global object, a new entry is created in the global model."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Occluded Objects"}),": For global objects not detected by sensors, maintain their state in the global model for a predefined duration to account for temporary occlusions."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"False Detections"}),": Implement thresholds and validation checks to filter out false positives before associating objects."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"code-snippet-associating-objects",children:"Code Snippet: Associating Objects"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'from scipy.optimize import linear_sum_assignment\n\ndef compute_cost_matrix(sensor_objects, global_objects, distance_threshold):\n    """\n    Computes the cost matrix based on Euclidean distance between sensor and global objects.\n\n    Parameters:\n    - sensor_objects: List of ObjectState instances from sensors.\n    - global_objects: List of ObjectState instances from the global model.\n    - distance_threshold: Maximum allowable distance for association.\n\n    Returns:\n    - Cost matrix with distances; entries exceeding the threshold are set to a large value.\n    """\n    cost_matrix = np.zeros((len(sensor_objects), len(global_objects)))\n    for i, sensor_obj in enumerate(sensor_objects):\n        for j, global_obj in enumerate(global_objects):\n            distance = np.linalg.norm(sensor_obj.position - global_obj.position)\n            if distance > distance_threshold:\n                cost_matrix[i, j] = 1e6  # Assign a large cost for non-associable pairs\n            else:\n                cost_matrix[i, j] = distance\n    return cost_matrix\n\ndef associate_objects(sensor_objects, global_objects, distance_threshold=50.0):\n    """\n    Associates sensor-level objects with global-level objects using the Hungarian algorithm.\n\n    Parameters:\n    - sensor_objects: List of ObjectState instances from sensors.\n    - global_objects: List of ObjectState instances from the global model.\n    - distance_threshold: Maximum distance for valid association.\n\n    Returns:\n    - associations: List of tuples (sensor_object, global_object).\n    - unmatched_sensor_objects: List of sensor objects not associated.\n    - unmatched_global_objects: List of global objects not associated.\n    """\n    cost_matrix = compute_cost_matrix(sensor_objects, global_objects, distance_threshold)\n    \n    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n    \n    associations = []\n    unmatched_sensor = set(range(len(sensor_objects)))\n    unmatched_global = set(range(len(global_objects)))\n    \n    for i, j in zip(row_ind, col_ind):\n        if cost_matrix[i, j] < 1e5:\n            associations.append((sensor_objects[i], global_objects[j]))\n            unmatched_sensor.discard(i)\n            unmatched_global.discard(j)\n    \n    unmatched_sensor_objects = [sensor_objects[i] for i in unmatched_sensor]\n    unmatched_global_objects = [global_objects[j] for j in unmatched_global]\n    \n    return associations, unmatched_sensor_objects, unmatched_global_objects\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Explanation"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"compute_cost_matrix"})," function calculates the Euclidean distance between each sensor object and global object, assigning a large cost to pairs exceeding a specified distance threshold to prevent unlikely associations."]}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"associate_objects"})," function applies the Hungarian algorithm to find the optimal matching based on the cost matrix. It also identifies unmatched sensor and global objects for further processing, such as creating new global objects or handling occlusions."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h3,{id:"step-3-object-fusion",children:"Step 3: Object Fusion"}),"\n",(0,i.jsx)(n.p,{children:"Once objects are associated, their states are fused to create a more accurate and reliable estimate of each object's state."}),"\n",(0,i.jsx)(n.h4,{id:"weighted-averaging",children:"Weighted Averaging"}),"\n",(0,i.jsx)(n.p,{children:"Different sensors provide measurements with varying degrees of accuracy and reliability. Weighted averaging ensures that more reliable sensor data has a greater influence on the fused state."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Weight Determination"}),": Assign weights based on sensor characteristics and current reliability. For example, lidar data may be weighted more heavily for distance accuracy, while camera data may be emphasized for lateral positioning."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Fusion Process"}),": Combine the position, velocity, and other state attributes using the determined weights to calculate the fused state."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"mitigating-uncertainty",children:"Mitigating Uncertainty"}),"\n",(0,i.jsx)(n.p,{children:"Fusion must account for uncertainties inherent in sensor measurements to maintain robustness."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Covariance Integration"}),": Combine the covariance matrices of associated measurements to reflect the combined uncertainty."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Bias Correction"}),": Address any systematic biases in sensor data to prevent skewed fused states."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"code-snippet-fusing-object-states",children:"Code Snippet: Fusing Object States"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def fuse_states(sensor_obj, global_obj, sensor_weight, global_weight):\n    """\n    Fuses the state of a sensor object with a global object using weighted averaging.\n\n    Parameters:\n    - sensor_obj: ObjectState instance from sensor.\n    - global_obj: ObjectState instance from global model.\n    - sensor_weight: Weight assigned to sensor data.\n    - global_weight: Weight assigned to global model data.\n\n    Returns:\n    - Fused ObjectState instance.\n    """\n    fused_position = (sensor_weight * sensor_obj.position + global_weight * global_obj.position) / (sensor_weight + global_weight)\n    fused_velocity = (sensor_weight * sensor_obj.velocity + global_weight * global_obj.velocity) / (sensor_weight + global_weight)\n    \n    # Combine uncertainties (assuming independence)\n    fused_uncertainty = sensor_weight * sensor_obj.uncertainty + global_weight * global_obj.uncertainty\n    \n    return ObjectState(fused_position, fused_velocity, fused_uncertainty)\n\ndef fuse_objects(associations, sensor_weights, global_weights):\n    """\n    Fuses the states of associated objects.\n\n    Parameters:\n    - associations: List of tuples (sensor_object, global_object).\n    - sensor_weights: Dictionary mapping sensor types to their weights.\n    - global_weights: Dictionary mapping global model to their weights.\n\n    Returns:\n    - List of fused ObjectState instances.\n    """\n    fused_objects = []\n    for sensor_obj, global_obj in associations:\n        # Example: Assign weights based on sensor type\n        sensor_type = sensor_obj.type  # Assuming ObjectState has a \'type\' attribute\n        sensor_weight = sensor_weights.get(sensor_type, 1.0)\n        global_weight = global_weights.get(\'default\', 1.0)\n        \n        fused_obj = fuse_states(sensor_obj, global_obj, sensor_weight, global_weight)\n        fused_objects.append(fused_obj)\n        \n    return fused_objects\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Explanation"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"fuse_states"})," function performs weighted averaging of the position and velocity vectors of a sensor object and a global object. It also combines their uncertainties, assuming independence."]}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"fuse_objects"})," function iterates through all associations, applying ",(0,i.jsx)(n.code,{children:"fuse_states"})," to each pair based on predefined sensor and global weights, resulting in a list of fused object states."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"advanced-techniques-and-optimizations",children:"Advanced Techniques and Optimizations"}),"\n",(0,i.jsx)(n.p,{children:"To enhance the performance and accuracy of the multi-instance Kalman filter, several advanced techniques and optimizations can be employed."}),"\n",(0,i.jsx)(n.h3,{id:"adaptive-noise-covariance",children:"Adaptive Noise Covariance"}),"\n",(0,i.jsx)(n.p,{children:"Dynamic adjustment of the process and measurement noise covariance matrices can significantly improve filter performance under varying conditions."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Process Noise Adaptation"}),": Modify the process noise based on the object's motion dynamics. For instance, accelerate the uncertainty increase for objects exhibiting erratic movements."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Measurement Noise Adaptation"}),": Adjust measurement noise based on sensor reliability in different environments (e.g., increase lidar measurement noise in foggy conditions)."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"handling-non-linear-dynamics",children:"Handling Non-linear Dynamics"}),"\n",(0,i.jsx)(n.p,{children:"Real-world object movements often exhibit non-linear behavior, necessitating the use of extended or unscented Kalman filters."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Extended Kalman Filter (EKF)"}),": Linearizes non-linear models around the current estimate."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Unscented Kalman Filter (UKF)"}),": Utilizes the unscented transform to handle non-linearities without explicit linearization."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"parallel-processing",children:"Parallel Processing"}),"\n",(0,i.jsx)(n.p,{children:"Leveraging parallel computing can help manage the computational load, especially when dealing with large numbers of objects and high-frequency sensor data."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Multithreading"}),": Distribute different tasks (e.g., prediction, association, fusion) across multiple threads."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"GPU Acceleration"}),": Utilize GPU computing for computationally intensive tasks like matrix operations and optimization algorithms."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"code-snippet-adaptive-noise-covariance",children:"Code Snippet: Adaptive Noise Covariance"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def adapt_process_noise(obj, current_velocity, base_process_noise):\n    """\n    Adapts the process noise based on the object\'s current velocity.\n\n    Parameters:\n    - obj: ObjectState instance.\n    - current_velocity: Current velocity of the object.\n    - base_process_noise: Base process noise covariance matrix.\n\n    Returns:\n    - Adapted process noise covariance matrix.\n    """\n    speed = np.linalg.norm(current_velocity)\n    adaptation_factor = 1 + speed / 30.0  # Example scaling\n    adapted_noise = base_process_noise * adaptation_factor\n    return adapted_noise\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Explanation"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"adapt_process_noise"})," function scales the process noise covariance based on the object's speed, allowing the filter to account for higher uncertainty in faster-moving objects."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"applications-in-automated-driving",children:"Applications in Automated Driving"}),"\n",(0,i.jsx)(n.p,{children:"The multi-instance Kalman filter is pivotal in various aspects of automated driving, enabling robust perception and decision-making."}),"\n",(0,i.jsx)(n.h3,{id:"1-environment-modeling",children:"1. Environment Modeling"}),"\n",(0,i.jsx)(n.p,{children:"Constructs a dynamic and consistent model of the vehicle's surroundings by tracking multiple objects over time. This model serves as the foundational layer for situational awareness and is essential for navigating complex environments."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Object Tracking"}),": Maintains continuous tracking of vehicles, pedestrians, cyclists, and other relevant objects."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Map Integration"}),": Incorporates static map data (e.g., road layouts, traffic signs) with dynamic object information."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-behavior-prediction",children:"2. Behavior Prediction"}),"\n",(0,i.jsx)(n.p,{children:"Analyzes tracked objects' trajectories to anticipate future movements, enabling proactive decision-making and collision avoidance."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Trajectory Forecasting"}),": Predicts the paths of surrounding vehicles and pedestrians based on current motion patterns."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Intent Recognition"}),": Infers the likely intentions of other road users (e.g., lane changes, turns) to adjust the vehicle's behavior accordingly."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-path-planning",children:"3. Path Planning"}),"\n",(0,i.jsx)(n.p,{children:"Uses the environment model and behavior predictions to generate safe and efficient trajectories for the autonomous vehicle."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Route Optimization"}),": Determines the optimal path considering current traffic conditions and planned maneuvers."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Collision Avoidance"}),": Adjusts the planned trajectory in real-time to prevent potential collisions based on dynamic object movements."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"implementation-in-ros-environment",children:"Implementation in ROS Environment"}),"\n",(0,i.jsx)(n.p,{children:"Implementing a multi-instance Kalman filter within the Robot Operating System (ROS) framework facilitates seamless integration with various sensors and other system components."}),"\n",(0,i.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"ROS Installation"}),": Ensure that ROS (e.g., ROS Noetic, ROS 2) is installed and properly configured on your development machine."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Sensor Drivers"}),": Install and configure drivers for all sensors (e.g., cameras, lidar, radar) to provide necessary data streams."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Development Tools"}),": Familiarity with C++ or Python, depending on the chosen ROS language bindings."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"step-by-step-guide",children:"Step-by-Step Guide"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Set Up ROS Workspace"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"mkdir -p ~/catkin_ws/src\ncd ~/catkin_ws/\ncatkin_make\nsource devel/setup.bash\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Create a New ROS Package"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ~/catkin_ws/src\ncatkin_create_pkg kalman_filter_pkg rospy std_msgs sensor_msgs\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Implement the Kalman Filter Node"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Navigate to the package directory and create a ",(0,i.jsx)(n.code,{children:"scripts"})," folder."]}),"\n",(0,i.jsx)(n.li,{children:"Develop Python or C++ scripts implementing the multi-instance Kalman filter logic."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Integrate Sensor Data"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Subscribe to relevant sensor topics (e.g., ",(0,i.jsx)(n.code,{children:"/lidar_points"}),", ",(0,i.jsx)(n.code,{children:"/camera_images"}),")."]}),"\n",(0,i.jsx)(n.li,{children:"Process incoming data to extract object detections."}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Publish Fused Object States"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["After fusion, publish the updated global object states to a dedicated topic (e.g., ",(0,i.jsx)(n.code,{children:"/fused_objects"}),")."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Launch the Node"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Create a launch file to start the Kalman filter node along with necessary sensor nodes."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Example ",(0,i.jsx)(n.code,{children:"launch"})," file:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<launch>\n  <node name="kalman_filter_node" pkg="kalman_filter_pkg" type="kalman_filter.py" output="screen"/>\n  \x3c!-- Include other sensor nodes as needed --\x3e\n</launch>\n'})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Start the launch:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"roslaunch kalman_filter_pkg kalman_filter.launch\n"})}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"code-examples",children:"Code Examples"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Python Example: Kalman Filter Node"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python\n\nimport rospy\nfrom sensor_msgs.msg import PointCloud2\nfrom std_msgs.msg import Header\nimport numpy as np\nfrom kalman_filter_pkg.msg import FusedObject  # Custom message type\n\nclass KalmanFilterNode:\n    def __init__(self):\n        rospy.init_node('kalman_filter_node')\n        \n        # Subscribers for different sensors\n        rospy.Subscriber('/lidar_points', PointCloud2, self.lidar_callback)\n        rospy.Subscriber('/camera_detections', DetectionArray, self.camera_callback)\n        rospy.Subscriber('/radar_detections', DetectionArray, self.radar_callback)\n        \n        # Publisher for fused objects\n        self.fused_pub = rospy.Publisher('/fused_objects', FusedObject, queue_size=10)\n        \n        # Initialize global model\n        self.global_model = []\n        self.process_noise = np.eye(6) * 0.1  # Example process noise\n        self.rate = rospy.Rate(10)  # 10 Hz\n\n    def lidar_callback(self, data):\n        # Process lidar data and update global model\n        sensor_objects = self.extract_objects(data)\n        self.update_global_model(sensor_objects, 'lidar')\n\n    def camera_callback(self, data):\n        # Process camera data and update global model\n        sensor_objects = self.extract_objects(data)\n        self.update_global_model(sensor_objects, 'camera')\n\n    def radar_callback(self, data):\n        # Process radar data and update global model\n        sensor_objects = self.extract_objects(data)\n        self.update_global_model(sensor_objects, 'radar')\n\n    def extract_objects(self, data):\n        # Placeholder for object extraction logic\n        return []\n\n    def update_global_model(self, sensor_objects, sensor_type):\n        # Prediction step\n        delta_time = 0.1  # Example delta_time\n        self.global_model = predict_global_model(self.global_model, delta_time, self.process_noise)\n        \n        # Association step\n        associations, unmatched_sensor, unmatched_global = associate_objects(sensor_objects, self.global_model)\n        \n        # Fusion step\n        sensor_weights = {'lidar': 1.5, 'camera': 1.0, 'radar': 1.2}\n        global_weights = {'default': 1.0}\n        fused_objects = fuse_objects(associations, sensor_weights, global_weights)\n        \n        # Update global model with fused objects\n        self.global_model.extend(unmatched_sensor)  # Add new objects\n        \n        # Publish fused objects\n        for obj in fused_objects:\n            fused_msg = FusedObject()\n            fused_msg.position = obj.position.tolist()\n            fused_msg.velocity = obj.velocity.tolist()\n            self.fused_pub.publish(fused_msg)\n\n    def run(self):\n        while not rospy.is_shutdown():\n            self.rate.sleep()\n\nif __name__ == '__main__':\n    node = KalmanFilterNode()\n    node.run()\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Explanation"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"KalmanFilterNode"})," class initializes ROS subscribers for lidar, camera, and radar data."]}),"\n",(0,i.jsx)(n.li,{children:"Each sensor callback processes incoming data, extracts objects, and updates the global model through prediction, association, and fusion steps."}),"\n",(0,i.jsxs)(n.li,{children:["The fused object states are published to the ",(0,i.jsx)(n.code,{children:"/fused_objects"})," topic for use by other system components."]}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"extract_objects"})," method is a placeholder and should be implemented to parse sensor data into ",(0,i.jsx)(n.code,{children:"ObjectState"})," instances."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,i.jsx)(n.p,{children:"The multi-instance Kalman filter is an indispensable component in the realm of automated and connected driving, facilitating robust sensor fusion and precise object tracking. By adeptly addressing challenges such as sensor-specific inaccuracies, environmental occlusions, and computational limitations, the filter ensures a reliable and real-time representation of the driving environment."}),"\n",(0,i.jsx)(n.p,{children:"Implementing and optimizing a multi-instance Kalman filter involves a deep understanding of both the theoretical underpinnings and practical considerations, including sensor characteristics, data association techniques, and fusion methodologies. Mastery of these elements empowers developers and engineers to enhance the safety, efficiency, and intelligence of automated driving systems."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>a,x:()=>r});var t=s(6540);const i={},o=t.createContext(i);function a(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);