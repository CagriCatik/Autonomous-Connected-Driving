"use strict";(self.webpackChunkacd=self.webpackChunkacd||[]).push([[5997],{7955:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"task/sensor_data_processing/Geometric-OGM","title":"Geometric Point Cloud Occupancy Grid Mapping","description":"ROS1","source":"@site/docs/task/02_sensor_data_processing/03_Geometric-OGM.md","sourceDirName":"task/02_sensor_data_processing","slug":"/task/sensor_data_processing/Geometric-OGM","permalink":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Geometric-OGM","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/task/02_sensor_data_processing/03_Geometric-OGM.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{},"sidebar":"taskSidebar","previous":{"title":"Deep Learning-based Point Cloud Occupancy Grid Mapping","permalink":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Deep-OGM"},"next":{"title":"Localization","permalink":"/Autonomous-Connected-Driving/docs/task/sensor_data_processing/Localization"}}');var o=s(4848),t=s(8453);const r={},a="Geometric Point Cloud Occupancy Grid Mapping",c={},l=[{value:"Task 1: Set up and test the ROS workspace",id:"task-1-set-up-and-test-the-ros-workspace",level:2},{value:"Task 2: Separate obstacles from ground",id:"task-2-separate-obstacles-from-ground",level:2},{value:"Task 3: Complete the geometric ISM",id:"task-3-complete-the-geometric-ism",level:2},{value:"Wrap-up",id:"wrap-up",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"geometric-point-cloud-occupancy-grid-mapping",children:"Geometric Point Cloud Occupancy Grid Mapping"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{src:"https://img.shields.io/badge/ROS1-blue",alt:"ROS1"})}),"\n",(0,o.jsxs)(n.p,{children:["You will now get some hands-on experience with inverse sensor models that create occupancy grid maps from lidar point clouds. In this task, you will complete an algorithm for a ",(0,o.jsx)(n.strong,{children:"geometric inverse sensor model"})," and in the next tasks you will train and use a deep learning-based model."]}),"\n",(0,o.jsx)(n.p,{children:"You will"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"start multiple ROS nodes using a launch file to run and visualize a geometric inverse sensor model using recorded lidar sensor data from a Rosbag,"}),"\n",(0,o.jsx)(n.li,{children:"learn how to use an existing ROS package to separate ground from obstacle points in the lidar point cloud and"}),"\n",(0,o.jsx)(n.li,{children:"implement a simple geometric inverse sensor model."}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"task-1-set-up-and-test-the-ros-workspace",children:"Task 1: Set up and test the ROS workspace"}),"\n",(0,o.jsxs)(n.p,{children:["We will use the same ROS environment and sensor data that was already used in the previous object detection task. So, make sure to have the ",(0,o.jsx)(n.strong,{children:"ROS framework"})," set up and the ",(0,o.jsx)(n.strong,{children:"Rosbag"})," containing recorded lidar data at ",(0,o.jsx)(n.code,{children:"acdc/bag/lidar_campus_melaten.bag"}),". If you skipped the object detection task, download the Rosbag ",(0,o.jsx)(n.a,{href:"https://rwth-aachen.sciebo.de/s/udlMYloXpCdVtyp",children:"here"}),"."]}),"\n",(0,o.jsxs)(n.p,{children:["If not running yet, start the Docker container by executing the ",(0,o.jsx)(n.code,{children:"docker/run.sh"})," script. Open a shell in the container by executing the script again from another terminal. In the container, build the ROS package and source the ROS workspace:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"catkin build pointcloud_ogm\nsource devel/setup.bash\n"})}),"\n",(0,o.jsx)(n.p,{children:"Start a roscore in the background and start playing the recorded data:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"roscore&\nrosbag play --loop ../bag/lidar_campus_melaten.bag\n"})}),"\n",(0,o.jsx)(n.p,{children:"You will see this output in the terminal:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"rosuser@******:~/ws/catkin_workspace$ roscore&\n[1] 14791\nrosuser@******:~/ws/catkin_workspace$ ... logging to /home/rosuser/.ros/log/52caca3c-4495-11ec-82b7-b49691b9ac50/roslaunch-I2102656-linux-14791.log\nChecking log directory for disk usage. This may take a while.\nPress Ctrl-C to interrupt\nDone checking log file disk usage. Usage is <1GB.\n\nstarted roslaunch server http://******:34069/\nros_comm version 1.15.11\n...\nstarted core service [/rosout]\nrosbag play --loop ../bag/lidar_campus_melaten.bag\n[ INFO] [1636816879.584949638]: Opening ../bag/lidar_campus_melaten.bag\n\nWaiting 0.2 seconds after advertising topics... done.\n\nHit space to toggle paused, or 's' to step.\n [RUNNING]  Bag Time: 1580916332.230592   Duration: 0.820823 / 119.955245\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Open another shell in the running container by executing the ",(0,o.jsx)(n.code,{children:"docker/run.sh"})," script again from another terminal window. In the container, source the workspace and execute the launch file that starts the geometric inverse sensor model:"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"source devel/setup.bash\nroslaunch pointcloud_ogm GeometricISM.launch\n"})}),"\n",(0,o.jsxs)(n.p,{children:["Rviz will open and visualize lidar point clouds. The points in the ",(0,o.jsx)(n.strong,{children:"originally recorded point cloud"})," are colored by the intensity of the reflection. A second ",(0,o.jsx)(n.strong,{children:"obstacle point cloud"})," is colored in purple. You can (de-)activate the visualization of the point clouds by (un-)ticking them in the left menu."]}),"\n",(0,o.jsx)("img",{src:"../images/rviz_points2.PNG",alt:"Description of image"}),"\n",(0,o.jsx)(n.h2,{id:"task-2-separate-obstacles-from-ground",children:"Task 2: Separate obstacles from ground"}),"\n",(0,o.jsxs)(n.p,{children:["The recorded lidar point clouds are published as ROS messages of type ",(0,o.jsx)(n.a,{href:"http://docs.ros.org/en/noetic/api/sensor_msgs/html/msg/PointCloud2.html",children:"sensor_msgs/PointCloud2"}),". Each message contains a set of points where each point is described by multiple fields. In our case each point has the fields ",(0,o.jsx)(n.code,{children:"x"}),", ",(0,o.jsx)(n.code,{children:"y"})," and ",(0,o.jsx)(n.code,{children:"z"})," for the position as well as fields describing the ",(0,o.jsx)(n.code,{children:"intensity"})," of reflection and number of the vertical sensor ",(0,o.jsx)(n.code,{children:"ring"})," that detected the reflection."]}),"\n",(0,o.jsx)("img",{src:"../images/rqt_points2_fields.PNG",alt:"Description of image"}),"\n",(0,o.jsxs)(n.p,{children:["At first step of the geometric algorithm, we have to ",(0,o.jsx)(n.strong,{children:"extract all ground points from the lidar point cloud"}),". For that, we can use the ",(0,o.jsx)(n.code,{children:"PassThrough"})," filter that is ",(0,o.jsx)(n.a,{href:"https://wiki.ros.org/pcl_ros/Tutorials/filters#PassThrough",children:"provided by the PointCloudLibrary"}),". It allows setting parameters to determine which points should be kept in the point cloud. The filter is already launched by the file ",(0,o.jsx)(n.code,{children:"workshops/section_2/pointcloud_ogm/launch/GeometricISM.launch"}),":"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-xml",children:'  <node pkg="nodelet" type="nodelet" name="GroundExtraction" args="load pcl/PassThrough $(arg nodelet_manager)" output="screen">\n    <remap from="~input" to="/points2" />\n    <remap from="~output" to="/points2_obstacles" />\n    <rosparam>\n      filter_limit_negative: False\n      filter_field_name: x\n      filter_limit_min: -50\n      filter_limit_max: 50\n    </rosparam>\n  </node>\n'})}),"\n",(0,o.jsxs)(n.p,{children:["The parameters are already set according to the ",(0,o.jsx)(n.a,{href:"https://wiki.ros.org/pcl_ros/Tutorials/filters#PassThrough",children:"specification in the ROS wiki"}),", but they do not fulfill the task. Your task is now to find a configuration that extracts most ground points from the point cloud but (as far as possible) no obstacle points. After changing the parameters, kill the running launch file by pressing ",(0,o.jsx)(n.code,{children:"CTRL-C"})," in the terminal and restart it to see the effects."]}),"\n",(0,o.jsx)(n.h2,{id:"task-3-complete-the-geometric-ism",children:"Task 3: Complete the geometric ISM"}),"\n",(0,o.jsxs)(n.p,{children:["The ROS nodelet ",(0,o.jsx)(n.code,{children:"pointcloud_ogm/GeometricISM"})," receives lidar point cloud and shall create occupancy grid maps using a geometric inverse sensor model and a binary Bayes filter. You can find the code in the file ",(0,o.jsx)(n.code,{children:"workshops/section_2/pointcloud_ogm/src/GeometricISM.src"}),". The following steps are performed by the code:"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["A point cloud is received and the ",(0,o.jsx)(n.code,{children:"messageCallback()"})," method is executed.","\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["A new grid map is initialized with occupancy probability of all cells set to ",(0,o.jsx)(n.code,{children:"0.5"})," (or ",(0,o.jsx)(n.code,{children:"50%"}),")."]}),"\n",(0,o.jsxs)(n.li,{children:["All points in the point cloud are evaluated sequentially","\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"A line is formed from each point to the sensor."}),"\n",(0,o.jsx)(n.li,{children:"The inverse sensor model determines occupancy probabilities of all grid cells on this line."}),"\n",(0,o.jsx)(n.li,{children:"A binary Bayes filter is used to combine the occupancy probabilities derived from the inverse sensor model with the previous state of the grid map."}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(n.li,{children:"The complete measurement grid map is published and can be visualized in Rviz."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["The following code iterates over all cells on the line between a reflection point and the sensor. The current occupancy probability of the cell can be accessed with the variable ",(0,o.jsx)(n.code,{children:"occupancy_probability"}),". A binary Bayes filter combines this value with the occupancy probability derived by the inverse sensor model ",(0,o.jsx)(n.code,{children:"p_ism"}),". The developer has already added comment describing the desired behavior but you will have to ",(0,o.jsx)(n.strong,{children:"add the code for this simple inverse sensor model"})," in the marked section."]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-cpp",children:'    int cell = 0;\n    for (grid_map::LineIterator iterator(grid_map_measurement, start_position, end_position); !iterator.isPastEnd(); ++iterator) {\n      auto& occupancy_probability = grid_map_measurement.at("occupancy_probability", *iterator);\n\n      /* inverse sensor model:\n          - cell containing reflection point: 90% occupancy probability\n          - next two cells towards sensor: 80%, 50% occupancy probability\n          - remaining cells towards sensor: 10% occupancy probability\n      */\n      double p_ism;\n      // TASK 2 BEGIN\n      // ADD YOUR CODE HERE...\n\n      // TASK 2 END\n      \n      // combine probability from ism with previous probability in cell using binary Bayes filter\n      occupancy_probability = (p_ism*occupancy_probability) /\n                              (p_ism*occupancy_probability + (1-p_ism)*(1-occupancy_probability));\n\n      cell++;\n    }\n'})}),"\n",(0,o.jsx)(n.p,{children:'Once you have added your code, compile the ros workspace and restart the launch file. Enable the checkbox next to "Grid Map (GeometricISM)" in the Rviz window to see the published occupancy grid maps.'}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"catkin build pointcloud_ogm\nroslaunch pointcloud_ogm GeometricISM.launch\n"})}),"\n",(0,o.jsx)(n.h2,{id:"wrap-up",children:"Wrap-up"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["You have learned how to ",(0,o.jsx)(n.strong,{children:"use a filter from the PointCloudLibrary"})," to preprocess point clouds."]}),"\n",(0,o.jsxs)(n.li,{children:["You wrote an ",(0,o.jsx)(n.strong,{children:"algorithm for a simple geometric inverse sensor model"})," to create occupancy grid maps from point clouds."]}),"\n",(0,o.jsxs)(n.li,{children:["You should notice that this approach has some noticeable ",(0,o.jsx)(n.strong,{children:"deficiancies"}),"."]}),"\n",(0,o.jsx)(n.li,{children:"In the next Python task, you will train a neural network to perform occupancy grid mapping from lidar point clouds"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>a});var i=s(6540);const o={},t=i.createContext(o);function r(e){const n=i.useContext(t);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);